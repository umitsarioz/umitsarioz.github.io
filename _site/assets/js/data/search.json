[
  
  {
    "title": "Getting Started with NLP | A Journey from RNNs to Transformers",
    "url": "/posts/nlp-101/",
    "categories": "",
    "tags": "machine-learning, supervised-learning, deep-learning, nlp, basics",
    "date": "2024-10-01 00:00:00 +0300",
    





    
    "snippet": "Natural language processing is a sub-branch of artificial intelligence that works to improve the communication between humans and the machines. Although it has become popular today with chatbots li...",
    "content": "Natural language processing is a sub-branch of artificial intelligence that works to improve the communication between humans and the machines. Although it has become popular today with chatbots like ChatGPT and voice assistants like Siri, it has actually been affecting our lives as a field much earlier. For example, the translation process from one language to another, which Google Translate does, is one of the problems solved by natural language processing.Natural language processing methods have continued to develop and increase over time. The chronological order of deep learning-based NLP architectures from the oldest to the newest is as follows: RNN, LSTM, GRU, BiLSTM, Transformer, LLM.Sequence ModelsSimple RNN Cell. Image by [4]Recurrent Neural Networks is one of revolutionary models that can work on sequential data. It is an artificial neural network architecture that allows predictions about the future using information from the previous time series. It has been used for tasks such as speech recognition and text generation, but it has some disadvantages. For example, when the input is too long, problems such as forgetting previous information and vanishing gradient are encountered.LSTM Cell. Image by [4]Operations. Image by [4]Long-Short Term Memory (LSTM) architecture, is supported by several mathematical gates and memory cells in addition to the RNN architecture. These gates ensure that some of the previous information is forgotten and updated with new information, while the memory cell ensures that the previous information is retained. In this way, only useful information related to the past is retained, and the vanishing gradient problem for longer time series can be partially solved.GRU Cell. Image by [4]Gated Recurrent Unit (GRU) architecture, is very similar to LSTM but uses different and fewer mathematical gates. The same advantages gained when moving from RNN to LSTM apply to this. By changing the activation functions in the gate, different and sometimes better results can be obtained.Bidirectional LSTM Architecture.Bidirectional LSTM, in addition to the LSTM architecture, a two-way training process is implemented, allowing it to learn better by looking at both past and future words. It is much more successful than normal LSTMs for tasks such as Named Entity Recognition and Part of Speech.Seq2Seq Paper.Encoder-Decoder architecture is a very powerful structure in seq2seq tasks. In this structure, architectures such as LSTM and RNN that can perform seq2seq tasks are used in both parts. The structures can be the same or can be differentiated, they are customized according to the tasks. On the encoder side, the input sequence is processed and the input is encoded and a context vector is created. This context vector produced on the decoder side is taken as input and decoded and the output sequence is created. In classical encoder-decoder architectures, the input sequence, context vector and output sequence shapes are fixed. This brings us some parameter restrictions that we need to determine beforehand. For example, when translating from one language to another, we need to give a rule such as you can produce a maximum of 5 words of output with a 3-word input. This causes semantic loss.Encoder-Decoder Architecture.Attention is All You Need Paper.With the Transformer architecture released in 2017, a solution was found to this problem. This architecture is basically based on the Attention mechanism. In another article, I will explain the following Transformer architecture step by step and code it from scratch with pytorch.Transformer ArchitectureThe Transformer architecture above has been diversified in different ways and is used in today‚Äôs models with architectures such as BERT, T5, GPT.Types of Sequence ModelsNLP models handle sequences of data in different ways, depending on the task. Here‚Äôs a quick overview of how these sequences are structured:  One-to-One: A single input produces a single output.(e.g., Image classification)  One-to-Many: A single input generates a sequence of outputs.(e.g., Image captioning,recommendation)  Many-to-One: Multiple inputs generate a single output.(e.g.,Sentiment analysis, traffic, next sale)  Many-to-Many: Both input and output are sequences.(e.g., Machine translation, chatbots, qa,llm, text summarization, speech recognition)Key NLP ConceptsWhen working with language data, there are several basic concepts you need to understand.TokenizationTokenization splits text into smaller pieces, usually words or subwords. It‚Äôs one of the first steps in processing text, as models work with these smaller units rather than the full sentence or paragraph.EncodingEncoding styles. Image by Avi Chawla [6]Encoding transforms text data into numerical values that models can process. Common encoding methods include one-hot encoding and integer encoding, though they don‚Äôt capture the meaning or relationships between words.EmbeddingsCommon Embedding ExampleEmbeddings represent words as dense vectors in a multi-dimensional space, where semantically similar words are closer together. Word2Vec and GloVe are common methods for generating word embeddings.Lemmatization and StemmingLemmatization &amp; Stemming. Image by PinterestBoth techniques are used to reduce words to their base form.  Stemming: Reduces words to their root form by chopping off suffixes (e.g., ‚Äúrunning‚Äù becomes ‚Äúrun‚Äù).  Lemmatization: Converts words to their dictionary form, considering the context (e.g., ‚Äúbetter‚Äù becomes ‚Äúgood‚Äù).Bag of Words (BoW)Bag of Words is a simple method that represents text as a collection of words, ignoring order and context. Each word is associated with a frequency count, which helps in identifying key terms, but it doesn‚Äôt capture meaning or relationships.TF-IDF(Term Frequency- Inverse Document Frequency)TF-IDF improves upon BoW by weighing words based on how important they are in a document relative to how common they are across all documents. Words that are frequent in a single document but rare across others are given more importance.Word2VecWord2Vec is an embedding technique that represents words in a continuous vector space. It captures the relationships between words based on their surrounding words, making it useful for semantic tasks like similarity detection and analogy solving. There are two common techniques, CBOW and n-skip gram. In both techniques, a certain window size is determined and a supervised dataset is created. In CBOW, the relationship between the middle word and the surrounding words is examined, in skip gram, the relationship between the surrounding words and the middle word is examined. Last but not least, there are other techniques like word2vec such as GloVe, FastText etc.ConclusionNLP has come a long way, evolving from simple models like RNNs to complex architectures like Transformers. Each new model improved upon the limitations of the previous one, leading to the powerful Large Language Models we have today. With these advances, computers are now capable of understanding and generating language at levels that were once thought impossible.NLP is now an integral part of modern technology, enabling everything from search engines to virtual assistants. The field will continue to evolve, and with it, the ways we interact with machines will become more seamless and natural.References  Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. ‚ÄúSequence to sequence learning with neural networks.‚Äù NIPS 2014.  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. ‚ÄúNeural machine translation by jointly learning to align and translate.‚Äù ICLR 2015.  Ashish Vaswani, et al. ‚ÄúAttention is all you need.‚Äù NIPS 2017.  Understanding LSTMS, Christopher Olah .  Wikidocs; LSTM,GRU,RNN   7 Must-know Techniques For Encoding Categorical Feature, Avi Chawla"
  },
  
  {
    "title": "Singular Value Decomposition (SVD)",
    "url": "/posts/singular-value-decompositon/",
    "categories": "",
    "tags": "algorithms, machine-learning, unsupervised-learning, dimensionality-reduction, from-scratch",
    "date": "2024-09-13 00:00:00 +0300",
    





    
    "snippet": "Singular Value Decomposition (SVD) is a mathematical technique that factorizes a matrix $A$ into three simpler matrices: $U$, $Œ£$, and $V^T$. This decomposition allows us to express the original ma...",
    "content": "Singular Value Decomposition (SVD) is a mathematical technique that factorizes a matrix $A$ into three simpler matrices: $U$, $Œ£$, and $V^T$. This decomposition allows us to express the original matrix as a product of these matrices, which makes it easier to analyze and manipulate.The formula for SVD is:\\[A = U Œ£ V^T\\]Where:  $A$ is an $m√ón$ matrix,  $U$ is an $m√óm$ orthogonal matrix (left singular vectors),  $Œ£$ is an $m√ón$ diagonal matrix (singular values),  $V^T$ is the transpose of an $n√ón$ orthogonal matrix (right singular vectors).This decomposition has a range of applications in unsupervised learning (like PCA), recommendation systems (like Netflix and Amazon), and Google‚Äôs PageRank algorithm.Components of SVD  Matrix $U$ (Left Singular Vectors) : $U$ contains the left singular vectors of matrix $A$. These vectors are orthogonal,meaning all of its columns are linearly independent, and describe the directions in the original space that are most important. These are the eigenvectors of $AA^T$.  Matrix $Œ£$ (Singular Values): The diagonal matrix $Œ£$ contains the singular values, which represent the importance or strength of each component. The singular values are the square roots of the eigenvalues of $A^TA$ (or $AA^T$) and are always non-negative. This tells us how important each direction is.  Matrix $V$ contains the right singular vectors of $A$, and when transposed (as $V^T$), it tells us how the data points are distributed in the feature space. These are the eigenvectors of $A^TA$.Each of these matrices($U,\\Sigma,V$) helps us understand the original matrix in a deeper way. It‚Äôs like taking apart a machine to see how all the parts work together!Mathematical Foundation of SVDSVD is closely related to eigenvalue decomposition, which you may have encountered before. To understand this, let‚Äôs recall that the eigenvectors and eigenvalues of a matrix $A$ are solutions to the equation:\\[Av=Œªv\\]Where $v$ is an eigenvector and $Œª$ is its corresponding eigenvalue. SVD builds on this by decomposing $A$ into three matrices based on eigenvalues and eigenvectors of $A^TA$ and $AA^T$.Eigenvalue Decomposition and SVD:  Eigenvectors of $AA^T$ form the columns of $U$.  Eigenvectors of $A^TA$ form the columns of $V$.  The square roots of eigenvalues of $A^TA$ (or $AA^T$) form the singular values in $Œ£$.Eigenvalues and Eigenvectors: What Are They?It may be trueTo understand SVD, we need to know about eigenvalues and eigenvectors. These are concepts from linear algebra that help us understand transformations (how things like matrices can stretch or rotate a shape).      Eigenvectors are special vectors that don‚Äôt change direction when a matrix is applied to them. They can stretch or shrink but always point in the same direction.        Eigenvalues tell us how much the eigenvectors stretch or shrink. A large eigenvalue means the vector stretches a lot, while a small eigenvalue means it shrinks.  Imagine pushing on a piece of elastic. The eigenvector is the direction in which the elastic stretches, and the eigenvalue is how far it stretches.In SVD, the eigenvalues are connected to the singular values, and the eigenvectors make up the U and V matrices. Together, they give us a way to understand the directions in which the matrix ‚Äúpulls‚Äù the data.If you want to learn more about eigen vectors and eigen values, i highly recommended you to watch 3Blue1Brown‚Äôs videoImage Source  WikimediaSubspaces and Dimensions: What Are We Doing with the Data?In simple terms, SVD helps us break down a complex dataset into smaller, simpler parts that are easier to work with.      Subspaces: A subspace is like a lower-dimensional version of a space. If we have a bunch of data points in 3D space (like the x, y, z coordinates), we can use SVD to project those points onto a 2D plane (like x, y coordinates), which makes the data easier to work with.        Dimensionality Reduction: Sometimes, we have too many dimensions (features) in our data, and that makes things complicated. SVD allows us to reduce the number of dimensions while keeping the most important information. This is called dimensionality reduction.  How Does SVD Help in Machine Learning?SVD is often used in unsupervised learning techniques for machine learning. One of its main uses is to reduce the dimensionality of a dataset.Why is Dimensionality Reduction Important?In high-dimensional datasets (like images, text, or big datasets with lots of features), a lot of the information might be redundant or unimportant. Reducing the number of dimensions can:  Speed up computation.  Make models easier to interpret.  Prevent the curse of dimensionality, which can cause models to overfit.By keeping only the most important singular values, we can reduce the number of features in the dataset while still keeping the most important information.Applications of SVD in Real-World Problems      Recommendation Systems (Netflix, Amazon):      When you watch a movie or buy something online, companies like Netflix and Amazon use SVD to analyze your preferences and recommend other movies or products. They decompose a user-item interaction matrix using SVD and predict what you might like based on similar patterns.        Google PageRank:  Google uses SVD in its PageRank algorithm, which helps rank web pages by their importance. It analyzes how web pages are linked and uses SVD to find patterns in those links.        Image Compression:      SVD can also be used to compress images. By decomposing the image into singular values and truncating the less important ones, we can reduce the file size without losing much visual quality.  Projection of  Principal Component1.gifSVD in Matrix FormGiven a matrix $A$, the SVD is:\\[A = U \\Sigma V^T\\]Where $U$, $Œ£$, and $V^T$ look like:\\[\\rightarrow  U = [u_1‚Äã,u_2‚Äã,...,u_m‚Äã ]_{1xm} \\ \\text{(left singular vectors)}\\]\\[\\rightarrow \\ \\Sigma = diag(œÉ_1,œÉ_2,...,œÉ_k) \\ \\text{(singular values)}\\]\\[\\rightarrow \\ V^T =\\begin{bmatrix} v_1^T \\\\ v_2^T \\\\ \\vdots \\\\ v_n^T \\end{bmatrix} \\text{(right singular vectors)}\\]Key Properties:  $U$ and $V$ are orthogonal matrices, meaning their columns are unit vectors and $U^TU=I$ and $V^TV=I$  $Œ£$ contains non-negative singular values $œÉ_1‚â•œÉ_2‚â•‚Ä¶‚â•œÉ_k‚â•0$An Example of SVD with Detailed CalculationsLet‚Äôs work through a simple numerical example to demonstrate how to calculate the SVD of a matrix. Consider a matrix:\\[A = \\begin{bmatrix} 1 &amp; 2 \\\\3 &amp; 3 \\end{bmatrix}\\]Step 1: Compute $A^TA$ and $AA^T$\\[A A^T = \\begin{bmatrix} 1 &amp; 2 \\\\3 &amp; 3 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 3 \\\\2 &amp; 3 \\end{bmatrix} = \\begin{bmatrix} 5 &amp; 9 \\\\9 &amp; 18 \\end{bmatrix}\\]\\[A^T A = \\begin{bmatrix} 1 &amp; 3 \\\\2 &amp; 3 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\\\3 &amp; 3 \\end{bmatrix} = \\begin{bmatrix} 10 &amp; 11 \\\\11 &amp; 13 \\end{bmatrix}\\]Step 2: Compute Eigenvalues\\[\\text{det}(A A^T - \\lambda I) = 0\\]\\[A A^T = \\begin{bmatrix} 1 &amp; 2 \\\\3 &amp; 3 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 3 \\\\2 &amp; 3 \\end{bmatrix} = \\begin{bmatrix} 5 &amp; 9 \\\\9 &amp; 18 \\end{bmatrix}-\\begin{bmatrix} \\lambda &amp; 0 \\\\ 0 &amp; \\lambda\\end{bmatrix}\\]\\[\\text{det} \\begin{bmatrix} 5 - \\lambda &amp; 9 \\\\9 &amp; 18 - \\lambda \\end{bmatrix} = (5 - \\lambda)(18 - \\lambda) - 81 = 0\\]\\[(5 - \\lambda)(18 - \\lambda) - 81 = \\lambda^2 - 23\\lambda + 9 = 0\\]  $ \\text(roots \\ of \\ an \\ equation) = \\Large \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $\\[\\lambda = \\frac{23 \\pm \\sqrt{23^2 - 4 \\cdot 1 \\cdot 9}}{2 \\cdot 1} = \\frac{23 \\pm \\sqrt{493}}{2}\\]\\[\\lambda_1 = \\frac{23 + \\sqrt{493}}{2} \\approx 22.6\\]\\[\\lambda_2 = \\frac{23 - \\sqrt{493}}{2} \\approx 0.4\\]Step 3: Calculate Singular ValuesThe singular values are the square roots of the eigenvalues$\\sigma_1 = \\sqrt \\lambda_1 = \\sqrt 22.6  \\approx 4.76 \\\\ \\ \\sigma_2 = \\sqrt \\lambda_2 = \\sqrt 0.4 \\approx 0.63$\\[\\Sigma = \\begin{bmatrix} 4.76 &amp; 0 \\\\0 &amp; 0.63 \\end{bmatrix}\\]Step 4: Compute Eigenvectors of $AA^T$ ($U$)Let‚Äôs calculate eigenvector for first eigen value $Œª_1 = 22.6:$\\[A A^T - \\lambda_1 I = \\begin{bmatrix} 5 - 22.6 &amp; 9 \\\\9 &amp; 18 - 22.6 \\end{bmatrix} = \\begin{bmatrix} -17.6 &amp; 9 \\\\9 &amp; -4.6 \\end{bmatrix}\\]\\[\\begin{bmatrix} -17.6 &amp; 9 \\\\9 &amp; -4.6 \\end{bmatrix} \\begin{bmatrix} u_1 \\\\u_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\0 \\end{bmatrix}\\]\\[-17.6\\cdot u_1 + 9\\cdot u_2 = 0\\]\\[u_2 = \\frac{17.6}{9} \\times u_1 \\approx 1.96 \\times u_1\\]\\[\\| u \\| = \\sqrt{u_1^2 + u_2^2} = \\sqrt{u_1^2 + (1.96 \\cdot u_1)^2} = u_1 \\sqrt{1 + 1.96^2} = u_1 \\times \\sqrt{4.82}\\]\\[u_1 = \\frac{1}{\\sqrt{4.82}} \\approx 0.46\\]\\[u_2 = 1.96 \\times 0.46 \\approx 0.89\\]\\[U_1 = \\begin{bmatrix} 0.46 \\\\ 0.89 \\end{bmatrix}\\]Calculate other eigen vector for eigen value  $\\lambda_2=0.4$:\\[A A^T - \\lambda_2 I = \\begin{bmatrix} 5 - 0.4 &amp; 9 \\\\9 &amp; 18 - 0.4 \\end{bmatrix} = \\begin{bmatrix} 4.6 &amp; 9 \\\\9 &amp; 17.6 \\end{bmatrix}\\]\\[\\begin{bmatrix} 4.6 &amp; 9 \\\\9 &amp; 17.6 \\end{bmatrix} \\begin{bmatrix} u_1 \\\\u_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\0 \\end{bmatrix}\\]\\[4.6 \\cdot u_1 + 9 \\cdot u_2 = 0\\]\\[u_2 = -\\frac{4.6}{9} u_1 \\approx -0.51 \\times u_1\\]\\[\\| u \\| = \\sqrt{u_1^2 + u_2^2} = \\sqrt{u_1^2 + (-0.51 u_1)^2} = u_1 \\sqrt{1 + 0.26} = u_1 \\times \\sqrt{1.26}\\]\\[u_1 = \\frac{1}{\\sqrt{1.26}} \\approx 0.89\\]\\[u_2 = -0.51 \\times 0.89 \\approx -0.45\\]\\[U_2 = \\begin{bmatrix} 0.89 \\\\ -0.45 \\end{bmatrix}\\]\\[U = \\begin{bmatrix} 0.46 &amp; 0.89 \\\\0.89 &amp; -0.45 \\end{bmatrix}\\\\text(Left\\ Singular\\ Values)\\]Step 5:  Compute Eigenvectors of $A^TA$ ($V$)  Eigenvalues are same for both $U$ and $V$ matrices, we already have calculated eigenvalues for $U$ matrix. So there is no need to re-calculate again. However, for proving that their eigen values are same, lets re-calculate only determinant of $A^TA$.\\[\\text Question :: det(A^TA-\\lambda I) = det(AA^T-\\lambda I) = \\lambda^2 - 23\\lambda + 9\\]\\[A^T A - \\lambda I= \\begin{bmatrix} 1 &amp; 3 \\\\2 &amp; 3 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\\\3 &amp; 3 \\end{bmatrix} - \\lambda I = \\begin{bmatrix} 10 &amp; 11 \\\\11 &amp; 13 \\end{bmatrix}-\\begin{bmatrix} \\lambda &amp; 0 \\\\ 0 &amp; \\lambda\\end{bmatrix}\\]\\[\\text{det} \\begin{bmatrix} 10 - \\lambda &amp; 11 \\\\11 &amp; 13 - \\lambda \\end{bmatrix} = (10 - \\lambda)(13 - \\lambda) - 121 =0\\]\\[(5 - \\lambda)(18 - \\lambda) - 121 = \\lambda^2 - 23\\lambda + 9 = 0\\]  As you can see above, both determinant are same. So there is no need to re-calculate eigenvalues again. We know that eigen value $\\lambda_1$ = 22.6 and $\\lambda_2$ = 0.4Let‚Äôs calculate eigenvector for first eigen value $Œª_1 = 22.6:$\\[A A^T - \\lambda_1 I = \\begin{bmatrix} 10 - 22.6 &amp; 11 \\\\11 &amp; 13 - 22.6 \\end{bmatrix} = \\begin{bmatrix} -12.6 &amp; 11 \\\\11 &amp; -9.6 \\end{bmatrix}\\]\\[\\begin{bmatrix} -12.6 &amp; 11 \\\\11 &amp; -9.6 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\0 \\end{bmatrix}\\]\\[-12.6\\cdot v_1 + 11\\cdot v_2 = 0\\]\\[v_2 = \\frac{12.6}{11} \\times v_1 \\approx 1.45 \\times v_1\\]\\[\\| v \\| = \\sqrt{v_1^2 + v_2^2} = \\sqrt{v_1^2 + (1.45 \\cdot v_1)^2} = v_1 \\sqrt{1 + 1.45^2} = v_1 \\times \\sqrt{2.31}\\]\\[v_1 = \\frac{1}{\\sqrt{2.31}} \\approx 0.65\\]\\[v_2 = 1.45 \\times 0.65 \\approx 0.95\\]\\[V_1 = \\begin{bmatrix} 0.65 \\\\ 0.95 \\end{bmatrix}\\]Calculate other eigen vector for eigen value  $\\lambda_2=0.4$:\\[A A^T - \\lambda_2 I = \\begin{bmatrix} 10 - 0.4 &amp; 11 \\\\11 &amp; 13 - 0.4 \\end{bmatrix} = \\begin{bmatrix} 9.6 &amp; 11 \\\\11 &amp; 12.6 \\end{bmatrix}\\]\\[\\begin{bmatrix} 9.6 &amp; 11 \\\\11 &amp; 12.6 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\0 \\end{bmatrix}\\]\\[9.6 \\cdot v_1 + 11 \\cdot v_2 = 0\\]\\[v_2 = -\\frac{9.6}{11} v_1 \\approx -0.87 \\times v_1\\]\\[\\| v \\| = \\sqrt{v_1^2 + v_2^2} = \\sqrt{v_1^2 + (-0.87 v_1)^2} = v_1 \\sqrt{1 + 0.76} = v_1 \\times \\sqrt{1.76}\\]\\[v_1 = \\frac{1}{\\sqrt{1.76}} \\approx 1.32\\]\\[v_2 = -0.87 \\times 1.32 \\approx -1.15\\]\\[V_2 = \\begin{bmatrix} 1.32 \\\\ -1.15 \\end{bmatrix}\\]\\[V = \\begin{bmatrix} 0.65 &amp; 1.32 \\\\0.95 &amp; -1.45 \\end{bmatrix}\\\\text(Right\\ Singular\\ Values)\\]Step 6: VerificationNow, we applied singular value decomposition method for $A$ matrix.\\[U = \\begin{bmatrix} 0.65 &amp; 1.32 \\\\0.95 &amp; -1.45 \\end{bmatrix}, \\quad\\Sigma = \\begin{bmatrix} 4.76 &amp; 0 \\\\0 &amp; 0.63 \\end{bmatrix}, \\quadV = \\begin{bmatrix} 0.8 &amp; -0.6 \\\\0.6 &amp; 0.8 \\end{bmatrix}\\]Thus, the SVD decomposition is :\\[A = U \\Sigma V^T = \\begin{bmatrix} 0.65 &amp; 1.32 \\\\0.95 &amp; -1.45 \\end{bmatrix}\\cdot \\begin{bmatrix} 4.76 &amp; 0 \\\\0 &amp; 0.63 \\end{bmatrix}\\cdot \\begin{bmatrix} 0.8 &amp; -0.6 \\\\0.6 &amp; 0.8 \\end{bmatrix}^T\\]Truncated to $r$ - RankLet‚Äôs say we want to truncate or dimension reduction to rank 1 for A matrix. We can do this step by step:  Step 1: Analyze the Singular Value Matrix $\\Sigma$  Step 2: Truncate the Left Singular Matrix $U$  Step 3: Truncate the Right Singular Matrix $V^T$  Step 4: Reconstruct the Rank-1 Truncated Matrix $A_1$Step 1: Analyze the Singular Value Matrix $\\Sigma$The diagonal matrix $\\Sigma$ contains the singular values, which are non-negative and arranged in descending order. In this case, the matrix $\\Sigma$ is:\\[\\begin{bmatrix} 4.76 &amp; 0 \\\\0 &amp; 0.63 \\end{bmatrix}\\]For rank-1 truncation, we only keep the largest singular value, which is $4.76$. We discard the smaller singular value $0.63$.Thus, the truncated $\\Sigma_1$ matrix become:\\[\\Sigma_1 = \\begin{bmatrix} 4.76 &amp; 0 \\end{bmatrix} \\rightarrow first (i,j) \\rightarrow (1,1)\\]Step 2: Truncate the Left Singular Matrix $U$The matrix $U$ contains the left singular vectors. It is:\\[\\begin{bmatrix} 0.65 &amp; 1.32 \\\\0.95 &amp; -1.45 \\end{bmatrix}\\]Each column in $U$ corresponds to a singular value in $\\Sigma$. Since we are doing a rank-1 truncation and only keeping the largest singular value, we need to keep only the first column of $U$, which corresponds to the largest singular value $4.76$. Thus, the truncated $U_1$ matrix becomes:\\[U_1 = \\begin{bmatrix} 0.65 \\\\0.95 \\end{bmatrix}\\]Step 3: Truncate the Right Singular Matrix $V^T$The matrix $V^T$ contains the right singular vectors and is the transpose of $V$. It is:\\[V = \\begin{bmatrix} 0.8 &amp; -0.6 \\\\0.6 &amp; 0.8 \\end{bmatrix}\\rightarrow V^T = \\begin{bmatrix} 0.8 &amp; 0.6 \\\\-0.6 &amp; 0.8 \\end{bmatrix}\\]Similar to $U$, each row of $V^T$ corresponds to a singular value in $\\Sigma$. For rank-1 truncation, we keep only the first row of $V^T$, which corresponds to the largest singular value $4.76$.Thus, the truncated $V_1^T$ matrix becomes:\\[V_1^T = \\begin{bmatrix} 0.8 &amp; 0.6 \\end{bmatrix}\\]Step 4: Reconstruct the Rank-1 Truncated Matrix $A_1$Now, we can multiply the truncated matrices $U_1$, $\\Sigma_1$‚Äã, and $V_1^T$‚Äã to get the rank-1 approximation $A_1$The formula for this multiplication is:\\[A_1 = U_1 \\cdot \\Sigma_1 ‚Äã\\cdot V_1^T\\]Let‚Äôs calculate this step by step.  Multiply $U_1$‚Äã and $\\Sigma_1$ :\\[\\begin{bmatrix} 0.65 \\\\0.95 \\end{bmatrix}_{2\\times1}\\begin{bmatrix} 4.76 \\end{bmatrix}_{1\\times1}= \\begin{bmatrix} 0.65 \\times 4.76  \\\\0.95 \\times 4.76 \\end{bmatrix}_{2\\times1}= \\begin{bmatrix} 3.09 \\\\4.52 \\end{bmatrix}_{2\\times1}\\]  Let‚Äôs multiply $V_1^T$ with result ,so $(U_1 \\cdot \\Sigma_1) \\cdot V_1^T $\\[\\begin{bmatrix} 3.09 \\\\4.52 \\end{bmatrix}_{2\\times1}\\cdot \\begin{bmatrix} 0.8 &amp; 0.6 \\end{bmatrix}_{1\\times2}= \\begin{bmatrix} 3.09 \\times 0.8 &amp; 3.09 \\times 0.6   \\\\4.52 \\times 0.8 &amp; 4.52 \\times 0.6 \\end{bmatrix}_{2\\times2}= \\begin{bmatrix} 2.47 &amp; 1.85\\\\3.62 &amp; 2.71\\end{bmatrix}_{2\\times2}\\]The rank-1 truncated matrix $A_1$‚Äã is:\\[A_1 = \\begin{bmatrix} 2.47 &amp; 1.85\\\\3.62 &amp; 2.71\\end{bmatrix}_{2\\times2}\\]Step-by-Step SVD in Python  Install NumPy to work with matrices : pip install numpy  Perform SVD with NumPyimport numpy as np# Create a simple matrixA = np.array([[1, 2],              [3, 3]])# Perform Singular Value DecompositionU, S, VT = np.linalg.svd(A)print(\"U Matrix:\")print(U)print(\"\\nSingular Values:\")print(S)print(\"\\nV^T Matrix:\")print(VT)  Reconstruct Matrix# Reconstruct the original matrixS_diag = np.diag(S)A_reconstructed = U @ S_diag @ VTprint(\"\\nReconstructed Matrix:\")print(A_reconstructed)  Truncated SVD (for rank-1)# Truncate to 1 dimensionsk = 1U_k = U[:, :k]S_k = np.diag(S[:k])VT_k = VT[:k, :]# Reconstruct with reduced dimensionsA_reduced = U_k @ S_k @ VT_kprint(\"\\nReduced Matrix:\")print(A_reduced)  Last but not least, you can use Singular Value Decomposition with scikit-learn Truncated_SVD.ConclusionSingular Value Decomposition is a powerful tool in the data scientist‚Äôs toolbox, enabling dimensionality reduction, latent feature extraction, and matrix approximation. Its applications range from recommendation systems to graph algorithms and data compression. Understanding the underlying mathematics and how to implement SVD in Python allows you to harness the full power of this technique in machine learning and data science workflows.By leveraging SVD, you can reduce computational complexity and make your models more efficient, interpretable, and faster!"
  },
  
  {
    "title": "Guide to Installing Apache Cassandra on Ubuntu",
    "url": "/posts/install-apache-cassandra/",
    "categories": "",
    "tags": "tutorials, setup, devops, big-data",
    "date": "2024-09-01 00:00:00 +0300",
    





    
    "snippet": "In this tutorial, we will try to follow original documentation.There still might be different codes a bit üôÇ According to documentation you can install cassandra three different ways. In this post d...",
    "content": "In this tutorial, we will try to follow original documentation.There still might be different codes a bit üôÇ According to documentation you can install cassandra three different ways. In this post dockerize version and debiand package version is shown.Method 1 - Docker InstallationSimple Setup  Step 1 ‚Äî Pull cassandra repository from docker hub docker pull cassandra:latest  Step 2 ‚Äî Run docker container docker run ‚Äîname cass_cluster cassandra:latest  Step 3 ‚Äî Use cassandra in the container docker exec -it cass_cluster cqlshMore Flexible but Complex SetupYou can install cassandra with docker-compose instead only docker commands. I write a docker-compose.yml file like below. Then run file docker-compose up -d to start my cassandra cluster system.version: '3.8'services:  cassandra-dc1-node1:    image: cassandra:4.0    container_name: cassandra-dc1-node1    environment:      - CASSANDRA_CLUSTER_NAME=MyCluster      - CASSANDRA_DC=Datacenter1      - CASSANDRA_RACK=Rack1      - CASSANDRA_LISTEN_ADDRESS=cassandra-dc1-node1      - CASSANDRA_RPC_ADDRESS=0.0.0.0      - CASSANDRA_BROADCAST_ADDRESS=cassandra-dc1-node1      - CASSANDRA_SEEDS=cassandra-dc1-node1,cassandra-dc2-node1    ports:      - \"9043:9042\"  # CQL port      - \"7001:7000\"  # Inter-node port      - \"7200:7199\"  # JMX port    volumes:      - cassandra-data-dc1-node1:/var/lib/cassandra    networks:      - cassandra-network    healthcheck:      test: [\"CMD\", \"nodetool\", \"status\"]      interval: 30s      retries: 3      start_period: 60s      timeout: 30s  cassandra-dc2-node1:    image: cassandra:4.0    container_name: cassandra-dc2-node1    environment:      - CASSANDRA_CLUSTER_NAME=MyCluster      - CASSANDRA_DC=Datacenter2      - CASSANDRA_RACK=Rack1      - CASSANDRA_LISTEN_ADDRESS=cassandra-dc2-node1      - CASSANDRA_RPC_ADDRESS=0.0.0.0      - CASSANDRA_BROADCAST_ADDRESS=cassandra-dc2-node1      - CASSANDRA_SEEDS=cassandra-dc1-node1,cassandra-dc2-node1    ports:      - \"9044:9042\"  # CQL port      - \"7002:7000\"  # Inter-node port      - \"7201:7199\"  # JMX port    volumes:      - cassandra-data-dc2-node1:/var/lib/cassandra    networks:      - cassandra-network    healthcheck:      test: [\"CMD\", \"nodetool\", \"status\"]      interval: 30s      retries: 3      start_period: 60s      timeout: 30svolumes:  cassandra-data-dc1-node1:    driver: local  cassandra-data-dc2-node1:    driver: localnetworks:  cassandra-network:    driver: bridgeYou can increase node numbers and connect these nodes to same or different racks. Also you can increase your data centers count. I tried to 2 dc, 2 node per dc, but my system can not work because of hardware issues. Let‚Äôs say you can compose up successfully, how can you know it is working right? You can look that this mini check_cassandra_health.sh script:#!/bin/bash# List of Cassandra containerscontainers=(\"cassandra-dc1-node1\" \"cassandra-dc2-node1\")for container in \"${containers[@]}\"; do  echo \"Checking status for $container\"  docker-compose exec -T \"$container\" nodetool status  echodoneIf you run this script sudo ./check_cassandra_health.sh, you should see stdout like in Figure 1.  If you show a langError, probably cassandra nodes are not up still. You just still a couple minutes, then try again.Figure 1.Docker Cassandra Setup VerifyMethod 2- Debian Package InstallationFirst of all, there are some prerequisities so please verify and install them if you needStep 1:  Prerequisities Verification  sudo apt install default-jre  sudo apt install default-jdk  sudo apt install curl  sudo apt-get updateAfter this verification and installation step, let‚Äôs continue to install cassandra.Step 2: Add Cassandra Repository  sudo su  curl -o /etc/apt/keyrings/apache-cassandra.asc https://downloads.apache.org/cassandra/KEYS  echo \"deb [signed-by=/etc/apt/keyrings/apache-cassandra.asc] https://debian.cassandra.apache.org 40x main\" &gt; /etc/apt/sources.list.d/cassandra.list  apt-get updateStep 3: Install Cassandra  apt install cassandra -yStep 4: Setup VerificationNow, let‚Äôs check setup is done successfully sudo systemctl status cassandra. Also you can check your clusters health by nodetool status. Stdout should be like in Figure 2.Figure 2. Setup VerificationStep 5: Enable Service &amp; ConfigurationTo enable service for cassandra sudo systemctl enable cassandra should be run. There are some configurations like cluster_name, broadcast_address, seeds, CQL port, Inter-node port, JMX port ,listen_address etc. You can update these parameters in cassandra.yml. If you do not know where config file is, you can type sudo find / -name cassandra.yaml. For example in my system, it is located in /etc/cassandra .  Cluster name: Related nodes should be same cluster name.  Seeds: A node‚Äôs discover range for other nodes  If cluster names are different, then nodes can not be connected each other properly when seeds are same.Possible Bugs  Your server locale should be en_US.UTF-8, locale  Ports should be unused, you can check eg. netstat -tulnp | grep 7199  Ports can be allowed, ufw allow 7199/tcp"
  },
  
  {
    "title": "Jupyter Lab Installation on Ubuntu",
    "url": "/posts/jupyter-lab/",
    "categories": "",
    "tags": "tutorials, setup, devops, big-data",
    "date": "2024-08-31 00:00:00 +0300",
    





    
    "snippet": "IntroductionJupyterLab is your go-to environment for interactive data analysis, blending notebooks, text editors, and terminals into one seamless workspace. It‚Äôs like having a digital laboratory wh...",
    "content": "IntroductionJupyterLab is your go-to environment for interactive data analysis, blending notebooks, text editors, and terminals into one seamless workspace. It‚Äôs like having a digital laboratory where you can experiment, visualize, and document all in real-time. Firstly, this guide will walk you through a hard setup, then we will move on easy setup to get you started swiftly. Using Docker Compose, you can quickly set up JupyterLab on Ubuntu without the usual setup headaches.JupyterLab: The Ultimate Workspace for Data ScienceA Versatile Workspace for Every LanguageImagine JupyterLab as your go-to lab for data science and research‚Äîa place where you can code, visualize data, and jot down notes all in one spot. It supports multiple programming languages like Python, R, and Julia, making it incredibly flexible for different tasks. But, juggling between these languages can get tricky, especially since some have better support than others.An Integrated Environment for Smooth WorkflowsOne of the best things about JupyterLab is how it brings everything together. You can write code, see your data visualizations, and take notes all in the same workspace. This integration helps you work more efficiently, but if your project gets big, the interface can start to feel a bit crowded, and you might miss out on some of the advanced features you get with traditional IDEs.Customizable with ExtensionsJupyterLab‚Äôs extensibility is a real game-changer. You can add various plugins to tailor the environment to your needs, whether it‚Äôs for version control, enhancing visualizations, or other functionalities. However, not all plugins are perfectly maintained, and some might not work with newer versions of JupyterLab, so you might need to tweak things yourself.Interactive Visualizations Made EasyOne of the coolest features of JupyterLab is its ability to create interactive visualizations. You can use libraries like Plotly or Bokeh to make your data come alive right in the notebook. Just keep in mind that if you‚Äôre working with very large datasets or complex visualizations, things might slow down a bit, which can affect your workflow.Scalability and PerformanceJupyterLab is also great for scaling up. Whether you‚Äôre using Docker or running it in the cloud, it helps ensure that your setup remains consistent across different environments. However, while it scales well, handling massive datasets might cause performance issues, and setting up the necessary infrastructure can require some extra work.  Overall, JupyterLab is a fantastic tool that simplifies data science and research, making complex tasks feel more manageable and engaging. After examine pros cons of jupyter lab let‚Äôs continue with installation approaches..Method 1 : Manual Installation on Linux DebianFigure 1  Step 1 ‚Äî Check python version python --version or  python3 --version. If it is not installed, then install.  Step 2 ‚Äî Check pip version pip --version If it is not installed then install pip sudo apt install -y python3-pip . If it is already installed run sudo apt install --upgrade -y python3-pip  and pip install --upgrade pip commands for updating.  Step 3 ‚Äî Install virtualenv package pip install virtualenv to create virtual environment. Then create virtual environment like virtualenv jupyter-venv . After that  activate the virtual environment source jupyter-env/bin/activate .  Figure 2  Step 4 ‚Äî Install Jupyter Lab pip install jupyterlab .  Step 5 ‚Äî Find jupyter-lab path find ~ -name jupyter-lab  Step 6 ‚Äî Add PATH echo \"export PATH=$PATH:~/jupyter-env/bin/\" &gt;&gt; ~/.bashrc  and after that reload bashrc source ~/.bashrc . Then if you need, activate jupyter-env again.  Step 7 ‚Äî Set password  jupyter-lab password .This step set password and also generate a json file include hashed password.Figure 3  Voila! Now,  you can run your Jupyter Lab with jupyter lab  or jupyter lab --ip=\"localhost\" --port=8888 --no-browser --allow-root .Figure 4You can login Jupyter Lab localhost:8888 . If you changed ip or port address or port is already used you can see in logs after started Jupyter Lab like in Figure 4.  Let‚Äôs do better this installation.  Step 8 ‚Äî Let‚Äôs control our configurations on a file, so we can manage Jupyter Lab easier          Step 8.1 ‚Äî Generate config file jupyter-lab --generate-config      Step 8.2 ‚Äî  Generated password is written a jupyter_server_config.json  file. Hashed password will be use next step.              Step 8.3 ‚Äî Open generated  jupyter_lab_config.py file and update these lines according to your needs.          c.ServerApp.ip = 'your-server-ip' # default localhost  c.ServerApp.open_browser = False # default false  c.ServerApp.password = 'hashed_password' # generated hashed password  c.ServerApp.port = 8888 # default port is 8888                    Figure 5  Step 9 ‚Äî Create a service for Jupyter Lab,          Step 9.1 ‚Äî Create a services file with sudo nano /etc/systemd/system/jupyter-lab.service      Step 9.2 ‚Äî Configure this file according the your username and jupyter-lab environment path etc. You can learn jupyter lab path with find ~ -name jupyter-lab , your user name id -un ,your group name id -gn          [Unit]  Description = JupyterLab Service      [Service]  User = umits  Group = umits  Type = simple  WorkingDirectory = /home/umits/  ExecStart = /home/umits/jupyter-env/bin/jupyter-lab --config=/home/umits/.jupyter/jupyter_lab_config.py  Restart = always  RestartSec = 10      [Install]  WantedBy = multi-user.target      Figure 6- Step 9.3 ‚Äî Reload system daemon `sudo systemctl daemon-reload`- Step 9.4 ‚Äî Start your new Jupyter Lab Service `sudo systemctl start jupyter-lab.service`- Step 9.5 ‚Äî Check your service status is running `sudo systemctl status jupyter-lab.service`Figure 7Voila! Another milestone is completed, I think this installation is enough for local development/projects. If you want to work remotely with your team/organization, you should add SSL certifi files and do remote port forwarding with nginx etc. There are many resources on the web üôÇ I might be update this post like included with them in the future.Method 2: Dockerized JupyterLab SetupIf you want a simpler setup that avoids managing dependencies and conflicts, you can containerize JupyterLab using Docker. Docker allows you to run JupyterLab in an isolated environment, making it easier to install, manage, and share.  Step 1 ‚Äî Check updates sudo apt update  Step 2 ‚Äî Install docker sudo apt install docker.io  Step 3 ‚Äî Install docker-compose sudo apt install docker-compose  Step 4 ‚Äî Create docker-compose.yml file  Step 5 ‚Äî Run docker-compose up -d and you can reach your JupyterLab localhost:8889Figure 8version: '3.8'services:  jupyterlab:    image: jupyter/base-notebook:latest    container_name: jupyterlab    ports:      - \"8889:8888\"    volumes:      - ./notebooks:/home/jovyan/work    environment:      - JUPYTER_ENABLE_LAB=yes      - GRANT_SUDO=yes      - JUPYTER_TOKEN=cokgizlitoken  # Set the token here    command: start.sh jupyter lab # --LabApp.token='' # log without tokenJupyterLab Installation: Manual vs. DockerizedManual Installation: Full Control and FlexibilityManual installation of JupyterLab offers full control over your environment. You can customize every aspect of the setup, ensuring that it fits your specific needs. With direct access to system resources, you can optimize performance and integrate seamlessly with existing applications. Additionally, there‚Äôs no additional overhead from container layers, which can make it more efficient in certain scenarios.However, managing dependencies manually can be complex, leading to potential conflicts between packages. This setup also poses a higher risk of version clashes and makes it harder to isolate different environments, which can complicate the management of multiple projects.Dockerized Setup: Convenience and ConsistencyA Dockerized setup provides an isolated environment that prevents dependency conflicts, making it easier to manage different projects without worrying about package clashes. Sharing and deploying JupyterLab across various machines becomes straightforward, and managing versions and updates is simplified with Docker. Furthermore, Docker‚Äôs ability to scale with multiple containers can handle increased workloads efficiently.On the flip side, containerization introduces a slight performance overhead compared to a native installation. Additionally, you‚Äôll need to have some knowledge of Docker and Docker Compose to set things up and maintain the environment. Also, direct access to system resources, like hardware acceleration, is limited within containers, which might impact performance for specific tasks.  Choosing between manual installation and a Dockerized setup depends on your needs for control versus convenience, performance versus isolation, and the complexity you‚Äôre willing to manage.ConclusionJupyterLab is a versatile tool that provides an interactive computing environment for data science, machine learning, and research. Depending on your needs, you can set it up manually or in a Dockerized environment.  Manual Installation: This is a good choice if you want more control over your environment and don‚Äôt mind managing dependencies manually. However, it can be complex and may require extra care when dealing with dependencies or configurations.  Dockerized Setup: This is the best choice for isolating your JupyterLab environment and ensuring that it runs consistently across different machines or setups. It‚Äôs easy to share with others and avoids dependency conflicts. However, it does add some overhead, such as managing Docker and storage.Whichever method you choose, JupyterLab is an indispensable tool for modern computational workflows. By providing an all-in-one interface for coding, documentation, and visualization, JupyterLab continues to empower developers, data scientists, and researchers to explore their data and share insights effectively."
  },
  
  {
    "title": "Apache Kafka on Ubuntu | Architecture, Installation, and Usage",
    "url": "/posts/apache_kafka/",
    "categories": "",
    "tags": "basics, tutorials, devops, big-data, streaming, apache",
    "date": "2024-08-30 00:00:00 +0300",
    





    
    "snippet": "IntroductionApache Kafka has become one of the most powerful tools for distributed streaming and messaging in large-scale data ecosystems. Whether you‚Äôre working with real-time data processing, eve...",
    "content": "IntroductionApache Kafka has become one of the most powerful tools for distributed streaming and messaging in large-scale data ecosystems. Whether you‚Äôre working with real-time data processing, event-driven architectures, or data pipelines, Kafka offers a reliable, scalable, and fault-tolerant platform that integrates seamlessly into modern architectures.Kafka was originally developed by LinkedIn, later open-sourced in 2011, and is now maintained by the Apache Software Foundation. Kafka is designed to handle vast amounts of data and distribute it across multiple systems efficiently, making it a key player in big data environments.What‚Äôs New with KafkaKafka continues to evolve with every release. Key recent improvements include:  Kafka Raft Consensus (KRaft): Kafka has introduced a Zookeeper-free mode, aiming to replace Zookeeper with an internal Raft protocol. This simplifies Kafka‚Äôs architecture and operational complexity.  Improved Consumer Rebalancing: Kafka has made significant progress in improving consumer rebalancing efficiency, reducing downtime during rebalance events.  Tiered Storage: Kafka now supports tiered storage for longer retention periods at reduced costs.  Kafka Streams: The Kafka Streams API is becoming more powerful, offering new ways to process streams directly within Kafka without external tools.Let‚Äôs assume we need a system collect logs and videos share/watch. If we want to build this with Apache kafka, we can draw a simple diagram like below.graph TB    subgraph \"Producers\"        P1[Producer 1]        P2[Producer 2]    end    subgraph \"Kafka Cluster\"        subgraph \"Broker 1\"            B1V[Videos Topic]            B1L[Logs Topic]            subgraph \"B1V Partitions\"                B1VP1[Partition 1]                B1VP2[Partition 2]            end            subgraph \"B1L Partitions\"                B1LP1[Partition 1]                B1LP2[Partition 2]            end        end        subgraph \"Broker 2\"            B2V[Videos Topic]            B2L[Logs Topic]            subgraph \"B2V Partitions\"                B2VP1[Partition 1]                B2VP2[Partition 2]            end            subgraph \"B2L Partitions\"                B2LP1[Partition 1]                B2LP2[Partition 2]            end        end    end    subgraph \"Consumer Groups\"        subgraph \"Consumer1\"            CG1L[LogsGroup]        end         subgraph \"Consumer2\"            CG2L[LogsGroup]        end         subgraph \"Consumer3\"            CG3V[MobileVideosGroup]        end         subgraph \"Consumer4\"            CG4V[DesktopVideosGroup]        end     end    P1 --&gt; B1V &amp; B1L    P2 --&gt; B2V &amp; B2L    B1VP1 &lt;--&gt; B2VP1    B1VP2 &lt;--&gt; B2VP2    B1LP1 &lt;--&gt; B2LP1    B1LP2 &lt;--&gt; B2LP2    B1L &amp; B2L --&gt; CG1L &amp; CG2L    B1V &amp; B2V --&gt; CG3V    B1V &amp; B2V --&gt; CG4V    classDef producer color:black,fill:#ffcccc,stroke:#ff6666,stroke-width:2px;    classDef broker color:black,fill:#ccffcc,stroke:#66ff66,stroke-width:2px;    classDef partition color:black,fill:#e6e6e6,stroke:#999999,stroke-width:2px;    classDef consumer color:black,fill:#ccccff,stroke:#6666ff,stroke-width:2px;    class P1,P2 producer;    class B1V,B1L,B2V,B2L broker;    class B1VP1,B1VP2,B1LP1,B1LP2,B2VP1,B2VP2,B2LP1,B2LP2 partition;    class CG1,CG2 consumer;  The system is designed to efficiently handle both logs and videos using Apache Kafka. Producers within the system collect various types of logs, including system logs, database logs, and crash reports, along with videos from desktop and mobile sources. This data is then sent to brokers within a Kafka cluster.In the Kafka cluster, logs and videos are organized into specific topics, which are further divided into partitions. These partitions help distribute the data across multiple brokers, ensuring the system is scalable and resilient to failures.Different consumer groups process the data once it is stored in Kafka. Logs are consumed by a group of consumers that share the same group ID, allowing consistent analysis across all log data. On the other hand, videos are consumed by separate consumer groups, each with a different group ID, ensuring that video streams are processed independently.This system allows for efficient processing of both logs and videos, ensuring data is managed and consumed effectively based on its type.Architecture OverviewAt its core, Kafka is a publish-subscribe messaging system (pub/sub) where Producers send messages to Brokers with topics, and Consumers retrieve those messages from Topics.  If we want to kafka act like pub-sub, we should set group-ids different for same topic. Otherwise we want to kafka work like queue we should set group-ids same for same topic.  In pub-sub format the message is published only once and consumed many times, whereas in queue format the message is published only once and consumed only once, after which the message is removed from the queue.Kafka is composed of the following core components:  Producer: Producers are clients that send data to Kafka. They push records (messages) to a specific topic within the Kafka cluster.  Consumer: Consumers read data from Kafka topics. They can subscribe to one or more topics and process incoming messages as needed.  Broker: A Kafka broker is a server that manages message storage and serves clients (producers and consumers). Brokers form a Kafka cluster, and data is distributed across them for load balancing and fault tolerance.  Topic: Topics are the main abstraction in Kafka. They represent a logical channel to which producers send messages, and consumers subscribe to them. Topics are partitioned, replicated across multiple brokers and each partition is an ordered sequence of records.  Partition: Each topic can be divided into multiple partitions to distribute load, enabling parallel processing and scalability .  Replication: To ensure fault tolerance, Kafka replicates partitions across multiple brokers. Replication ensures data durability even in the event of broker failure.  Offsets: A unique identifier assigned to each message within a partition. Consumers use offsets to track which messages have been processed.  Zookeeper/KRaft: Originally, Zookeeper was used for cluster coordination, but now KRaft mode is being adopted for better scalability and operational simplicity. (I will write another blog post about them.)  Cluster: A group of brokers working together. Data is distributed across brokers to ensure high availability and load balancing.Kafka‚Äôs log-based architecture ensures sequential writes to disk, minimizing overhead and providing low-latency throughput.Pipeline Working  Producer sends data (message/event):          A producer is an application or service that creates messages and sends them to Kafka. The producer sends data to a Kafka topic.      Partitioning: If the topic has multiple partitions, the producer decides which partition the message will go to. This can be based on the message‚Äôs key (deterministic partitioning) or a round-robin approach.      The producer sends messages to one or more Kafka brokers, which handle the topic‚Äôs partitions.        Data arrives at the Broker:          The broker receives the message from the producer and stores it in the appropriate partition within the topic. Kafka brokers are responsible for persisting the messages and managing the offsets.      Each partition acts as an ordered, immutable sequence of records that the broker maintains.        Replication:          Kafka supports replication for fault tolerance. The broker replicates data to other brokers based on the topic‚Äôs replication factor.      The leader of a partition replicates the data to its followers (replica brokers). If the leader fails, a follower takes over as the leader to ensure no data loss.        Offsets management:          Kafka maintains an offset for every message within a partition. This offset is a unique identifier that determines the position of the message within the partition. It acts as an index for tracking the position of the messages.      Each consumer in a consumer group tracks its own offset to know which message to read next. Offsets are committed by consumers to Kafka either automatically or manually, depending on the consumer‚Äôs configuration.        Consumer group reads data:          A consumer (or a group of consumers) subscribes to one or more topics and starts pulling data from Kafka brokers.      Consumer groups allow horizontal scaling, where each consumer in the group is assigned to read from a specific partition. This ensures that messages are processed in parallel but never by more than one consumer in a group.      Consumers can read messages sequentially from the assigned partition, starting from the offset where they last left off.        Error Handling and Fault Tolerance:          Kafka ensures fault tolerance through replication. If a broker crashes, the leader partition is automatically elected from the available replicas.      Kafka also allows consumers to re-read messages by resetting their offsets. This is useful for error recovery and reprocessing.        Acknowledgments:          When the message is successfully stored on the broker, Kafka sends an acknowledgment to the producer.      The producer can be configured to wait for acknowledgment from all replicas (strong durability) or just the leader (higher throughput but less durability).        Consumers process and commit offsets:          Once a consumer reads and processes the message, it can commit the offset to Kafka to record that the message has been processed. The next time the consumer reads from the partition, it will start at the next offset.      Committing offsets ensures that if a consumer crashes and restarts, it won‚Äôt reprocess already consumed messages.      Kafka Pipeline Diagramgraph TB    classDef producerStyle color:black,fill:#e0f7fa,stroke:#006064,stroke-width:2px;    classDef brokerStyle color:black,fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;    classDef consumerStyle color:black,fill:#c8e6c9,stroke:#388e3c,stroke-width:2px;    classDef errorStyle color:black,fill:#f8bbd0,stroke:#c2185b,stroke-width:2px;    %% nodes     A((\"&lt;b&gt;&lt;u&gt;Send Data&lt;/u&gt;&lt;/b&gt;    Producer sends     data to a Kafka topic\")):::producerStyle     B[(\"&lt;b&gt;&lt;u&gt;Partition &amp; Store&lt;/u&gt;&lt;/b&gt;    Broker receives and     stores data in the     correct partition\")]:::brokerStyle    C[(\"&lt;b&gt;&lt;u&gt;Replication&lt;/u&gt;&lt;/b&gt;    Cluster Data is     replicated to other     brokers for fault tolerance\")]:::brokerStyle    D[(\"&lt;b&gt;&lt;u&gt;Track Offset&lt;/u&gt;&lt;/b&gt;    Kafka assigns an    offset to each     message in a partition\")]:::brokerStyle    E(((\"&lt;b&gt;&lt;u&gt;Read Data&lt;/u&gt;&lt;/b&gt;    Consumers read     messages sequentially    from the partitions    they are assigned to    \"))):::consumerStyle    F(((\"&lt;b&gt;&lt;u&gt;Process Data&lt;/u&gt;&lt;/b&gt;    Consumers process    the messages and    commit the offsets    back to Kafka\"))):::consumerStyle    G{\"&lt;b&gt;&lt;u&gt;Error Handling&lt;/u&gt;&lt;/b&gt;    In case of broker     failure, replication      ensures no data      loss or downtime\"}:::errorStyle        %% directions     subgraph Producer        direction LR        A      end     subgraph Broker         direction LR        A -.-&gt; B        B -.-&gt; C        C -.-&gt; D    end     subgraph Consumer/Consumer Group        direction LR        D -.-&gt; E        E -.-&gt; F     end     subgraph Broker/Cluster        F -.-&gt; G     end    linkStyle 0 stroke:#006064,stroke-width:2px,stroke-dasharray:5,5;    linkStyle 1 stroke:#fbc02d,stroke-width:2px,stroke-dasharray:5,5;    linkStyle 2 stroke:#fbc02d,stroke-width:2px,stroke-dasharray:5,5;    linkStyle 3 stroke:#fbc02d,stroke-width:2px,stroke-dasharray:5,5;    linkStyle 4 stroke:#388e3c,stroke-width:2px,stroke-dasharray:5,5;    linkStyle 5 stroke:#c2185b,stroke-width:2px,stroke-dasharray:5,5;Message order may guarantee only per partition, but not for all messages within all topic. Image by Vladimir TopolevKafka Pipeline ExampleHere is an example to illustrate the working order in the Kafka pipeline:  Set up the Kafka environment using Docker with Zookeeper and multiple Kafka brokers  Create a Kafka topic named ‚Äòuser-events‚Äô with 3 partitions and a replication factor of 3  Create 5 producers and send a total of 20 messages to the ‚Äòuser-events‚Äô topic, distributing them randomly across partitions  Create a consumer group named ‚Äòconsumer-group-1‚Äô, read messages from the ‚Äòuser-events‚Äô topic, process them, and print to the screen  Run the system by first creating the topic, then starting the producers to send messages, and finally starting the consumer to read messagesInstall Kafka &amp; Python UsageYou can install kafka two ways, first you can install manually;download kafka, zookeeper and java open jdk. Second way is you can install kafka with docker. In this post, i will use docker version because it is easier than manual version.Kafka with ZookeeperApache Kafka with Zookeeper. Image by Vladimir TopolevFirst, you‚Äôll need a Docker setup to run Kafka with zookeeper. I installed kafka with zookeeper because KRaft configurations raise exception, and i did not figure out. In time i am planning to another blog post KRaft vs Zookeeper.Use the following docker-compose.yml file to install kafka with zookeeper:version: '3.7'services:  zookeeper:    image: confluentinc/cp-zookeeper:latest    environment:      ZOOKEEPER_CLIENT_PORT: 2181      ZOOKEEPER_SERVER_ID: 1    ports:      - \"2181:2181\"  kafka-1:    image: confluentinc/cp-kafka:latest    ports:      - \"9092:9092\"      - \"29092:29092\"    environment:      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL      KAFKA_ZOOKEEPER_CONNECT: \"zookeeper:2181\"      KAFKA_BROKER_ID: 1    depends_on:      - zookeeper  kafka-2:    image: confluentinc/cp-kafka:latest    ports:      - \"9093:9093\"      - \"29093:29093\"    environment:      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-2:19093,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093,DOCKER://host.docker.internal:29093      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL      KAFKA_ZOOKEEPER_CONNECT: \"zookeeper:2181\"      KAFKA_BROKER_ID: 2    depends_on:      - zookeeper  kafka-3:    image: confluentinc/cp-kafka:latest    ports:      - \"9094:9094\"      - \"29094:29094\"    environment:      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-3:19094,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094,DOCKER://host.docker.internal:29094      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL      KAFKA_ZOOKEEPER_CONNECT: \"zookeeper:2181\"      KAFKA_BROKER_ID: 3    depends_on:      - zookeeperKafka Package Install for PythonThe kafka-python package allows you to interact with Kafka in Python. Install the kafka-python package using pip:pip install kafka-pythonHere‚Äôs the Python code that demonstrates producing and consuming messages with Kafka using the kafka-python package:First of all, we need to create topic or topics. Let‚Äôs assume same example above user-events. Let‚Äôs create create_topics.py file.from kafka.admin import KafkaAdminClient, NewTopicdef create_topic(topic_name,num_partitions,replication_factor):    admin_client = KafkaAdminClient(        bootstrap_servers=['localhost:9092','localhost:9093','localhost:9094'],        client_id='test'    )    topic = NewTopic(        name=topic_name,        num_partitions=num_partitions,        replication_factor=replication_factor    )    admin_client.create_topics(new_topics=[topic], validate_only=False)    print(f\"Topic '{topic_name}' created with {num_partitions} partitions and replication factor {replication_factor}\")if __name__ == \"__main__\":    create_topic(topic_name = 'user-events',num_partitions = 3, replication_factor = 3)  Replication factor must not be bigger than number of our kafka replicas. In this example max. is 3.Then we need to send some messages with producers. So let‚Äôs create producer.py file.from kafka import KafkaProducerimport timeimport randomdef produce_messages():    bootstrap_servers = ['localhost:9092','localhost:9093','localhost:9094']    topic = 'user-events'    num_producers = 5    num_messages = 20    producers = [KafkaProducer(bootstrap_servers=bootstrap_servers) for _ in range(num_producers)]    for i in range(num_messages):        message = f\"User Signed Up {i}\".encode('utf-8')        producer = random.choice(producers)        partition = random.randint(0, 2)  # Randomly choose partition 0, 1, or 2        producer.send(topic, value=message, partition=partition)        print(f\"Message sent to topic '{topic}' on partition {partition}\")        time.sleep(1)  # Simulate a delay between messages    # Close all producers    for producer in producers:        producer.flush()        producer.close()if __name__ == \"__main__\":    produce_messages()  Partition must not be bigger than number of our kafka topic partition number. In this example max. is 3.Last but not least, we will create some consumers to read data from the topic are sended from producers. Let‚Äôs create consumer1.py and consumer2.py files.from kafka import KafkaConsumerimport time def consume_messages():    bootstrap_servers = ['localhost:9092','localhost:9093','localhost:9094']    topic = 'user-events'    group_id = 'consumer-group-1'    consumer = KafkaConsumer(        topic,        group_id=group_id,        bootstrap_servers=bootstrap_servers,        auto_offset_reset='earliest',        enable_auto_commit=True,        consumer_timeout_ms=1000    )    print(f\"Consumer 1 started reading \")    for message in consumer:        print(f\"Consumer 1 received message: {message.value.decode('utf-8')} read from partition {message.partition}\")           if __name__ == \"__main__\":    start = time.time()    while True:        consume_messages()        passed_time = time.time() - start         if passed_time &gt; 60:break Let‚Äôs create consumer2.py file:from kafka import KafkaConsumerimport time def consume_messages():    bootstrap_servers = ['localhost:9092','localhost:9093','localhost:9094']    topic = 'user-events'    group_id = 'consumer-group-1'    consumer = KafkaConsumer(        topic,        group_id=group_id,        bootstrap_servers=bootstrap_servers,        auto_offset_reset='earliest',        enable_auto_commit=False,        consumer_timeout_ms=1000    )    print(f\"Consumer 2 started reading\")    for message in consumer:        print(f\"Consumer 2 received message: {message.value.decode('utf-8')} from partition {message.partition}\")        consumer.commit()if __name__ == \"__main__\":    start = time.time()    while True:        consume_messages()        passed_time = time.time() - start         if passed_time &gt; 60:break Finally, let‚Äôs open 5 bash terminal and test it. Type in terminals with order :&gt; docker-compose up -d &gt; python create_topics.py &gt; python producer.py&gt; python consumer1.py  &gt; python consumer2.py   If you run consumer1.py and consumer2.py files at the same time, you can observe that some messages read from consumer1 and rest of messages read from consumer2. Because we set same group-id . I mean, it is queue formatted kafka example.  If you want to check topics etc. you can use codes below or look this documentationimport kafkaservers=['localhost:9092','localhost:9093','localhost:9094']admin_client = kafka.KafkaAdminClient(bootstrap_servers=servers)admin_client.list_topics() # list all topics admin_client.describe_topics() # look details of topics admin_client.delete_topics(topics=['user-events']) # delete topics Kafka is widely used in scenarios like ETL pipelines, event streaming, real-time analytics, log aggregation, and microservices communication.Advantages and DisadvantagesAdvantages:  Scalability: Kafka‚Äôs architecture allows you to easily scale producers, consumers, and brokers to handle growing workloads.  Fault Tolerance: With replication and partitioning, Kafka can survive broker failures without data loss.  High Throughput: Kafka is designed for high-throughput use cases, with low-latency data delivery.  Durability: Kafka persists messages to disk, ensuring data reliability.  Flexibility: Kafka supports both streaming and batch processing, making it adaptable to many use cases.Disadvantages:  Complex Setup: Kafka clusters can be complex to deploy, configure, and manage, particularly for beginners.  Zookeeper Dependency: Zookeeper can introduce operational complexity as it requires additional setup and management. (it is solved with KRaft)  Latency: Kafka is not the best solution for extremely low-latency systems; while Kafka is near real-time, other systems may outperform it for ultra-low-latency needs.  Message Ordering: Kafka guarantees order within a partition but not across the entire topic, which may require additional handling logic for certain applications.Similar TechnologiesThere are some similar technologies like RabbitMQ, Apache Flink, Apache, Pulsar or Apache Kinesis etc.            Technology      Comparison with Kafka                  RabbitMQ      Lightweight, built for message queueing.              Apache Pulsar      Similar to Kafka but offers multi-tenancy and more flexible topic models.              Amazon Kinesis      Fully managed and integrated with AWS but lacks Kafka‚Äôs flexibility.              Apache Flink      Focused on stateful computations, often paired with Kafka for stream processing.      While RabbitMQ is suited for message queuing, Kafka excels at distributed data streaming. Pulsar is another contender with advantages like multi-tenancy, but Kafka‚Äôs extensive ecosystem and maturity make it a preferred choice for many enterprises.ConclusionApache Kafka is a cornerstone technology in the world of real-time data streaming and event-driven architectures. It enables companies to build robust, scalable, and fault-tolerant data pipelines, handling high-throughput and distributed processing requirements. Kafka‚Äôs distributed architecture makes it suitable for everything from logging and monitoring to mission-critical enterprise applications.The key advantages of Kafka‚Äîscalability, durability, and high-throughput‚Äîmake it ideal for big data ecosystems and real-time analytics. Its replication and partitioning mechanisms ensure high availability even under heavy load, while recent innovations like KRaft mode and tiered storage keep Kafka evolving for modern needs.Kafka continues to stand out because of its flexibility in integrating with stream processing platforms like Flink, and it plays a vital role in microservices architectures and large-scale distributed systems. While Kafka has its challenges, the pros often outweigh the cons, especially for high-traffic systems that need real-time processing.References:Blog Posts  Kafka Storage Internals  by Rohith Sankepally.  Kafka Overview with pictures  by Vladimir Topolev.Videos   What is Kafka?  by IBM Technology   Apache Kafka in 6 minutes by James Cutajar    1 Videoda #Apache Kafka Nedir? Apache Kafka Neden Kullanƒ±lƒ±r? Apache Kafka Nasƒ±l Kurulur?  by kablosuzkedi."
  },
  
  {
    "title": "Exploring Apache Cassandra | Core Theoretical Concepts",
    "url": "/posts/cassandra-101/",
    "categories": "",
    "tags": "basics, tutorials, devops, nosql, apache, big-data",
    "date": "2023-11-12 00:00:00 +0300",
    





    
    "snippet": "IntroductionCassandra is distributed, de-centralized and fault-tolerant NoSQL database. Since Cassandra is distributed, it can be scaled horizontally with high performance. Apart from that, it has ...",
    "content": "IntroductionCassandra is distributed, de-centralized and fault-tolerant NoSQL database. Since Cassandra is distributed, it can be scaled horizontally with high performance. Apart from that, it has a decentralized structure, that is, it has a structure where there are nodes that can continue this role in case a single controller / master node crashes. This gives Cassandra high availability. Lastly, I would like to mention in this short introduction, it is also fault-tolerant thanks to its replication feature.Data ModelCassandra is a column-based database is formed nested maps. When we compare with traditional databases, they use keyspace instead of databases and column-family structures instead of tables. Due to their column-based structure, they cause more costly transaction operations than RDBMS. Therefore, they are not very suitable for OLTP, but they can do the reading effectively in terms of quickly accessing the information in the desired columns. In this respect, it is a very suitable database alternative for OLAP.Keys &amp; IndexesCassandra has a primary key as in RDBMS, but does not have a foreign key. Similarly, it has the concept of secondary index. However, the secondary index is not as efficient as the foreign key. At the primary key point, the RDBMS consists of a unique token and is different for each row. In addition, primary keys have a special importance in Cassandra. Because the distribution and storage of data is determined by them.\\[Primary \\ Key = Partition \\ Key + Clustering \\ Key\\]The primary key has the Partition Key, which contains information about how the data will be distributed, and the Clustering Key, which contains information about how this data will be sorted/stored in a node. A Partition function and hashing algorithm are used when creating the Partition Key. This partition algorithm is used to generate a value in the range \\([-2^{63}, 2^{63} - 1]\\). Today, there are two partitioners that are used most frequently, they differ according to the hashing algorithm they use.  RandomPartitioner: Generates tokens with MD5 hashing  Murmur3Partitioner: Generates tokens by Murmur hashing and this is Cassandra‚Äôs default setting.These two methods are often used, but tokens cannot be specifically assigned at certain intervals and do not allow running aggregation queries. Apart from these, there is another Partitioner, ByteOrderedPartitioner, this method generates tokens using hexadecimal representations. As an advantage, aggregation queries can be run and tokens can be generated by giving a specific range. But since many requests are made on some nodes, it causes an unstable system, which we do not want. That‚Äôs why ByteOrderedPartitioner is no longer preferred.Basic Structure1 Cluster Cassandra System ExampleThe servers running instances in Cassandra are known as Nodes. A Node is the fundamental infrastructure component of Cassandra. It can be a physical server, an EC2 instance, or a virtual machine. All nodes are organized in a ring network topology, forming a peer-to-peer structure where each node is independent and plays an equal role within the ring. Importantly, each node contains the actual data and is capable of handling both read and write requests. This means that regardless of where the data is stored within the cluster, you‚Äôll always receive the most up-to-date version. Last but not least, each node has virtual nodes.  ‚ÄúVnodes change this paradigm from one token or range per node, to many per node.‚Äù - Brandon WilliamsNodes are grouped into racks, which are then organized within data centers. One or multiple data centers collectively form a cluster. This hierarchical structure helps optimize data distribution and fault tolerance. Nodes communicate with their neighboring nodes through a protocol known as the Gossip Protocol, which exchanges information about their current status and operability, ensuring all nodes remain aware of each other‚Äôs state and activities.Hinted Hand-offWhen an action comes, if the node is unable to respond at that moment, this action is written to a hint file. This file keeps information about the action for a certain period of time. If the node is able to perform the action again within this period, the action information is taken from the file and applied. However, if it is not able to perform the action within this period, the file containing the action information is discarded. This mechanism is called Hinted Hand-off. Thanks to this mechanism, Cassandra ensures consistency at a certain point.Consistency LevelsCassandra has tunable consistency. Consistency levels and the rate of consistency can be controlled. There are different forms of consistency for Writing and Reading. There are four levels of consistency for writing:  One: If any of the replica nodes returns successful, the user returns the result that the action was completed successfully.  All: If a successful result is returned from all replica nodes, the user returns the result that the action was completed successfully.  Quorum: In order for the action to be considered successful, the minimum number of successes from the replica nodes is specified. If this number of successful results is returned, the user will be notified that the action was successful.  Local Quorum: In structures with more than one datacenter, quorum is determined for each datacenter and it is the level of quorum logic.While the consistency levels in the read state are similar to the write action, they are slightly different. There are three different levels:  One: The controller node tries to read the data as fast as it can reach among the replica nodes and transmit it to the user.  All: The result is expected from all replica nodes.  Quorum: As in writing, a minimum number of successful results are expected. Unlike here, the fastest result is read and the hash is kept. Then the hash of the replicas is taken according to the quorum number. Then all hashes are compared and if the hashes are the same, the first read data is returned.  If the hashes do not match, or if a different result is returned, the most recent timestamp is returned and the outdated data is updated according to the last timestamp.Replicas &amp; ConsistencyThe replicas, that is, the copied data in Cassandra, are basically implemented according to two important features. One of them is the replication strategy; If there is only one data center, the Simple Strategy is used, but if there is more than one data center, the Network Topology strategy is used. Another important feature is the replication factor. This factor indicates how many copies will be made of a replica. Also, this feature affects Cassandra‚Äôs consistency. In general, if consistency with the quorum is to be ensured, it is recommended to be as follows:\\[Replication \\ Factor &lt; Read Quorum + Write Quorum\\]For this, the following values ‚Äã‚Äãcan be used or the quorum number can be adjusted with a formula like below.            Replication Factor      Read Quorum      Write Quorum                  $N$      1      $N$              $N$      $N$      1              $N$      $(N+1) / 2$      $(N+1) / 2$      \\[\\text{quorum} = \\left\\lceil \\frac{\\text{sum of all replication factor} + 1}{2} \\right\\rceil\\]Storage SystemsData can be stored on disk or memory. Basically, the structures in which we hold the data are:  Commit log  Memtables  SSTables(Sorted String Tables): Index, Summary, Bloom Filters, Data  Row Cache &amp; Key CacheIn the memory, there are memtables, row cache, key cache, summary of index and bloom filters. On the disk, there are commit log, index table, data table and column bloom filters.  Commit log: It is the first place where information about the actions to be taken is written.  Data table: It is the file where the actual data is stored. For each line in the file, there is the time of deletion, the time marked for deletion, and the data in the line. The time marked for deletion keeps the time when the action is given for the first time, the deleted time keeps the time when it is completely deleted. The data is retrieved sequentially from memtables and after this process is finished, the commit log is purged.  Index Table: It consists of data pairs showing the relevant partition key and the location of this row in the data file.  Column Bloom Filters: It is a vector formed according to false positive that checks whether the searched column exists for each row. 1 represents existence and 0 represents non-existence.  Memtables: It is the temporary table where the actions written to the Commit log are written to SSTables before they are flushed. The data stays here until the commit log or memtables is full or until the flush is triggered for memtables.  Summary of Index: It is a summary file in which the locations of the indexes of the data in the index file and the start and end values ‚Äã‚Äãof the partition key ranges are matched. For this feature, indexes are taken at certain intervals. This property can be configured with the index_intervalparameter when creating a column-family  Bloom filters: Vector that decides whether a key is on disk or not. This vector is generated based on the false positive probability. It can be checked with bloom_filter_fp_chance when creating the column-family and its default value is 0.1. Very high values ‚Äã‚Äãincrease the likelihood bias of non-existent.  Key Cache: Data is stored in key:valuepairs. While the keys ‚Äã‚Äãhold the primary keys, the values ‚Äã‚Äãhold the offset of the row in the data file.  Row Cache: A certain number of rows are kept as cache. There can be three different parameters; the situation we never held, none; where we give a numeric value where all are kept, all or a certain number of rows are kept.  A node can have a certain number of SSTables. This number was specified with the min_threshold parameter when creating the column family. If it occurs more than the specified number of SSTables, then these SSTables are made into a single SSTable by applying compaction operation. SSTable compaction is performed by grouping partition keys and keeping last timestamp data, and if there are tombstones associated with these rows/columns, they are deleted.Conclusion  ‚ÄúCassandra is a distributed, decentralized, fault-tolerant, column-based NoSQL database. It is horizontally scalable, low cost, and performs well. It is also a very fast read, open source database alternative for OLAP, and according to the CAP theorem, it is an AP featured database alternative.‚Äù"
  },
  
  {
    "title": "Logging instead of Print Statements!",
    "url": "/posts/python-logging/",
    "categories": "",
    "tags": "python, software, tricks, logging",
    "date": "2023-10-24 00:00:00 +0300",
    





    
    "snippet": "Many of us started writing code by printing a hello world. Print statements are useful in many places such as logging and debugging while writing code, but they also have some limits. Print stateme...",
    "content": "Many of us started writing code by printing a hello world. Print statements are useful in many places such as logging and debugging while writing code, but they also have some limits. Print statements print an output to the console screen at the end of the day. Depending on the system used and default stdout buffer size settings, your code will unintentionally throw an error when the output in the console reaches the maximum stdout buffer size. In this case, while you are looking for the cause of the error in the general functioning of your code, you will be very surprised when you see that it actually comes from the print statement. In addition, reviewing an output with long print statements is a very difficult task, and as time goes by, you will realize that it makes you very tired. In summary, although print statements make our job much easier in small projects and rapid development, we must be very careful about the places we use when we go to productization. To deal with this situation, you can use some logging tools or modules written to keep logs.Logging is used to record while debugging or to keep track of some steps in the code at run time.There are many helpful tools for this. In this post, I will briefly talk about the simple but useful Python logging module and how it can be used.A logging object is simply defined within the logging library. Within the definitions, you can define information such as writing to the file, the mode in which you will write to the file, the log format it will print, and the log levels you want.** By connecting your code snippets or projects that run at certain times to the logging library, you can regularly save the log information to files and review it when necessary.** This feature allows us to develop safely without worrying about the print buffer size problem, helps us examine the logs of our codes over time and what actions we can take.  TL;DR:  Logging gives us three main benefits: reliability, flexibility, easy and fast debugging.Let‚Äôs assume we code a simple project with capabilities like the following:  Connecting to the database,  Performing some operations on the connected database,  Measuring the code duration and issuing a warning when it exceeds a certain time,  Throwing an error when a required file cannot be found in the code,  Closing the database connectionWhen we write the codes, design the project, and include the logging library in this project, we expect to get an output like Figure 1.Figure 1. Logging output example of the codes are¬†below.  Note: ls and cat are linux commands. ls command list files in current directory, cat command shows content of a file.Let‚Äôs quickly pseudo-code the project we have assumed and examine how we include and use the logging library in it.import loggingimport timefrom datetime import datetimedef connect_db():    # connect db some connection parameters and package    time.sleep(1)  # sleep 1 second    logging.info(\"Connected to database\")def do_some_cool_things():    # do some cool things like get a table and select some operations on rows    time.sleep(1)  # sleep 1 second    logging.info(\"Cool things is done!\")def be_angry():    # let's assume code raise an FileNotFound exception    time.sleep(1)  # sleep 1 second    logging.error(\"FileNotFoundError exist!\")def give_an_advice():    # let's assume code runs for too long    time.sleep(1)  # sleep 1 second    logging.warning(\"The code has been working for a long time.\")def close_db_connection():    # close database connection if not closed before    time.sleep(1)  # sleep 1 second    logging.info(\"Database connection is closed.\")def main():    logging.basicConfig(format='%(asctime)s :: [%(levelname)s] - %(message)s',                        datefmt='%Y-%m-%d %H:%M:%S',                        level=logging.INFO,                        filename=f'db_pipeline_{datetime.now().strftime(\"%Y%m%d\")}.log',                        filemode='w',                        encoding='utf-8')    connect_db()    do_some_cool_things()    give_an_advice()    be_angry()    close_db_connection()if __name__ == \"__main__\":    main()  ‚ÄúIn summary, although print statements make our work much easier in small projects and rapid development, we should be very careful about the places we use when we go to productization. Using alternative modules such as logging instead of print statements makes our job easier, gives us flexibility, and ensures that our code runs reliably and allows us to monitor¬†it.‚ÄùThanks for reading."
  },
  
  {
    "title": "Deep Dive into Densely Connected Convolutional Networks (DenseNet)",
    "url": "/posts/dense-cnn/",
    "categories": "",
    "tags": "algorithms, machine-learning, supervised-learning, from-scratch, deep-learning, pytorch",
    "date": "2023-09-28 00:00:00 +0300",
    





    
    "snippet": "IntroductionIn deep learning, architectures are becoming increasingly deep and complex. While depth contributes to stronger feature extraction, it also introduces problems such as vanishing gradien...",
    "content": "IntroductionIn deep learning, architectures are becoming increasingly deep and complex. While depth contributes to stronger feature extraction, it also introduces problems such as vanishing gradients and redundant feature learning. DenseNet, introduced by Gao Huang et al. in 2017, offers an elegant solution to these issues by introducing dense connections between layers.DenseNet improves feature propagation, encourages feature reuse, and offers a more efficient parameter structure compared to traditional Convolutional Neural Networks (CNNs). In this blog post, we‚Äôll explore DenseNet‚Äôs architecture, advantages, potential drawbacks, and implement it in PyTorch ‚Äî both using pre-built version.DenseNet Architecture OverviewDenseNet introduces the concept of dense blocks, where each layer\\((L)\\) is connected to every other layer within the same block. Instead of just passing information from one layer to the next, DenseNet concatenates the outputs of all previous layers as inputs to subsequent layers.DenseNet Block ConnectivityDenseNet connectivity can be mathematically represented as follows:\\[x_l = H_l([x_0,x_1,‚Ä¶,x_{l‚àí1}])\\]Where:  \\(H_l\\)‚Äã represents the operations of batch normalization, ReLU activation, and convolution.  \\([x_0,x_1,‚Ä¶,x_{l‚àí1}]\\) indicates the concatenation of feature maps from all previous layers.For a network with L layers, the number of direct connections is \\(\\frac{L \\times (L + 1)}{2}\\)There are various DenseNet architectures, each designed to reduce the number of parameters compared to other models, with ongoing efforts to minimize this further.To achieve this reduction, DenseNet architectures limit the number of inputs at each layer to a specific value (e.g., k=12). This k value represents the number of inputs, unlike traditional architectures where the number of inputs is not constrained. After limiting the inputs, these features are added to a feature map, and the architecture is completed with an average pooling layer and a softmax layer.This concatenation allows the model to reuse features learned by preceding layers, which enables DenseNet to extract richer feature representations with fewer parameters.Network VisualizationTo understand how dense connections work within a block, let‚Äôs visualize the data flow using a Mermaid diagram.flowchart LR    Input --&gt; L1[Layer 1] &amp; L2[Layer 2] &amp; L3[Layer 3] &amp; L4[Layer 4]    L1 --&gt; L2 &amp; L3 &amp; L4 &amp; Output    L2 --&gt; L3 &amp; L4 &amp; Output    L3 --&gt; L4 &amp; Output    L4 --&gt; OutputIn the diagram above, every layer in the block receives input from all preceding layers and concatenates their feature maps. This dense connectivity pattern encourages feature reuse.DenseNet Layers: Transition LayersAfter a dense block, a transition layer is typically used to reduce the size of the feature maps and the number of channels. The transition layer applies a 1x1 convolution followed by 2x2 average pooling to halve the spatial dimensions. The transition layer can be expressed as:\\[x_{out} = AvgPool(W_{conv} \\ ‚àó‚àó\\ x_{in})\\]Where \\(W_{conv}\\)‚Äã represents the 1x1 convolution filter, and ‚àó‚àó denotes the convolution operation.DenseNet ArchitectureA typical DenseNet model consists of:  Initial Convolutional Layer: This layer is usually a 7x7 convolution followed by max-pooling.  Dense Blocks: Dense blocks where each layer receives input from all preceding layers. The number of layers per block is determined by a hyperparameter.  Transition Layers: Located between dense blocks to down-sample feature maps using convolution and pooling operations.  Classification Layer: A fully connected layer for classification at the end of the network.The following image visualizes a simple DenseNet architecture with 2 dense blocks.graph LR;    Start[Input] --&gt; Conv1[7x7 Conv,     MaxPool]    Conv1 --&gt; DenseBlock1[Dense     Block 1]    DenseBlock1 --&gt; Transition1[Transition     Layer]    Transition1 --&gt; DenseBlock2[Dense     Block 2]    DenseBlock2 --&gt; Transition2[Transition     Layer]    Transition2 --&gt; FC[Fully     Connected     Layer]    FC --&gt; Output[Predictions]DenseNet VariantsDenseNet Types      DenseNet-B (Bottleneck Architecture): The goal here is to reduce model complexity using 1x1 matrices and convolutional/pooling structures.    DenseNet-C (Compactness Architecture): This variant aims to improve model efficiency by reducing the number of feature maps in transition layers. A specific compression factor is determined, and structures below this compression value are classified as DenseNet-C.  DenseNet-BC: When both bottleneck layers and transition layers with Œ∏&lt;1 are used, the architecture is referred to as DenseNet-BC.Pros and Cons of DenseNetPros  Efficient Parameter Usage: DenseNet significantly reduces the number of parameters compared to traditional deep architectures like ResNet.  Feature Reuse: DenseNet encourages feature reuse, which leads to more compact and robust feature representations.  Improved Gradient Flow: Dense connections help mitigate the vanishing gradient problem, resulting in more stable and effective training.  Less Overfitting: DenseNet tends to generalize better on smaller datasets due to the strong regularization effect of feature reuse.Cons  Memory Usage: Dense connections result in a high memory overhead because of the concatenation of feature maps.  Training Time: Due to the increased number of connections, DenseNet requires more computation per forward pass, leading to longer training times.  Diminishing Returns: Increasing the depth of DenseNet does not always yield significant improvements and may lead to redundant computations.ConclusionLast but not least, DenseNet presents a creative and effective way of addressing the challenges of deep learning architectures, such as vanishing gradients and feature redundancy. By densely connecting layers, DenseNet promotes feature reuse and optimizes network capacity, resulting in a highly efficient model.We explored both a pre-trained and a custom implementation of DenseNet in PyTorch, complete with a training and evaluation pipeline. Whether you‚Äôre working on image classification tasks or exploring dense architectures for more complex problems, DenseNet is a powerful tool to add to your deep learning toolkit.CodesDenseNet (Pre-trained) in PyTorchLet‚Äôs now implement DenseNet using PyTorch. We‚Äôll start by using a pre-trained DenseNet model, followed by building our own from scratch.Pre-trained DenseNet Using torchvisionimport torchimport torch.nn as nnimport torchvision.models as modelsimport torch.optim as optimimport torchvision.transforms as transformsimport torchvision.datasets as datasets# Load pre-trained DenseNet121 modelmodel = models.densenet121(weights='DEFAULT') # use pretrained = True for  version below 0.13print(model)# Modify the classifier to fit your dataset's number of classesnum_ftrs = model.classifier.in_featuresmodel.classifier = nn.Linear(num_ftrs, 10)  # CIFAR10 has possible 10 classes for an example# Prepare for trainingcriterion = nn.CrossEntropyLoss()optimizer = optim.Adam(model.parameters(), lr=0.001)# Data augmentation and normalization for trainingtransform = transforms.Compose([    transforms.Resize(224),  # Resize images to match the DenseNet input size    transforms.ToTensor(),    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])# Load datasettrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)# Training loopdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")model.to(device)for epoch in range(10):  # Loop over the dataset multiple times    running_loss = 0.0    for inputs, labels in train_loader:        inputs, labels = inputs.to(device), labels.to(device)                # Zero the parameter gradients        optimizer.zero_grad()        # Forward + backward + optimize        outputs = model(inputs)        loss = criterion(outputs, labels)        loss.backward()        optimizer.step()        # Print statistics        running_loss += loss.item()    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')# Evaluate on test settest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)model.eval()correct = 0total = 0with torch.no_grad():    for inputs, labels in test_loader:        inputs, labels = inputs.to(device), labels.to(device)        outputs = model(inputs)        _, predicted = torch.max(outputs.data, 1)        total += labels.size(0)        correct += (predicted == labels).sum().item()print(f'Test Accuracy: {100 * correct / total:.2f}%')  This code loads a pre-trained DenseNet121 model from torchvision and modifies the classifier to fit a custom dataset with 10 classes. We then perform training using the CIFAR-10 datasets. I trained this model just 1 epoch instead of 10 and then test on train and test datasets; Results are that train Accuracy is  86.82% and Test Accuracy is 84.84%.Custom DenseNet Implementation from Scratchimport torchimport torch.nn as nnclass DenseLayer(nn.Module):    def __init__(self, in_channels, growth_rate):        super(DenseLayer, self).__init__()        self.bn = nn.BatchNorm2d(in_channels)        self.relu = nn.ReLU(inplace=True)        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)        def forward(self, x):        out = self.bn(x)        out = self.relu(out)        out = self.conv(out)        return outclass DenseBlock(nn.Module):    def __init__(self, in_channels, growth_rate, n_layers):        super(DenseBlock, self).__init__()        self.layers = nn.ModuleList()        for _ in range(n_layers):            self.layers.append(DenseLayer(in_channels, growth_rate))            in_channels += growth_rate        def forward(self, x):        for layer in self.layers:            new_features = layer(x)            x = torch.cat([x, new_features], 1)        return x        class TransitionLayer(nn.Module):    def __init__(self, in_channels, out_channels):        super(TransitionLayer, self).__init__()        self.bn = nn.BatchNorm2d(in_channels)        self.relu = nn.ReLU(inplace=True)        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)        def forward(self, x):        x = self.bn(x)        x = self.relu(x)        x = self.conv(x)        x = self.pool(x)        return x        class DenseNet121(nn.Module):    def __init__(self, growth_rate=32, block_layers=[6, 12, 24, 16], num_classes=10):        super(DenseNet121, self).__init__()                # Initial Convolution Layer        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)        self.bn1 = nn.BatchNorm2d(64)        self.relu = nn.ReLU(inplace=True)        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)                # Dense Blocks and Transition Layers        self.dense_blocks = nn.ModuleList()        self.transition_layers = nn.ModuleList()                in_channels = 64        for i, num_layers in enumerate(block_layers):            block = DenseBlock(in_channels, growth_rate, num_layers)            self.dense_blocks.append(block)            in_channels += num_layers * growth_rate            if i &lt; len(block_layers) - 1:                transition = TransitionLayer(in_channels, in_channels // 2)                self.transition_layers.append(transition)                in_channels = in_channels // 2                # Final Classification Layer        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))        self.fc = nn.Linear(in_channels, num_classes)        def forward(self, x):        x = self.conv1(x)        x = self.bn1(x)        x = self.relu(x)        x = self.pool(x)                for block, transition in zip(self.dense_blocks, self.transition_layers):            x = block(x)            x = transition(x)                x = self.dense_blocks[-1](x)        x = self.global_avg_pool(x)        x = torch.flatten(x, 1)        x = self.fc(x)        return xmodel = DenseNet121(num_classes=10) # for cifar 10 classesprint(model)  This code create a Custom DenseNet121 model. We then perform training using the CIFAR-10 datasets. I trained this model just 1 epoch instead of 10 and then test on train and test datasets; Results are that train Accuracy is  54.76% and Test Accuracy is 44.83%."
  },
  
  {
    "title": "Deep Dive into Convolutional Neural Networks (CNNs)",
    "url": "/posts/convolutional-neural-networks/",
    "categories": "",
    "tags": "algorithms, machine-learning, supervised-learning, from-scratch, deep-learning, pytorch",
    "date": "2023-09-19 00:00:00 +0300",
    





    
    "snippet": "Convolutional Neural Networks (CNNs) have become a cornerstone of deep learning, particularly in the field of computer vision. From image recognition to object detection, CNNs have transformed the ...",
    "content": "Convolutional Neural Networks (CNNs) have become a cornerstone of deep learning, particularly in the field of computer vision. From image recognition to object detection, CNNs have transformed the way machines interpret visual data.graph LR    A[Input     Image     32x32x3]     B[Convolution     Layer     3x3 filter, 32 filters]    C[ReLU     Activation]    D[Max     Pooling     2x2]    E[Flattened     Feature      Map]    F[Fully     Connected     Layer]    G[Softmax     Function]    H[Class     Probabilities]    A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F--&gt; G --&gt; H In this blog post, we‚Äôll delve into the architecture of CNNs, explore their advantages and disadvantages, and provide a detailed implementation in PyTorch.Convolutional Neural Networks (CNNs) consist of several key layers that work together to process and analyze grid-like data such as images. Below is a detailed explanation of each layer in a typical CNN architecture, including mathematical formulas and a Mermaid graph to visualize the architecture.1. Convolution Layergraph LR    A[Input     Image]    B[Convolution Layer     3x3 filter, 32 filters]    C[Feature Map     32x32x32]    A ---&gt; B ---&gt; CThe convolutional layer is where the magic happens. It applies a set of filters (kernels) to the input image to produce feature maps. Each filter detects different features like edges, textures, or patterns.Kernel(Filter): A kernel (or filter) is a small matrix used to scan the input image and produce feature maps. It is the core of the convolution operation. Kernels are used to detect various features such as edges, textures, and patterns in the input data. Different kernels can detect different features. The kernel size refers to the dimensions of the kernel matrix. Common sizes include 3x3, 5x5, and 7x7.The kernel size determines how much of the input image is covered at a time during the convolution operation.\\[\\begin{bmatrix} 1 &amp; 0 &amp; -1 \\\\1 &amp; 0 &amp; -1\\\\1 &amp; 0 &amp; -1 \\\\\\end{bmatrix}_{3x3}\\quad\\]Example: 3x3 Vertical Edges Detection KernelStride: The stride is the number of pixels by which the kernel moves across the input image. It determines how much the kernel shifts after each operation. Stride controls the spatial dimensions of the output feature map. A larger stride results in a smaller output feature map.  Stride of 1: The kernel moves 1 pixel at a time.  Stride of 2: The kernel moves 2 pixels at a time, effectively downsampling the feature map.Padding: Padding refers to the addition of extra pixels around the border of the input image. Padding is often used to control the spatial dimensions of the output feature map. Padding helps in maintaining the size of the feature map or adjusting it according to requirements. It also ensures that the edge information of the input image is preserved.Types:  Valid Padding: No padding is applied, which reduces the feature map size.  Same Padding: Padding is added to ensure the output feature map size is the same as the input size.Example of padding with a value of 0 and a size of (2x2)\\[(W_{out}‚Äã,H_{out}‚Äã) = (\\dfrac{W_{in}‚Äã ‚àí K + 2P}{S}‚Äã+1,\\dfrac{H_{in}‚Äã ‚àí K + 2P}{S} ‚Äã+1 )\\]Where:  \\(W_{out}‚Äã, H_{out}‚Äã:\\) Output width and height  \\(W_{in}‚Äã, H_{in}‚Äã:\\) Input width and height  \\(K:\\) Kernel size  \\(P:\\) Padding  \\(S:\\) StrideExample of a convolution operation with a stride value of 1Convolution Operation: The convolution operation involves sliding the kernel over the input image and computing the dot product between the kernel and the portion of the image it covers.This operation transforms the input image into feature maps, which represent various features extracted from the input.\\[Feature\\ Map(i,j)=\\sum_{m=0}^{K‚àí1}\\sum_{n=0}^{K‚àí1}Input(i+m,j+n)‚ãÖKernel(m,n)\\]To further explain the concept of image input: Each image has specific pixel values, which are numerical values within a matrix. These pixels form the various elements of the image. Colored images are composed of three channels: Red (R), Green (G), and Blue (B), commonly referred to as RGB. In cases where there are three channels, the convolution operation is carried out as shown in the example below.Three Channel Convolutional Operation with 1 Stride2. Pooling LayerPooling TypesPooling is a down-sampling operation that reduces the spatial dimensions of the feature maps while preserving important features. Pooling helps reduce the computational load and provides spatial invariance by summarizing the features. There are two types:  Max Pooling: Takes the maximum value from each patch of the feature map.  Average Pooling: Takes the average value from each patch of the feature map.Pooling is typically used with the padding value set to ‚Äúsame,‚Äù while the default value is ‚Äúvalid.‚ÄùPadding Value Calculation: \\((f-1) / 2\\) Output Size Calculation: \\(\\left\\lfloor \\frac{n + 2p - f}{s} \\right\\rfloor + 1\\)  \\(n:\\) input size   \\(p:\\) padding   \\(f:\\) filter (kernel) size   \\(s:\\) stride value 3. Normalization LayerThis process compresses the distribution of data to enhance its readability. Typically, normalization in an RGB image involves dividing by 255 since there are 256 pixel values.4. Fully-Connected LayerFlattens the pooled feature maps and connects every neuron to every neuron in the next layer. This layer is responsible for classifying the extracted features.Converts 2D feature maps into a 1D vector and applies a linear transformation.Pros and ConsAdvantagesDisadvantagesAutomatic Feature Extraction: Learns features directly from data without manual feature engineering.Computationally Intensive:Requires significant computational resources, especially for large models and datasets.Translation Invariance:Can recognize features regardless of their position in the input image.Overfitting: Risk of overfitting with limited data, requiring regularization techniques.Scalability:Effective for various tasks and scales well with larger datasets.Complexity:Design and tuning of architecture can be complex and require domain expertise.Hierarchical Feature Learning:Learns different levels of features from edges to complex objects.Data Hungry:Requires large amounts of labeled data to achieve high performance.ConclusionCNNs leverage convolutional layers, activation functions, pooling, and fully connected layers to process and classify image data effectively. Their architecture allows them to detect and learn spatial hierarchies of features, making them powerful for tasks like image recognition.Codesimport torchimport torch.nn as nnimport torch.optim as optimimport torch.nn.functional as Ffrom torchvision import datasets, transformsfrom torch.utils.data import DataLoader# Define the CNN modelclass CNN(nn.Module):    def __init__(self):        super(CNN, self).__init__()        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Convolutional Layer 1        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # Convolutional Layer 2        self.fc1 = nn.Linear(64 * 7 * 7, 128) # Fully Connected Layer 1        self.fc2 = nn.Linear(128, 10) # Output Layer    def forward(self, x):        x = F.relu(F.max_pool2d(self.conv1(x), 2)) # Convolutional + ReLU + Pooling        x = F.relu(F.max_pool2d(self.conv2(x), 2)) # Convolutional + ReLU + Pooling        x = x.view(-1, 64 * 7 * 7) # Flatten        x = F.relu(self.fc1(x)) # Fully Connected + ReLU        x = self.fc2(x) # Output Layer        return F.log_softmax(x, dim=1) # Softmax for classification# Hyperparameters and data preparationdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=64, shuffle=True)test_loader = DataLoader(datasets.MNIST('./data', train=False, download=True, transform=transform), batch_size=1000, shuffle=False)model = CNN().to(device)optimizer = optim.Adam(model.parameters(), lr=0.001)# Training functiondef train(model, device, train_loader, optimizer, epoch):    print(\"train started\")    model.train()    for batch_idx, (data, target) in enumerate(train_loader):        data, target = data.to(device), target.to(device)        optimizer.zero_grad()        output = model(data)        loss = F.nll_loss(output, target)        loss.backward()        optimizer.step()        if batch_idx % 100 == 0:            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')# Testing and evaluating functiondef test(model, device, test_loader):    print(\"test started\")    model.eval()    test_loss = 0    correct = 0    with torch.no_grad():        for data, target in test_loader:            data, target = data.to(device), target.to(device)            output = model(data)            test_loss += F.nll_loss(output, target, reduction='sum').item()            pred = output.argmax(dim=1, keepdim=True)            correct += pred.eq(target.view_as(pred)).sum().item()    test_loss /= len(test_loader.dataset)    accuracy = 100. * correct / len(test_loader.dataset)    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')    return accuracy# Training and evaluating the modelfor epoch in range(1, 5):    print(epoch,\". epoch started\")    train(model, device, train_loader, optimizer, epoch)accuracy = test(model, device, test_loader)  Test set: Average loss: 0.0288, Accuracy: 9901/10000 (99.01%)"
  },
  
  {
    "title": "Classification Evaluation Metrics in Machine Learning",
    "url": "/posts/classification-metrics/",
    "categories": "",
    "tags": "metrics, machine-learning, classification, basics",
    "date": "2023-08-11 00:00:00 +0300",
    





    
    "snippet": "When building a machine learning model, particularly for classification tasks, one of the most critical aspects is evaluating its performance. You can‚Äôt simply rely on a model‚Äôs ability to make pre...",
    "content": "When building a machine learning model, particularly for classification tasks, one of the most critical aspects is evaluating its performance. You can‚Äôt simply rely on a model‚Äôs ability to make predictions without knowing how accurate or useful those predictions are. For example, in a healthcare setting, the consequences of misclassifying a patient with a serious illness can be grave, while in an email spam detection system, false positives might just mean a few harmless emails getting flagged as spam. Therefore, understanding classification evaluation metrics is key to developing reliable, effective models that meet specific real-world demands.Many new data scientists or machine learning engineers jump straight into using metrics like accuracy without fully appreciating its limitations‚Äîespecially when dealing with imbalanced datasets, where accuracy might give a false sense of model performance. In this blog post, we‚Äôll break down the various classification evaluation metrics, including sensitivity, specificity, true positive rate (TPR), and false positive rate (FPR), alongside precision and recall. We will also explore their mathematical underpinnings and use real-life examples to better illustrate their importance.The Confusion Matrix: A Foundation for MetricsA good place to start is the confusion matrix, which provides a structured way to view the performance of a classification model. It offers a breakdown of how the model‚Äôs predictions compare with actual outcomes, and is essential for deriving more nuanced metrics.The confusion matrix is a table like this:            #      Predicted Positive      Predicted Negative                  Actual Positive      True Positive (TP)      False Negative (FN)              Actual Negative      False Positive (FP)      True Negative (TN)        True Positives (TP): These are the cases where the model correctly predicted the positive class.  True Negatives (TN): These are instances where the model correctly predicted the negative class.  False Positives (FP): Here, the model incorrectly predicted the positive class when the actual outcome was negative. Also known as Type I errors.  False Negatives (FN): These are cases where the model predicted the negative class, but the actual outcome was positive. Known as Type II errors.With this matrix, we can compute several key metrics to assess model performance from different angles, depending on the problem we‚Äôre addressing.Accuracy: Simple but MisleadingThe most straightforward metric derived from the confusion matrix is accuracy.\\[\\text{Accuracy} = \\frac{TP+TN}{FP+FN+TP+TN‚Äã}\\]Accuracy measures the proportion of correct predictions out of all predictions. While intuitive and easy to compute, it‚Äôs not always reliable, especially in cases of imbalanced datasets. Consider a fraud detection system where only 1% of transactions are fraudulent. A model that predicts ‚Äúnon-fraud‚Äù for every transaction would achieve 99% accuracy, but it wouldn‚Äôt be effective at actually detecting fraud, which is the goal.Let‚Äôs say we are building a model to classify email as spam or not. We have the following confusion matrix after testing the model on 1000 emails:            #      Predicted Spam      Predicted Not Spam                  Actual Spam      30      70              Actual Not Spam      20      880      Here, the accuracy would be:\\[\\text{Accuracy} = \\frac{TP+TN}{FP+FN+TP+TN‚Äã} = \\frac{30+880}{30+880+20+70} = \\text{0.91}\\]  However, this high accuracy hides the fact that the model only catches 30 of the 100 spam emails‚Äîthus failing to flag 70 of them.Precision and Recall: Targeting Specific Error TypesThis is where precision and recall come into play. These metrics give you more insight into how the model performs with respect to false positives and false negatives.Precision  Precision answers the question: ‚ÄúOut of all the positive predictions, how many were correct?‚ÄùPrecision is particularly important in scenarios where false positives are costly, such as fraud detection or medical diagnoses.\\[\\text{Precision} = \\frac{TP}{TP+FP}\\]If we calculate precision for email classification example from above:\\[\\text{Precision} =  \\frac{30}{30+20}  = 0.60\\]  This means that the model only identified 60% of emails flagged as spam were actually spam.Recall (Sensitivity / True Positive Rate - TPR)  Recall answers the question:‚ÄúOut of all the actual positives, how many did the model correctly identify?‚ÄùRecall, also known as sensitivity or true positive rate (TPR).A high recall means that the model has identified most of the actual positive cases. This is critical in scenarios where missing a positive case (i.e., a false negative) is costly, such as in detecting serious illnesses. For example, In medical testing, recall is key when identifying patients with a disease. If a test has high sensitivity, it correctly identifies a large proportion of those who have the disease, minimizing the chance of missing a sick patient.\\[\\text{Recall} = \\frac{TP}{TP+FN}\\]If we calculate recall for email classification example from above:\\[\\text{Recall} =  \\frac{30}{30+70} = 0.30\\]  This means that the model only identified 30% of all actual spam emails, leaving 70% of them undetected.Specificity and False Positive Rate (FPR)While recall focuses on the true positives, specificity (also known as true negative rate or TNR) and false positive rate (FPR) evaluate the model‚Äôs ability to handle the negative class.Specificity (True Negative Rate - TNR)  Specificity answers the question: ‚ÄúOut of all the actual negatives, how many were correctly identified?‚Äù\\[\\text{Specificity} = \\frac{TN}{TN+FP}\\]High specificity is critical in scenarios where false positives are undesirable, such as in spam detection systems where legitimate emails should not be mistakenly classified as spam.False Positive Rate (FPR)The false positive rate (FPR) complements specificity and answers the question: ‚ÄúOut of all the actual negatives, how many were incorrectly predicted as positive?‚Äù\\[\\text{FPR} = \\frac{FP}{FP+TN}\\]In a cancer screening test, specificity helps reduce the number of healthy patients who are falsely flagged for additional, possibly stressful testing. Meanwhile, FPR quantifies how often a healthy individual would be incorrectly classified as needing further screening.  In an ideal model, we want the FPR to be as low as possible, meaning fewer false alarms.The F1-Score: Balancing Precision and RecallWhen precision and recall conflict, the F1-score is a useful metric that balances both. The F1-score is the harmonic mean of precision and recall:\\[\\text{F1} = 2 \\times \\frac {Precision \\times Recall}{Precision + Recall}\\]This metric is particularly helpful when dealing with imbalanced datasets and when the cost of false positives and false negatives is similar.For email example:\\[\\text{F1} = 2 \\times \\frac {0.60 \\times 0.30}{0.60 + 0.30}= 2 \\times \\frac{0.18}{0.90} \\approx 0.40\\]  The F1-score of 40% shows that neither precision nor recall is exceptionally high, and there‚Äôs room for improvement.AUC-ROC: Understanding the Trade-OffsAUC-ROC (TPR-FPR Curve)Another powerful tool for classification evaluation is the ROC curve (Receiver Operating Characteristic) and its corresponding metric, AUC (Area Under the Curve). The ROC curve (Receiver Operating Characteristic) plots the trade-off between the true positive rate (recall) and the false positive rate (FPR) at various thresholds. The AUC-ROC (Area Under the Curve) summarizes this trade-off,An AUC close to 1 indicates excellent model performance, while an AUC around 0.5 suggests the model is no better than random guessing.\\[\\text{AUC-ROC} = \\text{Area under the ROC Curve} = \\int_{0}^{1} \\text{TPR}(x) \\, d(\\text{FPR}(x))\\]For example, In credit card fraud detection, a high AUC-ROC score indicates that the model does a good job distinguishing between fraudulent and legitimate transactions, even when the dataset is imbalanced.Precision-Recall Curve  For problems with highly imbalanced classes, the precision-recall curve is often more informative than the ROC curve. It plots precision against recall at different thresholds, helping to find the right balance between catching positive cases and avoiding false positives.More Real Life ExamplesCredit Card Fraud DetectionIn this scenario, we are building a classification model to detect fraudulent credit card transactions. Out of 100,000 transactions, only 1,000 are fraudulent, which makes this an imbalanced dataset (fraudulent transactions represent only 1% of the total). We aim to evaluate the performance of a model designed to detect these fraudulent transactions.Confusion Matrix            ¬†      Predicted Fraud (Positive)      Predicted Not Fraud (Negative)                  Actual Fraud (Positive)      800      200              Actual Not Fraud (Negative)      9,000      90,000      From this confusion matrix:  True Positives (TP) = 800 (correctly identified fraudulent transactions)  False Negatives (FN) = 200 (missed fraudulent transactions)  False Positives (FP) = 9,000 (incorrectly flagged legitimate transactions as fraud)  True Negatives (TN) = 90,000 (correctly identified legitimate transactions)  Accuracy = $\\large\\frac{800+90000}{800+90000+9000+200} \\approx\\small 0.908$  Precision = $\\large\\frac{800}{800+9000} \\approx \\small0.081$  Recall (Sensivity) = $\\large\\frac{800}{800+200} \\approx \\small0.80$  Specificity = $\\large\\frac{90000}{90000+9000} \\approx \\small0.909$  F1 Score = $2\\times\\large\\frac{0.081\\times0.80}{0.081 + 0.80} \\approx \\small0.15$  Even though the Accuracy seems high at 90.8%, it doesn‚Äôt tell us much about the model‚Äôs ability to detect fraud in an imbalanced dataset. Precision is very low, meaning that out of all the transactions flagged as fraud, only 8.1% were actually fraudulent. This is critical, as we want a model that minimizes false alarms for customers. Recall is high, meaning that the model caught 80% of all fraudulent transactions. This is important in fraud detection, where catching as many fraud cases as possible is crucial. Specificity tells us that 90.9% of the legitimate transactions were correctly classified, which is a good rate for protecting legitimate users. F1-score is 15%, indicating that the balance between precision and recall is not optimal. We might need to tune our model to reduce false positives without sacrificing too much recall.Illness Detection (Cancer Screening)Now, let‚Äôs consider a cancer screening test for a rare type of cancer that affects 2% of the population. The dataset contains 10,000 patients, with 200 of them actually having the disease. We evaluate the performance of a classification model designed to detect the disease.Confusion Matrix            ¬†      Predicted Positive (Cancer)      Predicted Negative (No Cancer)                  Actual Positive (Cancer)      180      20              Actual Negative (No Cancer)      400      9,400      From this confusion matrix:  True Positives (TP) = 180 (correctly identified cancer cases)  False Negatives (FN) = 20 (missed cancer cases)  False Positives (FP) = 400 (healthy patients incorrectly flagged as having cancer)  True Negatives (TN) = 9,400 (correctly identified healthy patients)  Accuracy = $\\large\\frac{180+9400}{180+9400+400+20} \\approx \\small0.958$  Precision = $\\large\\frac{180}{180+400} \\approx \\small0.31$  Recall (Sensivity) = $\\large\\frac{180}{180+20} \\approx \\small0.90$  Specificity = $\\large\\frac{9400}{9400+400} \\approx \\small0.96$  F1 Score = $2\\times\\large\\frac{0.31\\times0.90}{0.31 + 0.90} \\approx \\small0.46$  The Accuracy of 95.8% looks great on the surface, but let‚Äôs examine the other metrics to get a better understanding. Precision is low at 31%, meaning that many patients who were flagged as having cancer do not actually have the disease. This could lead to unnecessary stress and further testing for those 400 healthy patients.Recall is high, indicating that the model is effective at catching 90% of actual cancer cases. This is crucial in medical screening, as we want to minimize false negatives and ensure most sick patients are detected. A specificity of 96% means that the model correctly identified a high proportion of healthy patients, reducing the risk of false positives. The F1-score is 46%, which balances the low precision with the high recall. While the model is good at identifying cancer cases, its tendency to falsely diagnose healthy patients requires further tuning.Practical Insights from the ExampleIn both examples, we see the importance of considering multiple evaluation metrics:  In fraud detection, recall is critical to catch as many fraud cases as possible, but the low precision indicates too many false alarms, which could frustrate users.  In cancer screening, high recall is essential to ensure that nearly all patients with cancer are detected, but the low precision means many healthy individuals are incorrectly flagged for further testing, which can cause unnecessary concern.By evaluating models with precision, recall, specificity, and F1-score, you can choose metrics that best suit your problem. In highly imbalanced datasets, such as fraud detection and illness screening, accuracy alone can be misleading, making these other metrics essential for a clear understanding of model performance.Comparing the MetricsThe table below summarizes key classification evaluation metrics and their ideal use cases:            Metric      Formula      Best Use Case      Pros      Cons                  Accuracy      $\\large\\frac{TP+TN}{FP+FN+TP+TN‚Äã}$       ‚Äã      Balanced datasets      Easy to compute      Misleading in imbalanced datasets              Precision      $\\large\\frac{TP}{TP+FP}$      When false positives are costly      Minimizes false positives      Ignores false negatives              Recall (Sensitivity / TPR)      $\\large\\frac{TP}{TP+FN}$      ‚Äã\tWhen false negatives are costly      Catches more true positives      Can lead to more false positives              Specificity (TNR)      $\\large\\frac{TN}{TN+FP}$      ‚Äã\tWhen true negatives are important      Minimizes false positives      Ignores false negatives              F1-Score      $\\large\\text{2 x }\\frac{Precision \\times Recall}{Precision + Recall}$      When balance between precision and recall is needed      Balances precision and recall      Harder to interpret than individual metrics              AUC-ROC      $\\text{Area Under the ROC Curve}$      Imbalanced datasets with binary classification      Provides insight into model discrimination ability      Less useful for highly skewed datasets              FPR      ‚Äã\t $\\large\\frac{FP}{FP+TN}$      When reducing false alarms is crucial      Helps assess false alarms      Can be high in imbalanced datasets      ConclusionIn conclusion, the selection of metrics depends on the nature of the problem and the trade-offs between different types of errors. While accuracy is easy to understand, it often falls short in real-world applications, especially with imbalanced datasets. Metrics such as precision, recall, sensitivity, and specificity provide a more nuanced view of the model‚Äôs performance. By carefully selecting the most appropriate metrics, you can develop models that are not only performant in theory but also suited to real-world demands."
  },
  
  {
    "title": "Decision Trees | Growing Your Way to Smarter Decisions",
    "url": "/posts/decision-trees/",
    "categories": "",
    "tags": "algorithms, machine-learning, supervised-learning, from-scratch",
    "date": "2023-07-08 00:00:00 +0300",
    





    
    "snippet": "Introduction to Decision TreesDecision trees are one of the most popular and widely used algorithms in machine learning, particularly for classification and regression tasks. They belong to the fam...",
    "content": "Introduction to Decision TreesDecision trees are one of the most popular and widely used algorithms in machine learning, particularly for classification and regression tasks. They belong to the family of supervised learning algorithms and work by splitting the data into smaller subsets based on certain criteria. The result is a tree-like model of decisions that leads to a final output (label). This structure resembles the flow of decisions and their possible outcomes, hence the name ‚Äúdecision tree.‚ÄùDataset Split in a Decision Tree. Image by ml2gifsThe key idea behind decision trees is to select the best feature at each node to partition the data into subsets that are as homogenous as possible. This process is repeated recursively for each subset until the stopping criteria are met, either when the maximum depth is reached or when the data cannot be split any further.Let‚Äôs say you are trying to decide what to wear. Your decision tree might ask: ‚ÄúIs it raining?‚Äù If yes, you grab an umbrella. If not, it might ask: ‚ÄúIs it cold?‚Äù and so on, until you have decided on your outfit for the day. In machine learning, decision trees do the same thing but with data, and instead of picking outfits, they are making predictions or classifications.Why Should You Care About Decision Trees?Ever tried explaining deep learning models to a non-techie friend? Yeah, not easy. But decision trees? They are a breeze to explain. You can see the decisions being made right in front of you, like following a flowchart. This makes them incredibly interpretable ‚Äî you can understand exactly why the model made a particular prediction, which is super handy when you are working with stakeholders who demand transparency.Another perk? Decision trees do not care about whether your data is on the same scale or even if it is numerical or categorical. Whether you are dealing with temperatures or zip codes, decision trees handle it all. Plus, they are great at dealing with complex, non-linear relationships. If there is a twisty path through the data, they will find it!When to Use Decision Trees?  You need something easy to explain: They are perfect for building models that both you and your team can understand and explain to others.  Your data has both numbers and categories: Trees handle both seamlessly, so you do not need to spend time converting everything to the same format.  You suspect there is a complex relationship hiding in your data: Trees will dig deep, splitting your data at every opportunity to find those hidden patterns.Overview of Decision TreesDecision Tree StructureIn a decision tree, each decision(internal) node represents a test or condition on a feature (e.g., ‚ÄúIs Feature 1 greater than 5?‚Äù), each branch represents the outcome of that condition, and each leaf node represents a class label or final decision. The algorithm splits data by selecting the feature and threshold that provides the highest information gain (or lowest impurity).Common Splitting Criteria:      Gini Index: Measures the degree of probability of a particular feature being classified incorrectly when randomly chosen.  \\[G(D) = 1 - \\sum_{i=1}^c p_i^2\\]      Entropy : Measures the amount of uncertainty or disorder in the data, aiming to reduce uncertainty with each split.  \\[H(D) = - \\sum_{i=1}^c p_i \\log_2(p_i)\\]\\[\\text{Information Gain}(D, \\text{split}) = H(D) - \\left( \\frac{|D_{\\text{left}}|}{|D|} H(D_{\\text{left}}) + \\frac{|D_{\\text{right}}|}{|D|} H(D_{\\text{right}}) \\right)\\]In these formulas:  $p_i$‚Äã represents the proportion of samples belonging to class ii.  $c$ is the number of classes.  $‚à£D_{left}‚à£$ and $‚à£D_{right}‚à£$ denote the number of samples in the left and right subsets, respectively.  $H(D)$, $H(D_{left})$, and $H(D_{right})$ are the entropies or Gini impurities before and after the split.Pros and Cons of Decision TreesAdvantages  Crystal clear: The decisions are laid out in a way that anyone can follow.  Flexibility: Trees work well with both numbers and categories, and you don‚Äôt need to do a ton of data prep beforehand.  Handles complexity: Trees can capture complex relationships that simpler models might miss.Disadvantages  Overfitting danger: If you let the tree grow too deep, it starts to memorize the data rather than learn from it‚Äîkind of like a detective who sees conspiracies everywhere. Pruning the tree or limiting its depth helps prevent this.  A bit fickle: Small changes in the data can sometimes result in a totally different tree, which makes them less stable than other models.  Bias risk: Trees can sometimes get obsessed with features that have many levels or categories, ignoring others that might be more important.Building a Decision Tree from Scratch: Step-by-StepLet‚Äôs dive into the algorithm to build a  decision tree from scratch using the Python code provided.1. Initialize the TreeWe start by creating a Node class that will represent each node in the decision tree. Each node will store important information such as the feature index, threshold for splitting, and pointers to left and right child nodes.In otherwords,  this node is like a fork in the road‚Äîit holds the feature we‚Äôre asking about and which way we‚Äôll go based on the answer.class Node:    def __init__(self,feature_index=None,threshold=None,condition_mark=None,left=None,right=None,score=None,criterion=None,information_gain=None,label=None):        self.feature_index = feature_index        self.threshold = threshold         self.condition_mark = condition_mark         self.left = left         self.right = right         self.score = score         self.criterion = criterion        self.information_gain = information_gain        self.label = label 2. Stopping ConditionsBefore splitting the data, we check whether we should stop further splitting. This can happen if the data at the node is pure (contains only one class) or if the maximum depth or minimum samples condition is met.def _should_stop(self,data,depth):    n_labels = len(np.unique(data[:,-1]))    n_samples = len(data)    condition = (n_labels == 1) or (n_samples &lt;= self.min_samples_count) or (depth &gt;= self.max_depth)    return condition 3. Splitting the DataAt each node, we calculate potential splits for both categorical and numerical features. For numerical features, we use the median as a threshold, while for categorical features, we split by unique values.def _get_potential_splits(self,data):    potential_splits_all = []    n_features = data.shape[1] - 1  # [feat_1, feat_2, ..., feat_n, labels]    for feature_idx in range(n_features):        data_feature = data[:,feature_idx]         if isinstance(data_feature[0],str) or isinstance(data_feature[0],bool):            thresholds = np.unique(data_feature)            condition_mark = '=='        else:            thresholds = [np.median(data_feature)]            condition_mark = '&lt;='        potential_splits_all.append({'idx':feature_idx,'thresholds':thresholds,'condition_mark':condition_mark})    return potential_splits_all4. Calculating Information GainNext, we evaluate the information gain for each potential split. The information gain tells us how much uncertainty is reduced after splitting the data. We choose the split that provides the highest information gain.def _calculate_information_gain(self,labels, left_idxs, right_idxs):    p_left = len(left_idxs) / len(labels)    p_right = 1 - p_left     weighted_impurity = p_left * self._calculate_impurity(labels[left_idxs]) + p_right * self._calculate_impurity(labels[right_idxs])    parent_impurity = self._calculate_impurity(labels)    information_gain = parent_impurity - weighted_impurity    return information_gain, weighted_impurity5. Recursive Tree BuildingUsing the best split found, we recursively build the left and right subtrees until a stopping condition is met.def _build_tree(self,data,depth=0):    if self._should_stop(data,depth):        leaf_label = self._get_label_as_majority(data)        return Node(label=leaf_label)    else:        potential_splits = self._get_potential_splits(data)        bests = self._find_best_split(data,potential_splits)        left_tree = self._build_tree(data = data[bests['left_idxs']],depth=depth+1)        right_tree = self._build_tree(data = data[bests['right_idxs']],depth = depth+1)        return Node(feature_index=bests['feature_index'], threshold=bests['threshold'],                    condition_mark=bests['condition_mark'], left=left_tree, right=right_tree,                    score=bests['impurity'], criterion=self.criterion,                    information_gain=bests['information_gain'])6. PredictionTo make predictions, we traverse the tree recursively. Starting from the root, we follow the condition marks and thresholds until we reach a leaf node that holds the predicted label.def _traverse_tree(self, data_point, node):    if node.label is not None:          return node.label    if node.condition_mark == '==':        if data_point[node.feature_index] == node.threshold:            return self._traverse_tree(data_point, node.left)        else:            return self._traverse_tree(data_point, node.right)    else:        if data_point[node.feature_index] &lt;= node.threshold:            return self._traverse_tree(data_point, node.left)        else:            return self._traverse_tree(data_point, node.right)Testing the Decision Tree on Real DatasetsIris Dataset. Image by DatacampWe can test our decision tree on well-known datasets such as the Iris dataset.def test_with_iris_dataset():    from sklearn.datasets import load_iris    from sklearn.model_selection import train_test_split    from sklearn.metrics import accuracy_score        data = load_iris()    X, y = data.data, data.target    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)    tree = DecisionTree(max_depth=3, min_samples_count=2, criterion='entropy')    tree.fit(X_train, y_train)    y_pred = tree.predict(X_test)    accuracy = accuracy_score(y_test, y_pred)    print(f\"Test accuracy: {accuracy * 100:.2f}%\")Iris Dataset Decision TreeConclusionDecision trees are powerful tools for both classification and regression tasks. They are easy to interpret and flexible, making them an excellent choice for many machine learning problems. However, care must be taken to prevent overfitting, which can be mitigated by using techniques such as pruning or by using ensemble methods like Random Forests or Gradient Boosting.By implementing decision trees from scratch, we not only deepen our understanding of the algorithm but also gain the ability to customize it to better suit specific tasks.  Decision Tree Codes from Scratch Using the PythonFull Codeimport numpy as np class Node:    def __init__(self,feature_index=None,threshold=None,condition_mark=None,left=None,right=None,score=None,criterion=None,information_gain=None,label=None):        self.feature_index = feature_index        self.threshold = threshold         self.condition_mark = condition_mark         self.left = left         self.right = right         self.score = score         self.criterion = criterion        self.information_gain = information_gain        self.label = label         class DecisionTree:    def __init__(self,max_depth=5,min_samples_count=3,criterion='entropy'):        self.max_depth = max_depth        self.min_samples_count = min_samples_count         self.tree = None         self.criterion = criterion         self.depth = 0         def _should_stop(self,data,depth):        n_labels = len(np.unique(data[:,-1]))        n_samples = len(data)        condition = (n_labels == 1) or (n_samples &lt;= self.min_samples_count) or (depth &gt;= self.max_depth)        return condition         def _get_label_as_majority(self,data):        labels,counts = np.unique(data[:,-1],return_counts=True)        idx_max = np.argmax(counts)        return labels[idx_max]        def _get_potential_splits(self,data):        potential_splits_all = []        n_features = data.shape[1] - 1 # [feat_1,feat_2,...,feat_n,labels]        for feature_idx in range(n_features): # iterate over all features            data_feature = data[:,feature_idx]                 if isinstance(data_feature[0],str) or isinstance(data_feature[0],bool):                thresholds = np.unique(data_feature)                condition_mark = '=='                potential_splits_all.append({'idx':feature_idx,'thresholds':thresholds,'condition_mark':condition_mark})            else:                thresholds = [np.median(data_feature)]                condition_mark = '&lt;='                potential_splits_all.append({'idx':feature_idx,'thresholds':thresholds,'condition_mark':condition_mark})        return potential_splits_all        def _find_best_split(self,data,potential_splits):        bests = {'feature_index':None,'threshold':None,'condition_mark':None,                   'information_gain':-float(\"inf\"),'impurity':None,'left_idxs':None,'right_idxs':None}                labels = data[:,-1]                for row in potential_splits:            feature_idx = row[\"idx\"]            thresholds = row[\"thresholds\"]            condition_mark = row[\"condition_mark\"]            features = data[:,feature_idx]            for threshold in thresholds:                if condition_mark == '==': # for categorical features                     cond = np.array([x == threshold for x in features])                else: # for numerical features                     cond = np.array([x &lt;= threshold for x in features])                                    left_idxs = np.where(cond)[0]                right_idxs = np.where(~cond)[0]                information_gain,impurity = self._calculate_information_gain(labels, left_idxs, right_idxs)                  if information_gain &gt; bests['information_gain']:                    dct = {'feature_index':feature_idx,'threshold':threshold,                           'condition_mark':condition_mark,'information_gain':information_gain,'impurity':impurity,                           'left_idxs':left_idxs,'right_idxs':right_idxs}                    bests.update(dct)                return bests                                            def _calculate_information_gain(self,labels, left_idxs, right_idxs):        if len(left_idxs) == 0 or len(right_idxs) == 0 :            information_gain, weighted_impurity = 0 ,0             return information_gain, weighted_impurity         else:            p_left = len(left_idxs) / len(labels)            p_right = 1 - p_left             weighted_impurity = p_left * self._calculate_impurity(labels[left_idxs]) + p_right * self._calculate_impurity(labels[right_idxs])            parent_impurity = self._calculate_impurity(labels)            information_gain = parent_impurity - weighted_impurity            return information_gain, weighted_impurity        def _calculate_impurity(self,labels):        if self.criterion == 'gini':            return self._calculate_gini(labels)        elif self.criterion == 'entropy':            return self._calculate_entropy(labels)        else:            raise Exception(\"Criterion must be 'gini' or 'entropy'.\")                def _calculate_entropy(self,labels):        _,counts= np.unique(labels,return_counts=True)        probs = counts / np.sum(counts)        score = -np.sum(probs*np.log2(probs+1e-9))# Add small value to avoid log(0)        return score         def _calculate_gini(self,labels):        _,counts= np.unique(labels,return_counts=True)        probs = counts / np.sum(counts)        score = 1 - np.sum(np.power(probs,2))         return score         def _build_tree(self,data,depth=0):        if self._should_stop(data,depth):            leaf_label = self._get_label_as_majority(data)            return Node(label=leaf_label)        else:            potential_splits = self._get_potential_splits(data)            bests = self._find_best_split(data,potential_splits)            left_tree = self._build_tree(data = data[bests['left_idxs']],depth=depth+1)            right_tree = self._build_tree(data = data[bests['right_idxs']],depth = depth+1)                        return Node(feature_index=bests['feature_index'],threshold=bests['threshold'],                        condition_mark=bests['condition_mark'],left=left_tree,right=right_tree,                        score=bests['impurity'],criterion=self.criterion,                        information_gain=bests['information_gain'])            def fit(self,X,y):        data = np.column_stack((X,y))        self.tree = self._build_tree(data)         def predict(self,X):        predictions = [self._traverse_tree(data_point, self.tree) for data_point in X]        return predictions    # Traverse the tree recursively to predict the label    def _traverse_tree(self, data_point, node):        if node.label is not None:  # If we're at a leaf, return the label            return node.label                if node.condition_mark == '==':            if data_point[node.feature_index] == node.threshold:                return self._traverse_tree(data_point, node.left)            else:                return self._traverse_tree(data_point, node.right)        else:            if data_point[node.feature_index] &lt;= node.threshold:                return self._traverse_tree(data_point, node.left)            else:                return self._traverse_tree(data_point, node.right)                        def test_with_iris_dataset():    from sklearn.datasets import load_iris    from sklearn.model_selection import train_test_split    from sklearn.metrics import accuracy_score        data = load_iris()    X, y = data.data, data.target    # Split the data into training and testing sets    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)    tree = DecisionTree(max_depth=3, min_samples_count=2, criterion='gini')    tree.fit(X_train, y_train)    # Make predictions on the test set    y_pred = tree.predict(X_test)    # Calculate accuracy    accuracy = accuracy_score(y_test, y_pred)    print(f\"Gini Test accuracy: {accuracy * 100:.2f}%\")    # Create and train the decision tree with 'entropy' criterion    tree_entropy = DecisionTree(max_depth=3, min_samples_count=2, criterion='entropy')    tree_entropy.fit(X_train, y_train)    # Make predictions on the test set using entropy criterion    y_pred_entropy = tree_entropy.predict(X_test)    # Calculate accuracy    accuracy_entropy =  accuracy_score(y_test, y_pred_entropy)    print(f\"Entropy Test accuracy: {accuracy_entropy * 100:.2f}%\")"
  },
  
  {
    "title": "Apriori Algorithm from Scratch | Building a Netflix Film Recommendation System",
    "url": "/posts/apriori/",
    "categories": "",
    "tags": "algorithms, machine-learning, unsupervised-learning, from-scratch, association-rule",
    "date": "2023-06-29 00:00:00 +0300",
    





    
    "snippet": "IntroductionThe Apriori algorithm is a fundamental technique in association rule mining, often used to analyze transaction data and find frequent itemsets. This algorithm works by discovering relat...",
    "content": "IntroductionThe Apriori algorithm is a fundamental technique in association rule mining, often used to analyze transaction data and find frequent itemsets. This algorithm works by discovering relationships in data, such as which items are commonly purchased together. In the case of Netflix, we can use Apriori to recommend movies to users based on what others with similar viewing histories have watched.Apriori algorithm finds frequent itemsets by iteratively scanning through transaction data and pruning infrequent items. Once frequent itemsets are identified, association rules can be generated to predict which items (or movies, in our case) are likely to occur together.In this blog post, we‚Äôll break down the Apriori algorithm step by step, use a movie recommendation system as an example, and implement the algorithm from scratch. We‚Äôll also explore when and why you might use Apriori, its strengths and weaknesses, and conclude with a look at related algorithms. In a Netflix movie recommendation scenario, Apriori could help identify patterns like ‚ÄúIf a user watches ‚ÄòThe Matrix,‚Äô they are also likely to watch ‚ÄòInception‚Äô.‚ÄùMathematical Foundations of AprioriThe Apriori algorithm operates on 3 key metrics: support, confidence, lift. Selecting appropriate values for minimum support, minimum confidence, and lift is crucial in association rule mining to balance finding meaningful rules and managing computational efficiency. Let‚Äôs break them down:  Support measures how often an item or itemset appears in the dataset. It helps filter out infrequent items.\\[\\text{Support}(X) = \\frac{\\text{Number of transactions containing } X}{\\text{Total number of transactions}}\\]  Confidence measures the likelihood of a consequent item appearing given the antecedent item. This is essential for generating reliable rules.\\[\\text{Confidence}(X \\rightarrow Y) = \\frac{\\text{Support}(X, Y)}{\\text{Support}(X)}\\]  Lift measures how much more likely item YY is purchased when item XX is purchased compared to its usual purchase rate. Let‚Äôs calculate the lift for $A \\rightarrow B$.\\[\\text{Lift}(X \\rightarrow Y) = \\frac{\\text{Confidence}(X \\rightarrow Y)}{\\text{Support}(Y)} = \\frac{0.75}{0.8} = 0.9375\\]These metrics are used to generate association rules, such as recommending movies to Netflix users based on past viewing habits.Netflix Example: Movie Recommendation SystemTo illustrate how the Apriori algorithm works, let‚Äôs consider an example using five hypothetical Netflix transactions where users watched various movies:            Transaction ID (User ID)      Movies Watched                  1      The Matrix, Inception, Interstellar              2      Inception, Interstellar              3      The Matrix, Interstellar              4      The Matrix, Inception              5      Inception, Interstellar      Our goal is to discover frequent itemsets and generate rules like ‚ÄúIf a user watched ‚ÄòThe Matrix‚Äô, they are likely to watch ‚ÄòInception‚Äô.‚ÄùStep-by-Step Breakdown of the Apriori AlgorithmStep 1: Define ItemsetsLet‚Äôs define the following itemsets and rules:  Items: ${set(The Matrix, Inception, Interstellar)}$  Itemsets:          Single items: ${set(The Matrix)}, set({Inception}), set({Interstellar}) $      Pairs: $set({The Matrix, Inception}), set({The Matrix, Interstellar}), set({Inception, Interstellar})$      Step 2: Calculate Support  Support is the proportion of transactions that contain the itemset. Minimum support filters out infrequent itemsets, with typical values ranging from 0.1 to 0.5, with common choices around 0.2 to 0.3.Support for Single Items:\\[\\text{Support(The Matrix)} = \\frac{\\text{Number of transactions containing The Matrix}}{\\text{Total number of transactions}} = \\frac{3}{5} = 0.6\\]\\[\\text{Support(Inception)} = \\frac{\\text{Number of transactions containing Inception}}{\\text{Total number of transactions}} = \\frac{4}{5} = 0.8\\]\\[\\text{Support(Interstellar)} = \\frac{\\text{Number of transactions containing Interstellar}}{\\text{Total number of transactions}} = \\frac{4}{5} = 0.8\\]Support for Item Pairs:\\[\\text{Support(The Matrix, Inception)} = \\frac{\\text{Number of transactions containing The Matrix and Inception}}{\\text{Total number of transactions}} = \\frac{2}{5} = 0.4\\]\\[\\text{Support(The Matrix, Interstellar)} = \\frac{\\text{Number of transactions containing The Matrix and Interstellar}}{\\text{Total number of transactions}} = \\frac{3}{5} = 0.6\\]\\[\\text{Support(Inception, Interstellar)} = \\frac{\\text{Number of transactions containing Inception and Interstellar}}{\\text{Total number of transactions}} = \\frac{3}{5} = 0.6\\]Step 3: Calculate Confidence  Confidence measures how often items in $Y$ appear in transactions that contain $X$. Minimum confidence measures rule reliability, usually set between 0.5 and 0.9, with 0.7 or 0.8 often used for strong predictive power.\\[\\text{Confidence}(The Matrix \\rightarrow Inception) = \\frac{\\text{Support(The Matrix, Inception)}}{\\text{Support(The Matrix)}} = \\frac{0.4}{0.6} = 0.67\\]\\[\\text{Confidence}(The Matrix \\rightarrow Interstellar) = \\frac{\\text{Support(The Matrix, Interstellar)}}{\\text{Support(The Matrix)}} = \\frac{0.6}{0.6} = 1.0\\]\\[\\text{Confidence}(Inception \\rightarrow Interstellar) = \\frac{\\text{Support(Inception, Interstellar)}}{\\text{Support(Inception)}} = \\frac{0.6}{0.8} = 0.75\\]Step 4: Calculate Lift  Lift measures the strength of a rule over the baseline probability of occurrence of $Y$. Lift evaluates the strength of associations, with values above 1 indicating positive correlations, and significantly higher values reflecting stronger associations.\\[\\text{Lift}(The Matrix \\rightarrow Inception) = \\frac{\\text{Confidence}(The Matrix \\rightarrow Inception)}{\\text{Support(Inception)}} = \\frac{0.67}{0.8} = 0.84\\]\\[\\text{Lift}(The Matrix \\rightarrow Interstellar)= \\frac{\\text{Confidence}(The Matrix \\rightarrow Interstellar)}{\\text{Support(Interstellar)}} = \\frac{1.0}{0.8} = 1.25\\]\\[\\text{Lift}(Inception \\rightarrow Interstellar) = \\frac{\\text{Confidence}(Inception \\rightarrow Interstellar)}{\\text{Support(Interstellar)}} = \\frac{0.75}{0.8} = 0.9375\\]Step 5: Generating Association RulesUsing the confidence and lift values, the association rule $\\text{The Matrix} \\rightarrow \\text{Inception}$ suggests that if a user watches Movie $\\text{The Matrix}$, there is a 67% chance they will watch Movie $\\text{Inception}$, and the lift indicates this is slightly less than expected by chance (since $Lift &lt; 1$).Wrap it up!In summary, the Apriori algorithm is a powerful tool for uncovering associations such as movie recommendations on Netflix. By setting appropriate thresholds for minimum support, minimum confidence, and lift, you can effectively manage the balance between discovering meaningful rules and optimizing computational resources. Understanding and adjusting these parameters based on your dataset‚Äôs size and domain-specific requirements is key to deriving valuable insights.Exploring the Apriori algorithm and its parameter settings can lead to actionable patterns and recommendations, enhancing user experiences and business strategies. As you implement and refine your association rule mining approach, consider experimenting with different thresholds and comparing results with other algorithms, such as FP-Growth or Eclat, to find the most effective method for your specific needs. The insights gained from these analyses not only improve recommendation systems but also provide a deeper understanding of user behaviors and preferences.Apriori from Scratch in PythonThe provided Python code implements the Apriori algorithm to identify frequent itemsets and generate association rules from transactional data, such as movie recommendations. The Apriori class initializes with minimum support and confidence thresholds and optionally an itemset size limit. Key methods include _find_unique_items_in_all_transactions, which extracts unique items from transactions; _find_candidates, which generates candidate itemsets of a specified size; and _calculate_support, which computes the support for each candidate itemset. The algorithm iterates through itemset sizes, filtering frequent itemsets based on the minimum support. It then generates association rules from these itemsets, calculating their confidence and retaining those that meet the minimum confidence threshold. The fit method orchestrates the entire process‚Äîpreparing transactions, finding frequent itemsets, and generating rules‚Äîwhile get_frequent_itemsets and get_rules methods retrieve the results. This code highlights the Apriori algorithm‚Äôs ability to extract meaningful patterns from data, making it a powerful tool for recommendation systems and market basket analysis.From Scratchfrom itertools import combinationsclass Apriori:    def __init__(self, min_support, min_confidence, itemset_limit=None):        self.min_support = min_support        self.min_confidence = min_confidence        self.itemset_limit = itemset_limit        self.frequent_itemsets = {}  # Changed from set() to dict() to store support values        self.rules = []    def _find_unique_items_in_all_transactions(self):        unique_items = set(item for transaction in self.transactions_list for item in transaction)        return unique_items    def _find_candidates(self, elem_count):        # Generate itemset combinations of size `elem_count`        candidates = set(frozenset(itemset) for itemset in combinations(self.unique_items, elem_count))        return candidates    def _prepare_transactions(self, transactions):        self.transactions = transactions        self.transactions_list = [set(t) for t in transactions]  # Convert each transaction to a set        self.unique_items = self._find_unique_items_in_all_transactions()        # Set itemset limit to the number of unique items if not provided        self.itemset_limit = len(self.unique_items) if self.itemset_limit is None else self.itemset_limit    def _calculate_support(self, itemset):        transaction_count = len(self.transactions_list)        subset_count = sum(1 for transaction in self.transactions_list if itemset.issubset(transaction))        support = subset_count / transaction_count        return support    def _filter_frequent_itemsets(self, candidates):        frequent_itemsets = {}        for candidate in candidates:            support = self._calculate_support(candidate)            if support &gt;= self.min_support:                frequent_itemsets[candidate] = support        return frequent_itemsets    def _find_frequent_itemsets(self):        for k in range(1, self.itemset_limit + 1):            print(\"Checking itemsets of length:\", k)            candidates = self._find_candidates(k)            curr_itemsets = self._filter_frequent_itemsets(candidates)                        if not curr_itemsets:                print(\"No frequent itemsets found for size\", k)                break  # Exit if no frequent itemsets are found                        print(\"Current frequent itemsets:\", curr_itemsets)            self.frequent_itemsets.update(curr_itemsets)    def _generate_association_rules(self):        \"\"\"Generate all possible association rules from the frequent itemsets.\"\"\"        for itemset, itemset_support in self.frequent_itemsets.items():            if len(itemset) &lt; 2:                continue  # No rules can be generated from single-item sets                        # Generate all non-empty subsets of the itemset (possible antecedents)            for antecedent_size in range(1, len(itemset)):                for antecedent in combinations(itemset, antecedent_size):                    antecedent = frozenset(antecedent)                    consequent = itemset - antecedent                                        if consequent:                        antecedent_support = self.frequent_itemsets.get(antecedent, 0)                        if antecedent_support &gt; 0:  # To avoid division by zero                            confidence = itemset_support / antecedent_support                            if confidence &gt;= self.min_confidence:                                rule = {                                    'antecedent': antecedent,                                    'consequent': consequent,                                    'confidence': confidence,                                    'support': itemset_support                                }                                self.rules.append(rule)    def fit(self, transactions):        self._prepare_transactions(transactions)        self._find_frequent_itemsets()        self._generate_association_rules()    def get_frequent_itemsets(self):        if not self.frequent_itemsets:            print(\"Frequent itemsets not found. Please fit the model first.\")        return self.frequent_itemsets    def get_rules(self):        if not self.rules:            print(\"No rules generated. Please fit the model and generate association rules.\")        return self.rulesif __name__ == \"__main__\":    apr = Apriori(min_support=0.3, min_confidence=0.7)        transactions_by_name = [    ['The Matrix', 'Inception', 'Interstellar'],    ['Inception', 'Interstellar'],    ['The Matrix', 'Interstellar'],    ['The Matrix', 'Inception'],    ['Inception', 'Interstellar']    ]        apr.fit(transactions_by_genre)    frequent_itemsets = apr.get_frequent_itemsets()    rules = apr.get_rules()        print(\"Frequent Itemsets:\", frequent_itemsets)    print(\"Association Rules:\")    for rule in rules:        print(f\"Rule: {rule['antecedent']} -&gt; {rule['consequent']}, Confidence: {rule['confidence']:.2f}, Support: {rule['support']:.2f}\")With mlxtendFirstly let‚Äôs install packages pip install pandas mlxtendimport pandas as pdfrom mlxtend.preprocessing import TransactionEncoderfrom mlxtend.frequent_patterns import apriori, association_rules# Sample transactionstransactions = [    ['The Matrix', 'Inception', 'Interstellar'],    ['Inception', 'Interstellar'],    ['The Matrix', 'Interstellar'],    ['The Matrix', 'Inception'],    ['Inception', 'Interstellar']]# Convert transactions to DataFramete = TransactionEncoder()te_ary = te.fit(transactions).transform(transactions)df = pd.DataFrame(te_ary, columns=te.columns_)# Apply Apriori algorithmmin_support = 0.3frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)# Generate association rulesmin_confidence = 0.7rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)# Output resultsprint(\"Frequent Itemsets:\")print(frequent_itemsets)print(\"\\nAssociation Rules:\")for _, row in rules.iterrows():    print(f\"Rule: {set(row['antecedents'])} -&gt; {set(row['consequents'])}, Confidence: {row['confidence']:.2f}, Support: {row['support']:.2f}\")"
  },
  
  {
    "title": "K-means Clustering",
    "url": "/posts/kmeans/",
    "categories": "",
    "tags": "algorithms, machine-learning, unsupervised-learning, from-scratch",
    "date": "2023-06-17 00:00:00 +0300",
    





    
    "snippet": "Understanding the K-Means Algorithm: A Simple GuideThe K-Means algorithm is a widely used method in machine learning for grouping data points into clusters. In this post, I‚Äôll explain how it works,...",
    "content": "Understanding the K-Means Algorithm: A Simple GuideThe K-Means algorithm is a widely used method in machine learning for grouping data points into clusters. In this post, I‚Äôll explain how it works, how to implement it, and how to evaluate its performance. I‚Äôll also touch on some advantages and disadvantages of the K-Means algorithm and compare it to other clustering techniques.What is K-Means?At its core, K-Means is a way to group similar data points together. You tell the algorithm how many groups (or clusters) you want, and it will organize your data accordingly.How Does K-Means Work?K-means Algorithm Process  Here‚Äôs a step-by-step breakdown of how K-Means clusters your data:      Step‚Äî1 Choose K Initial Points: The algorithm starts by picking K random points from your data. These points act as the initial ‚Äúcentroids‚Äù or centers of the clusters.    Step‚Äî2 Assign Data Points to Clusters: Each data point is assigned to the nearest centroid. This forms your initial clusters.    Step‚Äî3 Update Centroids: The algorithm recalculates the centroids of these clusters based on the current members of each cluster.    Step‚Äî4 Repeat: Steps 2 and 3 are repeated until the centroids no longer change significantly, meaning the clusters have stabilized.  The Math Behind K-MeansThe K-Means algorithm tries to minimize the distance between data points and their assigned cluster centroids. This distance is usually measured using the Euclidean distance formula:\\[d(x_i, \\mu_j) = \\sqrt{\\sum_{m=1}^{n} (x_{im} - \\mu_{jm})^2}\\]Where:  \\(x_i\\)‚Äã is a data point.  \\(Œº_j\\)‚Äã is the centroid of the cluster.  \\(n\\) is the number of features.The goal is to minimize the total sum of squared errors:\\[J = \\sum_{j=1}^{K} \\sum_{i=1}^{N_j} \\|x_i^{(j)} - \\mu_j\\|^2\\]Implementing K-Means from ScratchHere‚Äôs how you can write the K-Means algorithm as a Python class. This class allows you to customize things like the number of clusters (K) and how distances are calculated.import numpy as npclass KMeansCustom:    def __init__(self, K=3, distance_metric='euclidean', init='random', max_iters=300, tol=1e-4, random_state=None):        self.K = K        self.distance_metric = distance_metric        self.init = init        self.max_iters = max_iters        self.tol = tol        self.random_state = random_state               def _init_kmeans_plus_plus(self, X):        # K-Means++ Initialization        n_samples, n_features = X.shape        centroids = np.empty((self.K, n_features))        # Choose the first centroid randomly        centroids[0] = X[np.random.randint(n_samples)]                # Compute the remaining centroids        for k in range(1, self.K):            distances = np.min(np.linalg.norm(X[:, np.newaxis] - centroids[:k], axis=2), axis=1)            probabilities = distances / distances.sum()            next_centroid = X[np.random.choice(n_samples, p=probabilities)]            centroids[k] = next_centroid                return centroids    def fit(self, X):        np.random.seed(self.random_state)                # K-Means++ Initialization        if self.init == 'kmeans++':            centroids = self._init_kmeans_plus_plus(X)        else:            # Random initialization            centroids = X[np.random.choice(X.shape[0], self.K, replace=False)]                labels = np.zeros(X.shape[0])        for _ in range(self.max_iters):            # Compute distances            distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)            # Assign labels based on closest centroid            new_labels = np.argmin(distances, axis=1)            # Recompute centroids            new_centroids = np.array([X[new_labels == k].mean(axis=0) for k in range(self.K)])                        # Check for convergence            if np.all(np.linalg.norm(new_centroids - centroids, axis=1) &lt; self.tol):                break            labels = new_labels            centroids = new_centroids        return centroids, labelsEvaluating the Performance of K-MeansEvaluating how well K-Means performed is crucial. Here are some common methods:  Inertia: This measures how tightly the data points are clustered around the centroids. Lower inertia indicates better clustering.  Silhouette Score: This score shows how similar each data point is to its own cluster compared to other clusters. Scores close to 1 mean better clustering.The Silhouette Score is calculated as:\\[S(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\\]Where:  \\(a(i)\\) is the average distance between a data point and all other points in the same cluster.  \\(b(i)\\)b is the average distance between a data point and the points in the nearest different cluster.Custom KMeans Model Testfrom sklearn.datasets import make_blobs# Generate synthetic dataX, y = make_blobs(n_samples=500, centers=5, cluster_std=0.70, random_state=42)from sklearn.metrics import silhouette_score, davies_bouldin_scoreimport matplotlib.pyplot as plt# Train the KMeans modelkmeans_random = KMeansCustom(K=5, init='random', max_iters=300, tol=1e-4, random_state=42)centroids, labels = kmeans_random.fit(X)# Calculate evaluation metricssil_score = silhouette_score(X, labels)db_score = davies_bouldin_score(X, labels)# Plot the clustering results with legendsplt.figure(figsize=(10, 6))scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.8, edgecolors='w', s=50)# Add centroid pointsplt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='red', marker='X', label='Centroids')# Add legend for clustersunique_labels = np.unique(labels)for label in unique_labels:    plt.scatter([], [], color=plt.cm.viridis(label / max(labels)), label=f'Cluster {label + 1}')plt.title('K-Means Clustering with Custom Model')plt.xlabel('Feature 1')plt.ylabel('Feature 2')plt.legend()plt.colorbar(scatter, label='Cluster')plt.show()print(f\"Silhouette Score: {sil_score:.3f}\")print(f\"Davies-Bouldin Score: {db_score:.3f}\")Custom K-means Model Test ResultsImplementing K-Means with Scikit-LearnHere‚Äôs how you can use Scikit-learn‚Äôs built-in K-Means function and evaluate its performance.from sklearn.cluster import KMeans# Train the KMeans modelkmeans = KMeans(n_clusters=4,max_iter=100,tol=1e-4,random_state=42)kmeans.fit(X)labels = kmeans.labels_centroids = kmeans.cluster_centers_# Calculate evaluation metricssil_score = silhouette_score(X, labels)db_score = davies_bouldin_score(X, labels)# Plot the clustering results with legendsplt.figure(figsize=(10, 6))scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.8, edgecolors='w', s=50)# Add centroid pointsplt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='red', marker='X', label='Centroids')# Add legend for clustersunique_labels = np.unique(labels)for label in unique_labels:    plt.scatter([], [], color=plt.cm.viridis(label / max(labels)), label=f'Cluster {label + 1}')plt.title('K-Means Clustering with Legends')plt.xlabel('Feature 1')plt.ylabel('Feature 2')plt.legend()plt.colorbar(scatter, label='Cluster')plt.show()print(f\"Silhouette Score: {sil_score:.3f}\")print(f\"Davies-Bouldin Score: {db_score:.3f}\")Scikit-Learn K-means Model TestScikit-learn K-means Model Test ResultsThe Elbow MethodThe Elbow Method is used to determine the optimal number of clusters for K-Means. It involves plotting the inertia (or sum of squared distances from points to their assigned centroids) against different values of K. The ‚Äúelbow‚Äù point in the plot, where the rate of decrease sharply changes, indicates the optimal number of clusters. With the Python code below, the optimal k was determined using the inertia and elbow method, and a code was written to calculate the optimal k according to the maximum silhouette score.Implementing the Elbow MethodHere‚Äôs how you can use the Elbow Method to find the best K:from kneed import KneeLocator #pip install kneed# Compute KMeans for a range of K values and store inertia and silhouette scoresinertia = []silhouette_scores = []K_range = range(1, 11)for K in K_range:    kmeans = KMeans(n_clusters=K, random_state=42)    kmeans.fit(X)    inertia.append(kmeans.inertia_)    if K &gt; 1:  # Silhouette score is not defined for K=1        labels = kmeans.labels_        silhouette_scores.append(silhouette_score(X, labels))    else:        silhouette_scores.append(None)  # Placeholder for K=1# Convert inertia and silhouette_scores to numpy arrays for further processinginertia = np.array(inertia)silhouette_scores = np.array(silhouette_scores)# Apply Kneedle Algorithm to find the optimal K based on inertiakneedle = KneeLocator(K_range, inertia, curve='convex', direction='decreasing')optimal_k_inertia = kneedle.elbow# Find the optimal K based on silhouette scoresoptimal_k_silhouette = K_range[np.argmax(silhouette_scores[1:]) + 1]  # Skip K=1 for silhouette scores# Plot the Elbow Method with Kneedle Detection, Silhouette Scores, and Optimal Kfig, ax1 = plt.subplots(figsize=(12, 6))# Plot Inertiacolor = 'tab:blue'ax1.set_xlabel('Number of Clusters (K)')ax1.set_ylabel('Inertia', color=color)ax1.plot(K_range, inertia, marker='o', linestyle='-', color=color, markersize=8, linewidth=2, label='Inertia')ax1.tick_params(axis='y', labelcolor=color)# Plot Silhouette Scoresax2 = ax1.twinx()  # Instantiate a second y-axis that shares the same x-axiscolor = 'tab:red'ax2.set_ylabel('Silhouette Score', color=color)  # We already handled the x-label with ax1ax2.plot(K_range[1:], silhouette_scores[1:], marker='o', linestyle='--', color=color, markersize=8, linewidth=2, label='Silhouette Score')ax2.tick_params(axis='y', labelcolor=color)# Highlight the optimal K pointsax1.scatter(optimal_k_inertia, inertia[optimal_k_inertia - 1], color='black', s=100, edgecolor='black', zorder=5)ax1.annotate(f'Optimal K (Inertia) = {optimal_k_inertia}',             xy=(optimal_k_inertia, inertia[optimal_k_inertia - 1]),              xytext=(optimal_k_inertia + 1, inertia[optimal_k_inertia - 1] + 1000),             arrowprops=dict(facecolor='black', shrink=0.05),             fontsize=12, color='black')ax1.scatter(optimal_k_silhouette, inertia[optimal_k_silhouette - 1], color='green', s=100, edgecolor='black', zorder=5)ax1.annotate(f'Optimal K (Silhouette) = {optimal_k_silhouette}',             xy=(optimal_k_silhouette, inertia[optimal_k_silhouette - 1]),              xytext=(optimal_k_silhouette + 1, inertia[optimal_k_silhouette - 1] + 1000),             arrowprops=dict(facecolor='green', shrink=0.05),             fontsize=12, color='green')fig.tight_layout()  # Adjust layout to fit both y-axesplt.title('Elbow Method with Kneedle Detection, Silhouette Scores, and Optimal K')plt.grid(True)plt.show()Elbow MethodAdvantages and Disadvantages of K-MeansAdvantages:  Simplicity: Easy to understand and implement.  Scalability: Works well with large datasets.  Speed: Fast and efficient for clustering.Disadvantages:  Sensitivity to Initial Centroids: The final clusters can depend heavily on the initial centroids.  Assumes Spherical Clusters: K-Means assumes that clusters are spherical, which might not always be true.  Fixed Number of Clusters: You must decide the number of clusters (K) beforehand.ConclusionK-Means is a powerful tool for grouping and understanding your data. You can implement this algorithm from scratch using a class structure or use the Scikit-learn library to bring it to life. As can be seen from the results above, the k means algorithm in scikit-learn is better than the custom kmeans when we look at both the silhouette score and the davies bouldin score, so it would be better to use the model that we will call from scikit-learn for kmeans.Also selecting the right hyperparameters can significantly impact the algorithm‚Äôs performance, so it‚Äôs essential to use evaluation methods to measure your model‚Äôs success.However, K-Means has its limitations. For example, it assumes that clusters are spherical and equally sized, which may not always be true in real-world data. Additionally, K-Means is sensitive to the initial placement of centroids, which can lead to different results on different runs.For better clustering results, you might want to consider other algorithms, such as:  Hierarchical Clustering: Suitable for creating a hierarchy of clusters, especially when you don‚Äôt know the number of clusters beforehand.  DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Effective in finding clusters of arbitrary shape and handling noise in the data.  Gaussian Mixture Models (GMM): Provides a probabilistic approach to clustering, allowing for clusters of different shapes and sizes.  Each of these alternatives can offer better performance depending on the characteristics of your data. By understanding and applying the right clustering algorithm, you can unlock deeper insights from your data."
  },
  
  {
    "title": "Naive Bayes Simplified | From Theory to Code",
    "url": "/posts/naive-bayes/",
    "categories": "",
    "tags": "algorithms, machine-learning, supervised-learning, from-scratch",
    "date": "2023-04-18 00:00:00 +0300",
    





    
    "snippet": "IntroductionIn the realm of machine learning and statistical classification, Naive Bayes stands out as one of the simplest and most effective algorithms. Its name stems from the ‚Äúnaive‚Äù assumption ...",
    "content": "IntroductionIn the realm of machine learning and statistical classification, Naive Bayes stands out as one of the simplest and most effective algorithms. Its name stems from the ‚Äúnaive‚Äù assumption of feature independence, which, despite being a strong assumption, often leads to surprisingly accurate results. This blog post will guide you through the fundamental concepts of Naive Bayes, demonstrating its utility through real-life examples and a detailed step-by-step mathematical exploration. We will also cover various types of Naive Bayes classifiers, each tailored for different types of data, and implement them from scratch in Python.Bayes‚Äô Theorem: Foundation and FormulaBayes‚Äô theorem is a fundamental concept in probability theory and statistics, providing a way to update the probability of a hypothesis based on new evidence. It is named after the Reverend Thomas Bayes, who formulated it in the 18th century.The formula for Bayes‚Äô theorem is:\\[P(C \\mid x) = \\frac{P(x \\mid C) \\cdot P(C)}{P(x)}\\]Where:  $P(C‚à£x)$ is the posterior probability of class $C$ given the feature $x$.  $P(x‚à£C)$ is the likelihood of feature $x$ given class $C$.  $P(C)$ is the prior probability of class $C$.  $P(x)$ is the marginal probability of feature $x$.  Foundation: Bayes‚Äô theorem provides a way to update our beliefs about the likelihood of a class given new evidence. In classification problems, this means we can calculate the probability of a data point belonging to a particular class based on its features.Real-Life Usage Examples      Spam Filtering: Naive Bayes is widely used in email spam filters. By analyzing the frequency of words in spam and non-spam emails, the classifier can predict whether a new email is spam.        Text Classification: In sentiment analysis or document categorization, Naive Bayes can classify text into different categories based on the frequency of words.        Medical Diagnosis: Naive Bayes can be used to predict the likelihood of a disease based on symptoms and medical history.        Recommendation Systems: Naive Bayes can help recommend products based on user behavior and preferences.  Basic Problem and Mathematical SolutionLet‚Äôs solve a basic Naive Bayes problem step-by-step.** Problem Statement:** Suppose we have a dataset with two features $x_1$‚Äã and $x_2$‚Äã and a binary class label $C$. We want to classify a new data point ($x_1^‚Ä≤$,$x_2^‚Ä≤$).Step 1: Calculate Prior Probabilities  Prior probability is the initial probability of a hypothesis (or class) before observing any data. It represents the baseline belief about the class before considering the evidence.\\[P(C) = \\frac{\\text{Number of instances in class } C}{\\text{Total number of instances}} = \\frac{N_c}{N}\\]Where:  $P(C)$ is the prior probability of class $C$,  $N_c$‚Äã is the number of instances in class $C$‚Äã,  $N$ is the total number of instances.    Step 2: Calculate Likelihoods    Likelihood is the probability of observing the data given a particular hypothesis (or class). In Naive Bayes, it is the probability of the features given the class.For Gaussian Naive Bayes:\\(P(x_i \\mid C) = \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}} \\exp \\left( -\\frac{(x_i - \\mu_i)^2}{2 \\sigma_i^2} \\right)\\)Where $Œº_i$‚Äã and $œÉ_i^2$‚Äã are the mean and variance of feature $x_i$‚Äã in class $C$.Step 3: Calculate Posterior Probability  Marginal Probability $P(x)$: The overall probability of the features xx across all classes. It acts as a normalization factor to ensure that the posterior probabilities sum to 1.  Posterior probability is the probability of a particular outcome or hypothesis after considering new evidence or data. It represents an updated belief about the hypothesis once the evidence is taken into account. In the context of Naive Bayes and Bayesian inference, it refers to the probability of a class given a set of features or observations.\\[P(C \\mid x) = \\frac{P(C) \\prod_{i=1}^{d} P(x_i \\mid C)}{P(x)}\\]  Where $d$ is the number of features.  Naive Bayes TheoremNaive Bayes is a classification technique based on Bayes‚Äô theorem, assuming that the features are conditionally independent given the class label.  Assumptions      Feature Independence: Features are independent of each other given the class label.    Class Prior Probability: The prior probability of each class is known.    Feature Likelihoods: The likelihood of each feature given the class is modeled using specific distributions (e.g., Gaussian, multinomial, Bernoulli).  Types of Naive Bayes ClassifiersGaussian Naive Bayes:  Usage: Best for continuous data where features follow a normal distribution.\\[\\text{Formula} \\rightarrow P(x_i \\mid C) = \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}} \\exp \\left( -\\frac{(x_i - \\mu_i)^2}{2 \\sigma_i^2} \\right)\\]  + Simple and efficient with continuous data; works well with normally distributed data.  - Assumes features are normally distributed, which may not always be true.import numpy as npclass GaussianNaiveBayes:    \"\"\"    Gaussian Naive Bayes classifier for continuous data.    \"\"\"    def fit(self, X, y):        self.n_features = X.shape[1]        self.classes = np.unique(y)        self.n_classes = len(self.classes)        self.mean = np.zeros((self.n_classes, self.n_features))        self.var = np.zeros((self.n_classes, self.n_features))        self.priors = np.zeros(self.n_classes)                for i, c in enumerate(self.classes):            X_c = X[y == c]            self.mean[i, :] = X_c.mean(axis=0)            self.var[i, :] = X_c.var(axis=0)            self.priors[i] = X_c.shape[0] / X.shape[0]    def _gaussian_density(self, class_idx, x):        mean = self.mean[class_idx]        var = self.var[class_idx]        numerator = np.exp(- (x - mean) ** 2 / (2 * var))        denominator = np.sqrt(2 * np.pi * var)        return numerator / denominator    def predict(self, X):        n_samples = X.shape[0]        posteriors = np.zeros((n_samples, self.n_classes))                for i, c in enumerate(self.classes):            prior = np.log(self.priors[i])            likelihood = np.sum(np.log(self._gaussian_density(i, X)), axis=1)            posteriors[:, i] = prior + likelihood                    return self.classes[np.argmax(posteriors, axis=1)]Multinomial Naive Bayes:  Usage: Suitable for discrete data, such as word counts in text classification.\\[\\text{Formula} \\rightarrow P(x_j \\mid C) = \\frac{N_{Cj} + \\alpha}{N_C + \\alpha \\cdot |V|}\\]  + Effective for text classification with large vocabularies; handles feature count data well.  - Assumes that feature counts are conditionally independent.import numpy as npclass MultinomialNaiveBayes:    \"\"\"    Multinomial Naive Bayes classifier for discrete data.    \"\"\"    def fit(self, X, y, alpha=1.0):        self.n_classes = len(np.unique(y))        self.alpha = alpha        self.classes, self.class_counts = np.unique(y, return_counts=True)        self.class_priors = self.class_counts / y.size        self.feature_count = np.zeros((self.n_classes, X.shape[1]))                for i, c in enumerate(self.classes):            self.feature_count[i, :] = np.sum(X[y == c], axis=0) + alpha        self.feature_totals = np.sum(self.feature_count, axis=1)            def predict(self, X):        log_priors = np.log(self.class_priors)        log_likelihoods = np.log(self.feature_count / self.feature_totals[:, None])        log_likelihoods = np.sum(X * log_likelihoods, axis=1)        log_posterior = log_priors + log_likelihoods                return self.classes[np.argmax(log_posterior, axis=1)]Bernoulli Naive Bayes:  Usage: Ideal for binary/boolean features.\\[\\text{Formula} \\rightarrow P(x_j \\mid C) = \\frac{N_{Cj} + \\alpha}{N_C + \\alpha \\cdot 2}\\]  + Suitable for binary/boolean data; handles presence/absence features well.  - Assumes features are binary, which may not always be the case.import numpy as npclass BernoulliNaiveBayes:    \"\"\"    Bernoulli Naive Bayes classifier for binary/boolean data.    \"\"\"    def fit(self, X, y, alpha=1.0):        self.n_classes = len(np.unique(y))        self.alpha = alpha        self.classes, self.class_counts = np.unique(y, return_counts=True)        self.class_priors = self.class_counts / y.size        self.feature_probs = np.zeros((self.n_classes, X.shape[1]))                for i, c in enumerate(self.classes):            X_c = X[y == c]            self.feature_probs[i, :] = (np.sum(X_c, axis=0) + alpha) / (X_c.shape[0] + 2 * alpha)            def predict(self, X):        log_priors = np.log(self.class_priors)        log_likelihoods = np.log(self.feature_probs) * X + np.log(1 - self.feature_probs) * (1 - X)        log_posterior = log_priors + np.sum(log_likelihoods, axis=1)                return self.classes[np.argmax(log_posterior, axis=1)]Complement Naive Bayes:  Usage: Designed to handle imbalanced datasets better.\\[\\text{Formula} \\rightarrow P(x_j \\mid C) = \\frac{N_{C'j} + \\alpha}{N_{C'} + \\alpha \\cdot |V|}\\]  + Addresses class imbalance by using feature probabilities from the complement of each class.  - More complex to implement; may not perform well on balanced datasets.import numpy as npclass ComplementNaiveBayes:    \"\"\"    Complement Naive Bayes classifier for imbalanced data.    \"\"\"    def fit(self, X, y, alpha=1.0):        self.n_classes = len(np.unique(y))        self.alpha = alpha        self.classes, self.class_counts = np.unique(y, return_counts=True)        self.class_priors = self.class_counts / y.size        self.feature_count = np.zeros((self.n_classes, X.shape[1]))                for i, c in enumerate(self.classes):            X_c = X[y == c]            self.feature_count[i, :] = np.sum(X_c, axis=0) + alpha        self.feature_totals = np.sum(self.feature_count, axis=1)        self.complement_feature_count = np.sum(X, axis=0) + alpha        self.complement_feature_totals = np.sum(self.complement_feature_count)            def predict(self, X):        log_priors = np.log(self.class_priors)        log_likelihoods = np.log(self.feature_count / self.feature_totals[:, None])        log_complement = np.log(self.complement_feature_count / self.complement_feature_totals)        log_complement = np.log(1 - log_complement)        log_posterior = log_priors + np.sum(X * log_likelihoods - (1 - X) * log_complement, axis=1)                return self.classes[np.argmax(log_posterior, axis=1)]Advantages and Disadvantages of Naive BayesAdvantages:  Simplicity: Easy to understand and implement.  Efficiency: Fast training and prediction.  Scalability: Handles large datasets efficiently.  Performance: Often performs well even with the naive independence assumption.Disadvantages:  Conditional Independence Assumption: The assumption that features are independent given the class label may not hold true in practice.  Feature Dependence: Correlated features can lead to poor performance.  Gaussian Assumption: Gaussian Naive Bayes assumes normally distributed features, which may not always be valid.ConclusionNaive Bayes classifiers offer a powerful yet simple approach to classification problems. By leveraging Bayes‚Äô theorem and assuming feature independence, these classifiers can efficiently handle a variety of data types and scales. While the conditional independence assumption can be a limitation, the classifiers‚Äô efficiency and effectiveness often outweigh this drawback. From text classification to medical diagnosis, Naive Bayes provides a versatile tool for many real-world applications. By understanding the underlying principles and implementing these classifiers from scratch, you can better appreciate their strengths and tailor them to your specific needs.Feel free to experiment with different Naive Bayes variants and see how they perform on your datasets. The simplicity and efficiency of Naive Bayes make it a valuable addition to any data scientist‚Äôs toolkit."
  },
  
  {
    "title": "Your Friendly Neighborhood Algorithm is KNN",
    "url": "/posts/k-nearest-neighbours-from-scratch/",
    "categories": "",
    "tags": "algorithms, machine-learning, supervised-learning, from-scratch",
    "date": "2023-04-06 00:00:00 +0300",
    





    
    "snippet": "IntroductionI am sure many of us have heard the following expression in our daily life; Tell me about your friend and I will tell you who you are. Likewise, the k-nearest neighbor algorithm uses ot...",
    "content": "IntroductionI am sure many of us have heard the following expression in our daily life; Tell me about your friend and I will tell you who you are. Likewise, the k-nearest neighbor algorithm uses other points around it to define data points.k-Nearest Neighbours (KNN) algorithm is one of the supervised learning algorithms. It works with labeled data just like any other supervised algorithm. It establishes a relationship between labeled data and unlabeled data through their neighborhood and distance to them. The KNN algorithm can be diversified in three different ways:Number of Neighborhoods (k)¬†: We can specify how many of the surrounding data (points) we will associate with. (k = 2,3,5 etc.)Distance Calculation Method: With which distance method can we examine the relationship with the surrounding data (Euclidean, Manhattan, Minkowski, Chebyshev etc.)Equal Relationship Instant Decision Method: What method will the relationship be terminated when the relationship rate with the surrounding points is equal? ‚Äã‚Äã(randomness or select first etc.) Image 1. KNN Algorithm Demo for a Single White Dot.Image by  DatacampAlgorithm Steps  A demo of the KNN Algorithm for a single point is performed on Figure 1. Before we move on to this demo, it would be helpful to know algorithm:      Step 1‚Ää‚Äî‚ÄäWhile creating the algorithm, definitions are made with which method the algorithm will calculate the distances between the points and how many nearby points it will look at.    Step 2‚Ää‚Äî‚ÄäLabeled data and unlabeled data to be predicted are loaded into or trained by the algorithm, which we call the fit step.    Step 3‚Ää‚Äî‚ÄäThe distances between each point in the data to be estimated and the labeled data are calculated.    Step 4‚Ää‚Äî‚ÄäAccording to the calculations, the distances are sorted from smallest to largest, that is, from near to far.    Step 5‚Ää‚Äî‚ÄäIn this order, the number of neighbors we use while defining the algorithm is looked at as well as its neighbors, that is, the points near it. Unlabeled data is labeled according to the label of the number of points that have a majority among these points. If there is equality, a random selection can be made.  Let‚Äôs assume that the distance calculation method in the first step is defined in the demo in Figure 1. Besides at this point, I would like to share with you a nice visual and article about the distance calculation methods that can be used. Image 2. Distance Methods. Image by  Maarten GrootendorstOkay let‚Äôs continue to Figure 1¬†, in the continuation of first step, we need to determine the number of neighbors. As seen in the demo, our estimation changes as the number of neighbors changes. Even at a time when it has equal neighbors, the algorithm enters a tie. At this point, we can have our algorithm randomly choose or choose the first label it sees.Also, in the example in the 3rd figure below, we want to predict whether a new point will be red or blue in a knn algorithm that will look at 3 neighborhoods. Since this black dot is close to 1 blue and 2 red dots, it is labeled as red because the majority are red dots. Image 3. KNN Classification for K=3 | 1 blue &lt; 2 red | new point is red. Image by¬†Gfycat &lt;/a&gt;Advantages and DisadvantagesThe KNN Algorithm is advantageous in terms of being quickly set up, implemented and explained. However, it is a costly algorithm in terms of time and space. Calculating the distance of a point from all other points in the dataset is a costly task. Curse of dimensionality can occur, especially when working with high-dimensional data. For this reason, many people use it with small data sets, but it is not recommended to be used with large data sets.Let‚Äôs code this algorithm from scratch with the iris dataset that most of us know.Step by Step KNN CodeStep 1‚Ää-‚ÄäInitalize Neighbour count and distance method.class KNearestNeighbours:  def __init__(self,k=5,distance_method='euclidean'):      self.neighbour_count = k      self.distance_method = distance_method        def set_neighbour_count(self,k:int) -&gt; None:      self.neighbour_count = k    def get_neighbour_count(self)-&gt;int:      return self.neighbour_count    def set_distance_method(self,distance_method:str) -&gt; None:      self.distance_method = distance_method    def get_distance_method(self)-&gt;str:      return self.distance_methodStep 2‚Ää-‚ÄäFit the Algorithm with Training Datadef fit(self,x:np.array,y:np.array):  self.X_train = x  self.y_train = yStep 3‚Ää-‚ÄäCalculate Distance Between Pointsfrom scipy.spatial import distancedef __calculate_distance(self,u:np.array,v:np.array) -&gt; float:  methods = {      'euclidean':distance.euclidean(u,v),      'minkowski':distance.minkowski(u,v),      'manhattan':distance.cityblock(u,v),      'chebyshev':distance.chebyshev(u,v),      'jaccard':distance.jaccard(u,v),      'cosine': distance.cosine(u,v),      }  return methods[self.distance_method]Step 4 &amp; Step 5- Sort Distances and Select k Nearest Pointsfrom collections import Counterdef __predict(self,x_pred:np.array):  # step 3  distances = [self.__calculate_distance(x_pred,x_real) for x_real in self.X_train]    # step 4   sorted_distances_as_idx = np.argsort(distances)  knn_indices = sorted_distances_as_idx[:self.neighbour_count]   predicted_values = self.y_train[knn_indices].squeeze().tolist()     # step 5   most_common_values = Counter(predicted_values).most_common()   prediction = most_common_values[0][0]   return prediction    #Function that is below is apply step 4 &amp; step 5 for all test data points.  def predict(self,X_test:np.array) -&gt; list:    if X_test.ndim == 1:        X_test = np.expand_dims(X_test,axis=0)        predictions = [self.__predict(x_pred) for x_pred in X_test]    return predictionsTest on Iris DatasetLet‚Äôs test this module with iris dataset using scikit-learn. The iris dataset can be easily loaded from scikit-learn. It basically consists of 5 columns, 4 feature and 1 target column. Let‚Äôs convert the dataset to pd.DataFrame using scikit-learn, NumPy and Pandas.from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitimport numpy as np import pandas as pd dataset = load_iris()data = np.concatenate((dataset.data,dataset.target.reshape(-1,1)),axis=1)columns = dataset.feature_names + ['class']# cols:'sepal length (cm), sepal width (cm), petal length (cm), petal width (cm), class'df = pd.DataFrame(data=data,columns=columns)class_map = dict(enumerate(dataset.target_names)) # class_map = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}df['class_name'] = df['class'].apply(lambda v:class_map[int(v)])df = df.sample(frac = 1) # shuffle dataset df = df.reset_index(drop=True) # reset index on shuffled data # Train Test SplitX,y = df.iloc[:,:-2],df.iloc[:,-2]X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,test_size=0.1)First 5th Rows of the Dataframe [ df.head( )¬†]Let‚Äôs create and fit our k-Nearest Neighbours model. Then, predict test data with the model. Finally, let‚Äôs calculate the accuracy.knn = KNearestNeighbours(k=3,distance_method='euclidean')knn.fit(x=X_train.values,y=y_train.values)predictions = knn.predict(X_test=X_test.values)acc = np.sum(predictions == y_test.values) / len(y_test)Results of Our Training &amp; PredictionFull Codefrom scipy.spatial import distancefrom collections import Counterfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitimport numpy as np import pandas as pd class KNearestNeighbours:    def __init__(self,k=5,distance_method='euclidean'):        self.neighbour_count = k        self.distance_method = distance_method            def set_neighbour_count(self,k:int) -&gt; None:        self.neighbour_count = k        def get_neighbour_count(self)-&gt;int:        return self.neighbour_count        def set_distance_method(self,distance_method:str) -&gt; None:        self.distance_method = distance_method        def get_distance_method(self)-&gt;str:        return self.distance_method        def fit(self,x:np.array,y:np.array):        self.X_train = x        self.y_train = y            def __calculate_distance(self,u:np.array,v:np.array) -&gt; float:        methods = {            'euclidean':distance.euclidean(u,v),            'minkowski':distance.minkowski(u,v),            'manhattan':distance.cityblock(u,v),            'chebyshev':distance.chebyshev(u,v),            'jaccard':distance.jaccard(u,v),            'cosine': distance.cosine(u,v),            }        return methods[self.distance_method]        def __predict(self,x_pred:np.array):        distances = [self.__calculate_distance(x_pred,x_real) for x_real in self.X_train]        sorted_distances_as_idx = np.argsort(distances)        knn_indices = sorted_distances_as_idx[:self.neighbour_count]        predicted_values = self.y_train[knn_indices].squeeze().tolist()        most_common_values = Counter(predicted_values).most_common()        prediction = most_common_values[0][0]        return prediction         def predict(self,X_test:np.array) -&gt; list:        if X_test.ndim == 1:            X_test = np.expand_dims(X_test,axis=0)                predictions = [self.__predict(x_pred) for x_pred in X_test]        return predictionsdataset = load_iris()data = np.concatenate((dataset.data,dataset.target.reshape(-1,1)),axis=1)columns = dataset.feature_names + ['class']df = pd.DataFrame(data=data,columns=columns)class_map = dict(enumerate(dataset.target_names))df['class_name'] = df['class'].apply(lambda v:class_map[int(v)])df = df.sample(frac = 1)df = df.reset_index(drop=True)# Train Test SplitX,y = df.iloc[:,:-2],df.iloc[:,-2]X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,test_size=0.1)knn = KNearestNeighbours(k=3,distance_method='euclidean')knn.fit(x=X_train.values,y=y_train.values)predictions = knn.predict(X_test=X_test.values)acc = np.sum(predictions == y_test.values) / len(y_test)Conclusion The k-Nearest Neighbors (KNN) algorithm is like the friendly neighbor who knows everyone in the neighborhood. By examining the closest \"neighbors\" to a data point, KNN makes predictions based on the characteristics of these nearby data points. This approach offers several advantages, such as simplicity and ease of implementation. However, it‚Äôs essential to be aware of its limitations, including sensitivity to noisy data and the computational cost associated with large datasets. Despite these challenges, KNN remains a valuable tool when used with care."
  },
  
  {
    "title": "When Simplicity Meets Power | Linear Models",
    "url": "/posts/linear-models/",
    "categories": "",
    "tags": "algorithms, machine-learning, supervised-learning, from-scratch",
    "date": "2023-03-10 00:00:00 +0300",
    





    
    "snippet": "Getting Started with Linear Models: When Simplicity Meets PowerIf you‚Äôre dipping your toes into machine learning, you‚Äôve likely come across linear models. They‚Äôre one of the simplest and most intui...",
    "content": "Getting Started with Linear Models: When Simplicity Meets PowerIf you‚Äôre dipping your toes into machine learning, you‚Äôve likely come across linear models. They‚Äôre one of the simplest and most intuitive tools in the data scientist‚Äôs toolkit, but don‚Äôt let their simplicity fool you‚Äîthey can pack a powerful punch when used correctly. In this post, we‚Äôll explore what linear models are, why they‚Äôre so popular, and when you might want to reach for them in your own projects.What Are Linear Models?At their core, linear models are all about making predictions by drawing straight lines. Imagine you‚Äôre trying to predict someone‚Äôs height based on their shoe size. A linear model would help you draw a straight line through your data points‚Äîlike connecting the dots‚Äîand use that line to predict heights for new shoe sizes. It‚Äôs as simple as it sounds.In more scientific terms, linear models are mathematical models used to predict a dependent variable (the target) based on one or more independent variables (features). These models estimate the target variable as a linear combination of the independent variables. Mathematically, a linear model can be expressed as:\\[\\hat{y} = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b\\]      \\(\\hat{y}\\) is the predicted value (dependent variable),    \\(x_1,x_2,\\ldots,x_n\\)‚Äã are the independent variables (features),    \\(w_1,w_2,\\ldots,w_n\\) ‚Äã are the coefficients (weights) associated with the features,    \\(b\\) is the bias (intercept) term.  But don‚Äôt worry about the math too much. The key idea is that the model tries to predict the target value \\(y\\) (like someone‚Äôs height) by combining the input features \\(x_1,x_2, \\ldots,x_n\\)‚Äã (like shoe size and other measurements) in a straight-line fashion.Types of Linear ModelsLinear models come in a few different flavors, depending on how many inputs you have and how you want to handle them. Here are the most common types:1. Simple Linear RegressionThis is the bread and butter of linear models. You‚Äôve got one input and one output, and you‚Äôre just trying to find the straight line that best fits your data. Think of it like trying to predict a person‚Äôs height based on just their shoe size.2. Multi Linear RegressionSometimes, one input isn‚Äôt enough. Maybe you want to predict house prices based on square footage, number of bedrooms, and location. That‚Äôs where multi linear regression comes in‚Äîit lets you handle several inputs at once, drawing a straight line through multi-dimensional space (don‚Äôt worry, the math handles that for you).\\[\\text{Sum of Squared Errors}(y, \\hat{y}) = {\\sum_{i=0}^{N - 1} (y_i - \\hat{y}_i)^2}\\]3. Lasso and Ridge RegressionThese are fancier versions of multiple linear regression that include something called regularization. Without getting too deep into the weeds, regularization helps prevent your model from getting too fancy with the data, which can actually make it less accurate on new data. Lasso and Ridge are your go-to tools when you want to avoid overfitting.Lasso regression (Least Absolute Shrinkage and Selection Operator) adds an L1 penalty term to the loss function, which can lead to sparse solutions where some coefficients are exactly zero. L1 pentaly term is equal to the absolute value of the coefficients.\\[\\text{Cost Function} = \\text{Sum of Squared Errors} + \\lambda \\sum_{i=1}^{n} \\left | w_i \\right |\\]Key Points:  Feature Selection: Lasso can shrink some coefficients to zero, effectively selecting a subset of features.  Interpretability: Useful when you have many features, as it helps in identifying the most important ones.Ridge regression adds an L2 penalty term to the loss function to prevent overfitting by shrinking coefficients. L2 penalty term is  equal to the square of the coefficients.\\[\\text{Cost Function} = \\text{Sum of Squared Errors} + \\lambda \\sum_{i=1}^{n} w_i^2\\]Key Points:  Shrinking Coefficients: It shrinks coefficients but doesn‚Äôt necessarily set them to zero, keeping all features in the model.  Useful for Multicollinearity: Helps when features are highly correlated, stabilizing the coefficient estimates.Building a Linear Model from ScratchNow, let‚Äôs roll up our sleeves and build a simple linear model from scratch in Python. Don‚Äôt worry‚Äîwe‚Äôll take it step by step.import numpy as npclass LinearRegressionCustom:    def __init__(self, alpha=0.0, iterations=1000, learning_rate=0.01, lasso=False, ridge=False):        self.alpha = alpha  # Regularization strength        self.iterations = iterations  # Number of iterations for gradient descent        self.learning_rate = learning_rate  # Learning rate        self.lasso = lasso  # Flag for Lasso regularization        self.ridge = ridge  # Flag for Ridge regularization    def fit(self, X, y):        m, n = X.shape        self.weights = np.zeros(n)  # Initialize weights (excluding bias)        self.bias = 0  # Initialize bias term        for _ in range(self.iterations):            # Make predictions            predictions = X.dot(self.weights) + self.bias            errors = predictions - y            # Compute gradients            dw = (2/m) * X.T.dot(errors)  # Derivative w.r.t weights            db = (2/m) * np.sum(errors)  # Derivative w.r.t bias            # Add regularization terms            if self.lasso:                dw += self.alpha * np.sign(self.weights)  # Lasso regularization            elif self.ridge:                dw += 2 * self.alpha * self.weights  # Ridge regularization            # Update parameters            self.weights -= self.learning_rate * dw            self.bias -= self.learning_rate * db    def predict(self, X):        return X.dot(self.weights) + self.biasThe Upsides of Linear Models (Advantages)So, why do people love linear models so much? Here are a few reasons:  Simplicity: Linear models are straightforward and easy to understand, making them a great starting point for anyone new to machine learning.  Speed: They‚Äôre fast to train, which means you can quickly get results, especially with smaller datasets.  Interpretability: It‚Äôs easy to explain the results of a linear model, which is a huge plus when you need to communicate your findings to non-technical stakeholders.  Scalability: Linear models can handle very large datasets, especially when using techniques like stochastic gradient descent (SGD).The Downsides of Linear Models (Disadvantages)However, linear models aren‚Äôt perfect. Here‚Äôs where they might fall short:  Limited Flexibility: They assume a straight-line relationship between the inputs and the output, which isn‚Äôt always the case in real-world data.  Outlier Sensitivity: Linear models can be thrown off by outliers‚Äîthose extreme values that don‚Äôt fit the pattern.  Overfitting with Too Many Features: While regularization helps, a model with too many features can still overfit the training data.When to Use Linear ModelsLinear models are best used when:  You Have a Linear Relationship: If you suspect that the relationship between your features and target is linear, a linear model is a good choice.  You Need a Fast, Interpretable Model: When you need quick results and a model that‚Äôs easy to explain, linear models are ideal.  You‚Äôre Dealing with a Large Dataset: They scale well with large datasets, making them suitable for big data applications.Let‚Äôs Play with Regression ModelsSimple Linear Regression Test ResultsSimple Linear Regression Test Resultsfrom sklearn.linear_model import LinearRegression as SklearnLRdef test_simple_linear_regression(X_train,X_test,y_train,y_test,save_file=False):    simple_reg = LinearRegressionCustom()    simple_reg.fit(X_train[:, [0]], y_train)      y_pred_custom_train = simple_reg.predict(X_train[:, [0]])    y_pred_custom_test = simple_reg.predict(X_test[:, [0]])    sklearn_simple_reg = SklearnLR()    sklearn_simple_reg.fit(X_train[:, [0]], y_train)        y_pred_train_sklearn = sklearn_simple_reg.predict(X_train[:, [0]])    y_pred_test_sklearn = sklearn_simple_reg.predict(X_test[:, [0]])        evaluate_plot_results(X_train,y_train,y_pred_custom_train,y_pred_custom_test,X_test,y_test,y_pred_train_sklearn,y_pred_test_sklearn,title='Simple Linear Regression',save_file=save_file)Multi-Linear Regression Test ResultsMulti Linear Regression Test Resultsfrom sklearn.linear_model import LinearRegression as SklearnLRdef test_multi_linear_regression(X_train,X_test,y_train,y_test,save_file=False):    multi_reg = LinearRegressionCustom()    multi_reg.fit(X_train, y_train)    y_pred_custom_train = multi_reg.predict(X_train)    y_pred_custom_test = multi_reg.predict(X_test)        sklearn_multi_reg = SklearnLR()    sklearn_multi_reg.fit(X_train, y_train)    y_pred_train_sklearn = sklearn_multi_reg.predict(X_train)    y_pred_test_sklearn = sklearn_multi_reg.predict(X_test)    evaluate_plot_results(X_train,y_train,y_pred_custom_train,y_pred_custom_test,X_test,y_test,y_pred_train_sklearn,y_pred_test_sklearn,title='Multi Linear Regression',save_file=save_file)Lasso Regression Test ResultsLasso Linear Regression Test Resultsfrom sklearn.linear_model import Lassodef test_lasso_linear_regression(X_train,X_test,y_train,y_test,save_file=False):    lasso_reg = LinearRegressionCustom(alpha=0.1, lasso=True)    lasso_reg.fit(X_train, y_train)    # Predictions (Custom Implementation)    y_pred_custom_train = lasso_reg.predict(X_train)    y_pred_custom_test = lasso_reg.predict(X_test)        # Fit models (Scikit-Learn)    sklearn_lasso_reg = Lasso(alpha=0.1)    sklearn_lasso_reg.fit(X_train, y_train)        y_pred_train_sklearn = sklearn_lasso_reg.predict(X_train)    y_pred_test_sklearn = sklearn_lasso_reg.predict(X_test)    evaluate_plot_results(X_train,y_train,y_pred_custom_train,y_pred_custom_test,X_test,y_test,y_pred_train_sklearn,y_pred_test_sklearn,title='Lasso Linear Regression',save_file=save_file)Ridge Regression Test ResultsRidge Linear Regression Test Resultsfrom sklearn.linear_model import Ridgedef test_ridge_linear_regression(X_train,X_test,y_train,y_test,save_file=False):    ridge_reg = LinearRegressionCustom(alpha=0.1, ridge=True)    ridge_reg.fit(X_train, y_train)            y_pred_custom_train = ridge_reg.predict(X_train)    y_pred_custom_test = ridge_reg.predict(X_test)    sklearn_ridge_reg = Ridge(alpha=0.1)    sklearn_ridge_reg.fit(X_train, y_train)    y_pred_train_sklearn = sklearn_ridge_reg.predict(X_train)    y_pred_test_sklearn = sklearn_ridge_reg.predict(X_test)    evaluate_plot_results(X_train,y_train,y_pred_custom_train,y_pred_custom_test,X_test,y_test,y_pred_train_sklearn,y_pred_test_sklearn,title='Ridge Linear Regression',save_file=save_file)Dataset &amp; Evaluation Codesfrom sklearn.datasets import make_regressionfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_errorimport matplotlib.pyplot as plt from matplotlib.gridspec import GridSpecdef create_dataset() -&gt; tuple:    np.random.seed(0)    X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=0)    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)    return X_train,X_test,y_train,y_testdef evaluate_plot_results(x_train,y_train, y_pred_train_custom, y_pred_test_custom, x_test,y_test, y_pred_train_sklearn, y_pred_test_sklearn,title,feature_idx=0,save_file=False):    # Default check for first feature on features, because feature index parameter is 0.         # Calculate MSE    mse_custom_train = mean_squared_error(y_train, y_pred_train_custom)    mse_sklearn_train = mean_squared_error(y_train, y_pred_train_sklearn)    mse_custom_test = mean_squared_error(y_test, y_pred_test_custom)    mse_sklearn_test = mean_squared_error(y_test, y_pred_test_sklearn)        fig = plt.figure(figsize=(30, 20))    gs = GridSpec(2, 3, figure=fig)    ax1 = fig.add_subplot(gs[0, 0])        x_train_line = np.linspace(x_train[:,feature_idx].min(), x_train[:,feature_idx].max(), len(y_train)).reshape(-1, 1)        # Scatter plot for train predictions    ax1.scatter(x_train_line, y_train, color='blue', label='Real Values')    ax1.plot(x_train_line,y_pred_train_custom, color='red', label=title + 'Custom Model',linestyle='--')    ax1.plot(x_train_line,y_pred_train_sklearn, color='green', label=title + 'Scikit-Learn Model',linestyle=':')    ax1.set_title(title + ' on Train Dataset',fontweight='bold')    ax1.set_xlabel(\"True Values\",fontweight='bold')    ax1.set_ylabel(\"Predictions\",fontweight='bold')    ax1.legend()    # Scatter plot for test predictions    x_test_line = np.linspace(x_test[:,feature_idx].min(), x_test[:,feature_idx].max(),  len(y_test)).reshape(-1, 1)        ax2 = fig.add_subplot(gs[0, 2])    ax2.scatter(x_test_line, y_test, color='blue', label='Real Values')    ax2.plot(x_test_line,y_pred_test_custom, color='red', label=title + 'Custom Model',linestyle='--')    ax2.plot(x_test_line,y_pred_test_sklearn, color='green', label=title + 'Scikit-Learn Model',linestyle=':')    ax2.set_title(title + ' on Test Dataset',fontweight='bold')    ax2.set_xlabel(\"True Values\",fontweight='bold')    ax2.set_ylabel(\"Predictions\",fontweight='bold')    ax2.legend()    # MSE comparison bar plot    ax3 = fig.add_subplot(gs[0, 1])    x = np.arange(2)  # position of the bars    width = 0.35  # width of bars    bars1 = ax3.bar(x - width/2, [mse_custom_train, mse_custom_test], width, label='Custom Model MSE', color='mediumseagreen')    bars2 = ax3.bar(x + width/2, [mse_sklearn_train, mse_sklearn_test], width, label='Scikit-Learn Model MSE', color='xkcd:sky blue')    # Add text for the bar heights    for bar in bars1:        height = bar.get_height()        ax3.text(bar.get_x() + bar.get_width()/2.0, height, f'{height:.2f}', ha='center', va='bottom')    for bar in bars2:        height = bar.get_height()        ax3.text(bar.get_x() + bar.get_width()/2.0, height, f'{height:.2f}', ha='center', va='bottom')    ax3.set_xlabel('Dataset',fontweight='bold')    ax3.set_ylabel('Mean Squared Error',fontweight='bold')    ax3.set_title('MSE Comparison',fontweight='bold')    ax3.set_xticks(x)    ax3.set_xticklabels(['Train', 'Test'])    ax3.legend()        gs.update(hspace=.3, wspace=.3)    if save_file:plt.savefig(title+'.png',dpi=300, bbox_inches='tight')    else:plt.show()  ClassificationFor binary classification, for example, we can set up a linear model as follows:\\[\\hat{y} = w_0 x_0 + w_1 x_1 + \\ldots + w_n x_n + b &gt; 0\\]As seen, this formula closely resembles linear regression. However, it includes a threshold value. If the computed value is greater than 0, the model returns +1; otherwise, it returns -1. Linear models can be classified into different types based on the regularization algorithms and measurement metrics they use. The same applies to classification tasks. Two well-known methods are Logistic Regression and Support Vector Machines (SVMs).Logistic RegressionDespite the name, logistic regression should not be confused with regression methods. Logistic regression is a classification algorithm.  Problems have multiple features, one feature may be selected and the others separated for solving the problem.Conclusion \"If you‚Äôre tackling a new problem and need a quick and effective tool, linear models are worth considering. They might not be the fanciest option, but they often provide a solid foundation for more complex models.\""
  },
  
  {
    "title": "A Modern Guide | Linearity and Non-Linearity in Machine Learning",
    "url": "/posts/linearity/",
    "categories": "",
    "tags": "basics, data-science, linear-algebra, from-scratch, tutorials",
    "date": "2023-03-09 00:00:00 +0300",
    





    
    "snippet": "Linearity and Non-Linearity in Machine Learning: A Modern GuideIn machine learning, understanding the concepts of linearity and non-linearity is like learning the alphabet before reading a book. Th...",
    "content": "Linearity and Non-Linearity in Machine Learning: A Modern GuideIn machine learning, understanding the concepts of linearity and non-linearity is like learning the alphabet before reading a book. These principles shape the way we build algorithms and model data. But what do these terms really mean, and why do they matter?Let‚Äôs dive into the world of linearity and non-linearity with a focus on real-life examples, visualizations, and applications in machine learning.What is Linearity?At its core, linearity refers to a relationship between input and output that can be expressed as a straight line. In other words, changes in the input lead to proportional changes in the output.Imagine you‚Äôre driving at a constant speed of 60 km/h. The relationship between your speed and the distance you travel is linear. For every additional hour you drive, you‚Äôll travel 60 km more. This can be expressed in a simple mathematical formula:\\[Distance = Speed √ó Time = 60 √ó t\\]Real Life Example: Predicting IncomeA common example of linearity is predicting income based on years of experience. Let‚Äôs assume that for every year of experience, income increases by a fixed amount:\\[Income = w ‚ãÖ Experience + Œ≤\\]  Where:      \\(w\\) is the slope, representing how much income increases per year of experience.    \\(Œ≤\\) is the intercept, representing the starting income with zero experience.  This simple relationship can often approximate real-world data surprisingly well.Figure 1What is Non-Linearity ?Non-linearity, on the other hand, represents relationships that cannot be expressed by a straight line. In these cases, changes in the input lead to disproportionate changes in the output.Imagine you‚Äôre driving, but this time you‚Äôre navigating hills, valleys, and sharp turns. Your speed will vary depending on whether you‚Äôre going uphill, downhill, or turning sharply. The relationship between your speed and time won‚Äôt be a straight line anymore, but rather a curve. Here‚Äôs a formula that might capture that:\\[Speed = w_1 ‚ãÖ Slope + w_2 ‚ãÖ Turn Sharpness + Œ≤\\]This non-linear relationship reflects the fact that the speed isn‚Äôt constant‚Äîit depends on complex factors like the slope of the road and how sharp the turns are.Real-Life Example: House PricesIn real estate, house prices often exhibit non-linear relationships with features like size, location, and age. For instance, doubling the size of a house doesn‚Äôt necessarily double its price. Instead, the price might increase exponentially as the house gets larger or is located in a prime area.Figure 2The graph on the left shows the relationship between the dependencies of the speed variable on slope and sharpness values. As can be seen, there is no linear increase or decrease, that is, there is a non-linear situation. Although the data was not prepared very well because the data was generated randomly, I think I was able to explain the subject. In the graph on the right, if we consider the problem of house price estimation, which may have a non-linear relationship in real life, it can be seen that there is a non-linear but increasing relationship between the size of the house and its price.If we assume that speed has linear relations with other values. Then you will see a 3D Plot like that:Figure 3More real life example:Figure 4Model calculations were made and visualized by assuming that there was a linear or non-linear relationship between the two features on the above graphs. The dataset has been prepared intuitively by hand and will be shared with all plot codes at the end of the article. For the example on the left, a scenario between salary and years of experience is considered. In the example on the right, the scenario of house prices and sizes is taken as an example.Comparison of Linearity and Non-Linearity in Machine Learning            Aspect      Linear Models      Non-Linear Models                  Relationship Type      Straight line, proportional changes      Curved or complex, disproportionate changes              Complexity      Simple, easy to interpret      More complex, harder to interpret              Use Cases      Simple relationships (e.g., income vs. experience)      Complex relationships (e.g., image recognition)              Algorithms      Linear regression, logistic regression      Decision trees, neural networks, SVM              Computation      Fast and computationally efficient      Slower and more computationally expensive      ConclusionIn summary, understanding the difference between linear and non-linear relationships is fundamental to building effective machine learning models. While linear models are easy to interpret and work well for simpler relationships, non-linear models allow us to tackle more complex real-world problems where data relationships are not so straightforward.Both types of models have their place in machine learning, and knowing when to use each is key to creating models that can make accurate predictions. As we move forward into more advanced algorithms, these foundational concepts will serve as the building blocks for understanding more sophisticated methods like neural networks, random forests, and support vector machines.CodesFigure 1import numpy as npimport matplotlib.pyplot as plt# Plot linear relationshiptime = np.linspace(0, 10, 10)distance = 60 * timeplt.figure(figsize=(12,4))plt.subplot(1,2,1)plt.scatter(time, distance, color='black',marker='o')plt.plot(time, distance, color='tab:red', linestyle='-.',label='Linear :: y = 60 * t',alpha=.7)plt.xlabel('Time (hours)')plt.ylabel('Distance (km)')plt.title('Linearity Example: Constant Speed')plt.legend()plt.grid(True,alpha=.3,linestyle='--')plt.subplot(1,2,2)experience = np.linspace(0, 10, 10)bias = 2000 weight = 700 income = weight * experience + bias plt.scatter(experience, income, color='black',marker='o')plt.plot(experience, income, color='tab:blue', linestyle='-.',label='Income :: y = 700 * experience + 2000',alpha=.7)plt.xlabel('Experience (years)')plt.ylabel('Income ($)')plt.title('Real Life Example: Income and Experience')plt.legend()plt.grid(True,alpha=.3,linestyle='--')plt.show()Figure 2import numpy as npimport matplotlib.pyplot as pltnp.random.seed(1234)plt.figure(figsize=(15,5))plt.subplot(1,2,1)speed_bias = 20 slopes = np.random.randint(low=0,high=30,size=10)sharpness = np.random.randint(low=0,high=90,size=10)weight_of_slopes = np.random.uniform(low=-1,high=1,size=10)weight_of_sharpness =  np.random.uniform(low=-1,high=1,size=10)initial_speed = 60 speed = initial_speed + (weight_of_slopes * (slopes)) + (weight_of_sharpness * sharpness) + speed_bias plt.scatter(slopes, speed, color='tab:red', marker='x',label='slope')plt.scatter(sharpness, speed, color='tab:blue', marker='o',label='sharpness')plt.plot(slopes, speed, color='tab:red', linestyle='-.',label='slope &amp; speed')plt.plot(sharpness, speed, color='tab:blue', linestyle='-.',label='sharpness &amp; speed')plt.xlabel('Values')plt.ylabel('Speed')plt.title('Non-Linearity Example: Speed')plt.legend()plt.grid(True,alpha=.3,linestyle='--')plt.subplot(1,2,2)house_sizes = np.linspace(50, 400, 100)house_weights = 55house_bias = 100house_prices = house_weights * np.log(house_sizes) + house_biasplt.plot(house_sizes, house_prices, color='red', linestyle='-.',label='Non-Linear: y = 1000 * log(x) + 50000')plt.xlabel('Size (sq. meters)')plt.ylabel('Price ($)')plt.title('Non-Linearity in Real Estate: House Prices')plt.legend()plt.grid(True,alpha=.3,linestyle='--')plt.show()Figure 3# note: figure 3 codes is realted with figure 2 code plot 1 slopes_grid, sharpness_grid = np.meshgrid(np.linspace(slopes.min(), slopes.max(), len(slopes)),                               np.linspace(sharpness.min(), sharpness.max(), len(sharpness)))initial_speed = 60 speed_grid =  initial_speed + (weight_of_slopes * (slopes_grid)) + (weight_of_sharpness * sharpness_grid) + speed_bias # Create a 3D plotfig = plt.figure(figsize=(12,15))ax = fig.add_subplot(111, projection='3d')# Plot the data pointsax.scatter(slopes, sharpness, speed, color='red', marker='o',label='speed = w1 * slopes + w2 * sharpness + bias')# Plot the plane with color fillax.plot_surface(slopes_grid, sharpness_grid, speed_grid, color='blue', alpha=0.4,rstride=5,cstride=5)# Labels and titleax.set_xlabel('slopes',fontweight='bold')ax.set_ylabel('sharpness',fontweight='bold')ax.set_zlabel('speed',fontweight='bold')ax.set_title('Multiple Linear Regression for Speed - 3D Plot ')plt.legend()plt.show()Figure 4import pandas as pd from sklearn.linear_model import LinearRegressionfrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.pipeline import make_pipeline# Create datasethouse_data = {    'Size': [50, 100, 150, 200, 250, 300, 350, 400],    'Price': [10000, 20000, 26000, 30000, 41000, 59000, 90000, 172000]}df_house = pd.DataFrame(house_data)# Create a simple datasetsalary_data = {    'Years_of_Experience': [1, 2, 3, 4, 5,6,7,8,9,10],    'Salary': [2000,3000, 3500, 4000,6000, 7000,10000,10500,11500,12000]}df_salary = pd.DataFrame(salary_data)X_salary = df_salary[['Years_of_Experience']]y_salary = df_salary['Salary']salary_poly_model = make_pipeline(PolynomialFeatures(degree=5), LinearRegression())salary_poly_model.fit(X_salary, y_salary)salary_lr_model  =  LinearRegression()salary_lr_model.fit(X_salary, y_salary)y_salary_pred_nonlinear = salary_poly_model.predict(X_salary)y_salary_pred_linear = salary_lr_model.predict(X_salary)plt.figure(figsize=(12,4))plt.subplot(1,2,1)plt.scatter(X_salary, y_salary, color='tab:blue', label='Actual Data')plt.plot(X_salary, y_salary_pred_linear, linestyle='--',color='tab:green',alpha=.8, label='Linear Predicted Curve')plt.plot(X_salary, y_salary_pred_nonlinear, linestyle='--',color='tab:red',alpha=.8, label='Non-Linear Predicted Curve')plt.xlabel('Experience (years)')plt.ylabel('Salary ($)')plt.title('Salary vs. Experience')plt.legend()plt.grid(True,linestyle='--',alpha=.4)# Polynomial Regression (Non-linear)X_house = df_house[['Size']]y_house = df_house['Price']house_poly_model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())house_poly_model.fit(X_house, y_house)house_lr_model  =  LinearRegression()house_lr_model.fit(X_house, y_house)# Predict and ploty_house_pred_nonlinear = house_poly_model.predict(X_house)y_house_pred_linear = house_lr_model.predict(X_house)plt.subplot(1,2,2)plt.scatter(X_house, y_house, color='tab:blue', label='Actual Data')plt.plot(X_house, y_house_pred_linear, linestyle='--',color='tab:green',alpha=.8, label='Linear Predicted Curve')plt.plot(X_house, y_house_pred_nonlinear, linestyle='--',color='tab:red',alpha=.8, label='Non-Linear Predicted Curve')plt.xlabel('House Size (sq. meters)')plt.ylabel('Price ($)')plt.title('House Size vs. Price')plt.legend()plt.grid(True,linestyle='--',alpha=.4)plt.show()"
  },
  
  {
    "title": "Essential Python Libraries for Data Science and Machine Learning",
    "url": "/posts/datascience-101/",
    "categories": "",
    "tags": "python, data-science, basics, devops, setup",
    "date": "2022-12-30 00:00:00 +0300",
    





    
    "snippet": "1. Machine Learning with Scikit-learnWhen you‚Äôre starting out with machine learning, Scikit-learn is the perfect place to begin. It is the wild card of machine learning libraries, offering everythi...",
    "content": "1. Machine Learning with Scikit-learnWhen you‚Äôre starting out with machine learning, Scikit-learn is the perfect place to begin. It is the wild card of machine learning libraries, offering everything from basic classification and regression algorithms to more advanced clustering and dimensionality reduction techniques. Plus, it‚Äôs got all the tools you need for evaluating models, like cross-validation and grid search. In short, Scikit-learn makes it easy to dive into machine learning without a lot of extra hassle.2. Data Manipulation with Pandas, Polars, PySpark, and DaskEvery data science project starts with data manipulation‚Äîwhether you‚Äôre cleaning messy data, transforming it, or just making sense of it. Pandas is the go-to library for in-memory data wrangling, letting you slice, dice, and group your data into useful formats. But if you‚Äôre working with larger datasets or need something more performance-driven, Polars is a great alternative that offers faster processing. Meanwhile, PySpark and Dask help you scale up to big data, letting you work on distributed systems without breaking a sweat. Whatever the size of your data, there‚Äôs a tool here to help you manage it efficiently.3. Numerical Computations with NumPyFor anything math-related, NumPy is the foundational library. It‚Äôs like the backbone of Python‚Äôs entire data ecosystem, powering everything from array manipulation to advanced linear algebra. Whether you‚Äôre performing basic calculations or diving into more complex matrix operations, NumPy makes sure you‚Äôre working efficiently.4. Statistical Modeling with Statsmodels and SciPyNeed to dive deep into statistical analysis? This is where Statsmodels and SciPy step in. Statsmodels is a fantastic library for building statistical models, running hypothesis tests, or analyzing time series data. On the other hand, SciPy extends NumPy‚Äôs capabilities by providing tools for optimization, signal processing, and even integration. If you‚Äôre dealing with more than just simple stats, these libraries are key to unlocking deeper insights from your data.5. Data Visualization with Matplotlib, Seaborn, and PlotlyVisualization is one of the most important steps in any data project‚Äîit helps you understand patterns and tell compelling stories with your data. Matplotlib is like the workhorse for creating all types of plots, but if you want something a little prettier and easier to use, Seaborn builds on Matplotlib to produce beautiful, statistical-style plots right out of the box. And for those times when you need interactive visualizations, Plotly is a fantastic choice, especially if you‚Äôre building dashboards or want to engage viewers with more dynamic data presentations.6. Deep Learning with TensorFlow, Keras, and PyTorchIf you‚Äôre looking to get into deep learning, TensorFlow, Keras, and PyTorch are your go-to libraries. Keras offers a simple interface that makes it easy to build neural networks, while TensorFlow provides the power and flexibility for more complex tasks, especially in production environments. PyTorch is loved by researchers for its dynamic computation graph and easy debugging. Together, these libraries let you build anything from basic neural networks to cutting-edge deep learning models for things like image recognition, natural language processing, and even reinforcement learning.Whether you‚Äôre working on:  Image classification (using pre-trained models like ResNet or YOLO),  Speech recognition (with tools like SpeechRecognition and DeepSpeech),  Or even graph neural networks (using PyTorch Geometric or Deep Graph Library),these libraries give you the power to tackle the most challenging deep learning problems.7. Specialized Libraries for Niche TasksAs your projects become more complex, you‚Äôll find yourself needing more specialized libraries. For instance, if you‚Äôre working with text data, NLTK and spaCy are fantastic for natural language processing. OpenCV and Pillow (PIL) are essential for image processing and computer vision tasks, whether you‚Äôre resizing images, detecting objects, or working on facial recognition. And if you‚Äôre diving into speech analytics, SpeechRecognition helps you convert audio to text, making voice-controlled applications much easier to build.Installing These LibrariesInstalling these libraries is straightforward with Python‚Äôs package manager, pip. Here‚Äôs how you can install the core and specialized libraries you‚Äôll need for your projects:pip install numpy pandas scikit-learn matplotlib seaborn nltk spacy dask pyspark opencv-python pillow keras tensorflow torch torchvision torchaudio  Recommendation: If you will develop deep learning project(s), choose one tool from keras,tensorflow or pytorch and install different environments.Because sometimes there might be some version conflicts.  Installation: Some packages like pyspark, pytorch, keras, tensorflow need other installations for example CUDA, Java etc. So please follow installing instructions in their documentations or websites."
  },
  
  {
    "title": "Understanding Pip | Python‚Äôs Package Manager",
    "url": "/posts/pip-tool/",
    "categories": "",
    "tags": "python, devops, tutorials",
    "date": "2022-12-20 00:00:00 +0300",
    





    
    "snippet": "Understanding Pip: Python‚Äôs Package ManagerHello Python enthusiasts! Let‚Äôs get technical and break down what pip is, how it works, and its role in your development environment.What is Pip?Pip (whic...",
    "content": "Understanding Pip: Python‚Äôs Package ManagerHello Python enthusiasts! Let‚Äôs get technical and break down what pip is, how it works, and its role in your development environment.What is Pip?Pip (which stands for Pip Installs Packages or Pip Installs Python) is a command-line tool that simplifies the process of installing and managing Python libraries and packages. It interacts with the Python Package Index (PyPI) and other repositories to provide a seamless experience for managing dependencies in your projects.  Python Package Index (PyPI): PyPI is the primary repository for Python packages. It hosts a vast collection of libraries and tools that you can integrate into your Python projects.How Does Pip Work?Pip operates by leveraging a few core mechanisms to streamline package management:  Package Repositories:  Pip queries PyPI for packages. PyPI is a comprehensive index of Python libraries and tools, providing developers with access to a wide array of resources.  Installation Process:  When you execute a command like pip install requests, pip downloads the requests package and its dependencies from PyPI. It installs these packages into your Python environment, ensuring that all necessary components are available for your project.  Version Management:  Pip allows for precise control over package versions. You can specify exact versions or version ranges to ensure compatibility with your project requirements.          pip install numpy==1.21.0      Virtual Environments:Pip integrates seamlessly with Python‚Äôs virtual environments, created using tools like venv or virtualenv. Virtual environments allow you to isolate project dependencies, preventing conflicts between packages used in different projects. You can activate a virtual environment and use pip within that isolated space to manage project-specific packages.Upgrading and Uninstalling:  Pip simplifies the process of upgrading packages to their latest versions with:pip install --upgrade package_name  To remove a package, you can use: pip uninstall package_nameRequirements Files:  A requirements.txt file lists all the packages required for a project. You can generate this file using: pip freeze &gt; requirements.txt  To install all packages specified in the requirements.txt file, use: pip install -r requirements.txtCommon Pip CommandsHere are some essential pip commands to streamline your workflow:  Install a Package: pip install package_name  Upgrade a Package: pip install --upgrade package_name  Uninstall a Package: pip uninstall package_name  List Installed Packages: pip list  Show Package Details: pip show package_nameVisualizing Pip‚Äôs WorkflowTo better understand how pip operates, let‚Äôs visualize the process using a flowchart:flowchart LR    A[User Requests       Package] --&gt; B{Check       PyPI}    B --&gt; C[Download     Package]    C --&gt; D[Install     Package]    D --&gt; E{Check     Dependencies}    E --&gt; F[Install     Dependencies]    F --&gt; G[Complete     Installation]In this flowchart:  \\(A\\) represents the user requesting a package.  \\(B\\) shows pip checking PyPI for the requested package.  \\(C\\) indicates downloading the package.  \\(D\\) involves installing the package.  \\(E\\) checks for any dependencies required by the package.  \\(F\\) installs those dependencies.  \\(G\\) marks the completion of the installation.Additional Resources  Pip Documentation: Pip Docs provides comprehensive information about using pip, including advanced features and troubleshooting tips.  Virtualenv Documentation: Virtualenv offers details on creating and managing isolated Python environments.  PyPI: Python Package Index is where you can search for and download Python packages.ConclusionPip is a powerful tool that plays a pivotal role in modern Python development. It simplifies the process of managing libraries and dependencies, ensuring that your projects have access to the tools they need. By leveraging pip and virtual environments, you can maintain clean, organized, and efficient development workflows. If you want to package your own packages you can visit this link or you want to know more technical details about installing packages you can visit this link!Understanding and effectively using pip is essential for any Python developer. With its ability to manage package installations, upgrades, and removals, pip streamlines the development process, allowing you to focus more on coding and less on configuration. Happy coding, and may your Python projects be as smooth as your pip commands! üöÄüêç"
  },
  
  {
    "title": "How to Install Python 3.9 on Windows | A Step-by-Step Guide",
    "url": "/posts/python-101/",
    "categories": "",
    "tags": "python, setup, devops",
    "date": "2022-12-10 00:00:00 +0300",
    





    
    "snippet": "How to Install Python 3.9 on Windows: A Step-by-Step GuideHey there! If you‚Äôre ready to start coding with Python 3.9 on your Windows machine, you‚Äôre in the right place. Python is an incredibly vers...",
    "content": "How to Install Python 3.9 on Windows: A Step-by-Step GuideHey there! If you‚Äôre ready to start coding with Python 3.9 on your Windows machine, you‚Äôre in the right place. Python is an incredibly versatile programming language, and getting it set up is a breeze. Let‚Äôs walk through the installation process and cover a bit about what makes Python so special.What is Python?  Python is a high-level, interpreted programming language known for its readability and ease of use. It‚Äôs popular among developers, data scientists, and hobbyists for a variety of reasons:  Readability: Python‚Äôs syntax is clear and straightforward, making it easy to learn and write.  Versatility: You can use Python for web development, data analysis, artificial intelligence, scientific computing, automation, and more.  Community Support: With a massive user base and a wealth of libraries and frameworks, Python has a strong and supportive community.Advantages of Python:  Easy to Learn: Its simple syntax allows beginners to pick it up quickly.  Rich Ecosystem: Python has a vast collection of libraries and frameworks for almost any task.  Cross-Platform: Python runs on Windows, macOS, and Linux, so you can use it on any system.  Open Source: Python is free to use and has a strong, open-source community.Disadvantages of Python:  Performance: Python can be slower than compiled languages like C++ or Java due to its interpreted nature.  Mobile Development: Python isn‚Äôt commonly used for mobile app development, though there are tools that can help.  Memory Consumption: Python‚Äôs memory consumption can be high, which might be a concern for applications requiring efficient memory use.Using Python:Python is incredibly versatile. Here are some areas where it shines:  Web Development: Frameworks like Django and Flask make web development a breeze.  Data Science: Libraries such as Pandas, NumPy, and SciPy are staples for data analysis.  Machine Learning: TensorFlow, Keras, and Scikit-learn are popular for machine learning and AI projects.  Automation: Python is great for scripting and automating repetitive tasks.Installing Python 3.9 on WindowsLet‚Äôs get Python 3.9 installed on your Windows machine. Follow these steps:  Download the Installer:  Go to the official Python website and download the Python 3.9 installer for Windows. Make sure to choose the version that matches your system architecture (32-bit or 64-bit). I selected x86-64 version because of my system is 64-bit windows 10.Install Files on the official Python Website  Run the Installer:  Once the download is complete, open the installer. Here‚Äôs an important step: Check the box that says ‚ÄúAdd Python 3.9 to PATH‚Äù. This ensures that you can run Python from the Command Prompt.Step1      Choose Installation Type:  You‚Äôll see options to ‚ÄúInstall Now‚Äù or ‚ÄúCustomize Installation‚Äù. If you‚Äôre new to Python, ‚ÄúInstall Now‚Äù is usually the best choice. If you need specific configurations, select ‚ÄúCustomize Installation‚Äù and adjust as needed.        Complete the Installation:  Click ‚ÄúInstall Now‚Äù and let the installer do its thing. It will also set up pip, Python‚Äôs package manager, which is great for installing additional libraries and tools.    Verify the Installation:  To check if Python is installed correctly, open Command Prompt (you can find it by searching cmd in the Start menu) and type:          python ‚Äìversion      pip ‚Äìversion      Check SetupYou should see Python 3.9.x displayed. If you do, Python 3.9 is successfully installed and ready to use!ConclusionCongrats on getting Python 3.9 installed! As you embark on your programming journey, it‚Äôs worth reflecting on Python‚Äôs guiding philosophy, often referred to as ‚ÄúThe Zen of Python.‚Äù This set of principles encapsulates the essence of Python and serves as a reminder of what makes it such a powerful and enjoyable language to work with.To sum up, here are a few key lines from The Zen of Python that you might find inspiring:      ‚ÄúBeautiful is better than ugly.‚Äù Python encourages writing clean, readable code that others can easily understand and appreciate.    ‚ÄúExplicit is better than implicit.‚Äù Python emphasizes clarity and straightforwardness, making it easier for developers to follow and maintain code.    ‚ÄúSimple is better than complex.‚Äù Python promotes simplicity in design and implementation, which can help avoid unnecessary complications.    ‚ÄúReadability counts.‚Äù One of Python‚Äôs core strengths is its focus on code readability, which improves collaboration and reduces errors.  Rest of them you can read like that:  Python ZenAs you dive into Python, remember these guiding principles. They‚Äôll help you write better code and make your programming experience more enjoyable. Happy coding, and may your journey with Python be as elegant and efficient as its philosophy!"
  }
  
]

